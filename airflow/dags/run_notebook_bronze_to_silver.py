"""
Airflow DAG Ä‘á»ƒ cháº¡y notebook Load_Data_Bronze_To_Silver.ipynb trÃªn Spark Cluster
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import nbformat
from nbconvert.preprocessors import ExecutePreprocessor
import papermill as pm
import os

# ================================
# CONFIG
# ================================
NOTEBOOK_PATH = "/opt/airflow/notebooks/bronze/Load_Data_Bronze_To_Silver.ipynb"
OUTPUT_NOTEBOOK_PATH = "/opt/airflow/notebooks/bronze/Load_Data_Bronze_To_Silver_output_{}.ipynb"
SPARK_MASTER_URL = "spark://spark-master:7077"

# ================================
# FUNCTION: Execute Notebook ON Spark Master Container (BEST SOLUTION)
# ================================
def execute_notebook_on_spark_master(**context):
    """
    ğŸ¯ GIáº¢I PHÃP Tá»T NHáº¤T: Cháº¡y notebook TRá»°C TIáº¾P trÃªn Spark Master container
    VÃ¬ Spark Master Ä‘Ã£ cÃ³ Java vÃ  PySpark, notebook sáº½ cháº¡y khÃ´ng cáº§n Java gateway local
    """
    import subprocess
    
    execution_date = context['execution_date'].strftime('%Y%m%d_%H%M%S')
    output_path = OUTPUT_NOTEBOOK_PATH.format(execution_date)
    
    print(f"ğŸš€ Äang submit notebook lÃªn Spark Master container...")
    print(f"ğŸ“ Input: {NOTEBOOK_PATH}")
    print(f"ğŸ“ Output: {output_path}")
    
    # Kiá»ƒm tra xem file notebook cÃ³ tá»“n táº¡i trong container khÃ´ng
    print("\nğŸ” Kiá»ƒm tra file notebook trong Spark Master container...")
    try:
        check_cmd = ['docker', 'exec', 'spark-master', 'ls', '-la', NOTEBOOK_PATH]
        check_result = subprocess.run(check_cmd, capture_output=True, text=True)
        print(f"ğŸ“ File check result:\n{check_result.stdout}")
        if check_result.returncode != 0:
            print(f"âš ï¸  Warning: {check_result.stderr}")
    except Exception as e:
        print(f"âš ï¸  KhÃ´ng thá»ƒ kiá»ƒm tra file: {str(e)}")
    
    try:
        # Cháº¡y papermill TRONG spark-master container
        # KhÃ´ng dÃ¹ng --parameters vá»›i JSON, thay vÃ o Ä‘Ã³ dÃ¹ng -p cho tá»«ng parameter
        cmd = [
            'docker', 'exec', 'spark-master',
            'papermill',
            NOTEBOOK_PATH,
            output_path,
            '--kernel', 'python3',
            '--log-output',
            '--cwd', '/opt/airflow/notebooks/bronze'  # Set working directory
        ]
        
        print("â³ Äang thá»±c thi notebook trÃªn Spark Master...")
        print(f"ğŸ”— Command: {' '.join(cmd)}")
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=True,
            timeout=3600  # 1 hour timeout
        )
        
        print("âœ… Notebook Ä‘Ã£ thá»±c thi thÃ nh cÃ´ng trÃªn Spark Master!")
        print(f"ğŸ“Š Kiá»ƒm tra káº¿t quáº£ táº¡i: {output_path}")
        print(f"ğŸ” Xem Spark jobs táº¡i: http://spark-master:8080")
        
        if result.stdout:
            print(f"\nğŸ“‹ Output (last 2000 chars):\n{result.stdout[-2000:]}")
        
        return output_path
        
    except subprocess.TimeoutExpired as e:
        print(f"â° Timeout: Notebook cháº¡y quÃ¡ lÃ¢u (> 1 giá»)")
        print(f"ğŸ“Š Stdout:\n{e.stdout}")
        print(f"ğŸ“Š Stderr:\n{e.stderr}")
        raise
    except subprocess.CalledProcessError as e:
        print(f"âŒ Lá»—i khi thá»±c thi notebook: {str(e)}")
        print(f"ğŸ“Š Return code: {e.returncode}")
        if e.stdout:
            print(f"ğŸ“Š Stdout (last 2000 chars):\n{e.stdout[-2000:]}")
        if e.stderr:
            print(f"ğŸ“Š Stderr (last 2000 chars):\n{e.stderr[-2000:]}")
        raise
    except Exception as e:
        print(f"âŒ Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh: {str(e)}")
        import traceback
        traceback.print_exc()
        raise


# ================================
# FUNCTION: Execute Notebook with Papermill (Cháº¡y notebook, PySpark submit jobs lÃªn cluster)
# ================================
def execute_notebook_with_papermill(**context):
    """
    âš ï¸ KHÃ”NG HOáº T Äá»˜NG: VÃ¬ Airflow khÃ´ng cÃ³ Java, khÃ´ng thá»ƒ khá»Ÿi táº¡o PySpark local gateway
    Sá»­ dá»¥ng execute_notebook_on_spark_master thay tháº¿
    """
    execution_date = context['execution_date'].strftime('%Y%m%d_%H%M%S')
    output_path = OUTPUT_NOTEBOOK_PATH.format(execution_date)
    
    print(f"ğŸš€ Báº¯t Ä‘áº§u thá»±c thi notebook: {NOTEBOOK_PATH}")
    print(f"ğŸ“ Káº¿t quáº£ sáº½ Ä‘Æ°á»£c lÆ°u táº¡i: {output_path}")
    print(f"âš¡ PySpark jobs sáº½ cháº¡y trÃªn Spark cluster: {SPARK_MASTER_URL}")
    
    try:
        # Sá»­ dá»¥ng Papermill Ä‘á»ƒ thá»±c thi notebook
        # Notebook sáº½ táº¡o SparkSession vÃ  submit jobs lÃªn Spark cluster
        pm.execute_notebook(
            input_path=NOTEBOOK_PATH,
            output_path=output_path,
            parameters={
                'spark_master_url': SPARK_MASTER_URL,
                'execution_date': execution_date
            },
            kernel_name='python3',
            progress_bar=False,
            log_output=True
        )
        
        print("âœ… Notebook Ä‘Ã£ thá»±c thi thÃ nh cÃ´ng!")
        print(f"ğŸ“Š Kiá»ƒm tra káº¿t quáº£ táº¡i: {output_path}")
        print(f"ğŸ” Xem Spark jobs táº¡i: http://spark-master:8080")
        
        return output_path
        
    except Exception as e:
        print(f"âŒ Lá»—i khi thá»±c thi notebook: {str(e)}")
        raise


# ================================
# FUNCTION: Execute Notebook on Spark Cluster via HTTP (Remote Execution)
# ================================
def execute_notebook_on_spark_remote(**context):
    """
    Submit vÃ  cháº¡y notebook trá»±c tiáº¿p trÃªn Spark cluster thÃ´ng qua Livy REST API
    YÃªu cáº§u: Livy server cháº¡y trÃªn Spark cluster
    """
    import requests
    import time
    
    execution_date = context['execution_date'].strftime('%Y%m%d_%H%M%S')
    livy_url = "http://spark-master:8998"  # Livy REST API endpoint
    
    print(f"ğŸš€ Submit notebook lÃªn Spark cluster qua Livy...")
    print(f"ğŸ”— Livy URL: {livy_url}")
    
    # Äá»c notebook vÃ  convert sang code
    with open(NOTEBOOK_PATH) as f:
        nb = nbformat.read(f, as_version=4)
    
    # Extract Python code tá»« notebook
    code_cells = [cell['source'] for cell in nb.cells if cell['cell_type'] == 'code']
    code = '\n\n'.join(code_cells)
    
    # Create Livy session
    session_data = {
        "kind": "pyspark",
        "conf": {
            "spark.sql.catalog.nessie": "org.apache.iceberg.spark.SparkCatalog",
            "spark.sql.catalog.nessie.catalog-impl": "org.apache.iceberg.nessie.NessieCatalog",
            "spark.sql.catalog.nessie.uri": "http://nessie:19120/api/v1",
            "spark.hadoop.fs.s3a.endpoint": "http://minio:9000",
            "spark.hadoop.fs.s3a.access.key": "admin",
            "spark.hadoop.fs.s3a.secret.key": "admin123"
        }
    }
    
    try:
        # Create session
        print("ğŸ“ Táº¡o Livy session...")
        response = requests.post(f"{livy_url}/sessions", json=session_data)
        session_id = response.json()['id']
        print(f"âœ… Session ID: {session_id}")
        
        # Wait for session ready
        while True:
            status = requests.get(f"{livy_url}/sessions/{session_id}").json()
            if status['state'] == 'idle':
                break
            time.sleep(5)
        
        # Submit code
        print("ğŸš€ Submit code lÃªn Spark...")
        code_data = {"code": code}
        response = requests.post(f"{livy_url}/sessions/{session_id}/statements", json=code_data)
        statement_id = response.json()['id']
        
        # Wait for completion
        while True:
            result = requests.get(f"{livy_url}/sessions/{session_id}/statements/{statement_id}").json()
            if result['state'] in ['available', 'error', 'cancelled']:
                break
            time.sleep(10)
        
        print("âœ… Notebook Ä‘Ã£ cháº¡y xong trÃªn Spark cluster!")
        return result
        
    except Exception as e:
        print(f"âŒ Lá»—i: {str(e)}")
        raise
    finally:
        # Cleanup session
        try:
            requests.delete(f"{livy_url}/sessions/{session_id}")
        except:
            pass


# ================================
# FUNCTION: Execute Notebook with nbconvert (Alternative)
# ================================
def execute_notebook_with_nbconvert(**context):
    """
    Thá»±c thi notebook sá»­ dá»¥ng nbconvert
    PhÆ°Æ¡ng phÃ¡p thay tháº¿ náº¿u khÃ´ng dÃ¹ng Papermill
    """
    execution_date = context['execution_date'].strftime('%Y%m%d_%H%M%S')
    output_path = OUTPUT_NOTEBOOK_PATH.format(execution_date)
    
    print(f"ğŸš€ Báº¯t Ä‘áº§u thá»±c thi notebook: {NOTEBOOK_PATH}")
    print(f"ğŸ“ Káº¿t quáº£ sáº½ Ä‘Æ°á»£c lÆ°u táº¡i: {output_path}")
    
    try:
        # Äá»c notebook
        with open(NOTEBOOK_PATH) as f:
            nb = nbformat.read(f, as_version=4)
        
        # Cáº¥u hÃ¬nh executor
        ep = ExecutePreprocessor(
            timeout=3600,  # 1 giá» timeout
            kernel_name='python3',
            allow_errors=False  # Dá»«ng náº¿u cÃ³ lá»—i
        )
        
        # Thá»±c thi notebook
        print("â³ Äang thá»±c thi notebook...")
        ep.preprocess(nb)
        
        # LÆ°u káº¿t quáº£
        with open(output_path, 'w', encoding='utf-8') as f:
            nbformat.write(nb, f)
        
        print("âœ… Notebook Ä‘Ã£ thá»±c thi thÃ nh cÃ´ng!")
        print(f"ğŸ“Š Kiá»ƒm tra káº¿t quáº£ táº¡i: {output_path}")
        
        return output_path
        
    except Exception as e:
        print(f"âŒ Lá»—i khi thá»±c thi notebook: {str(e)}")
        raise


# ================================
# FUNCTION: Submit PySpark Job to Cluster
# ================================
def submit_pyspark_job(**context):
    """
    Submit PySpark job lÃªn Spark Cluster báº±ng spark-submit
    PhÆ°Æ¡ng phÃ¡p nÃ y convert notebook thÃ nh Python script vÃ  submit lÃªn cluster
    """
    import subprocess
    import json
    
    execution_date = context['execution_date'].strftime('%Y%m%d_%H%M%S')
    
    print(f"ğŸš€ Äang submit PySpark job lÃªn Spark Cluster...")
    print(f"ğŸ”— Spark Master URL: {SPARK_MASTER_URL}")
    
    # Convert notebook to Python script
    script_path = f"/tmp/load_bronze_to_silver_{execution_date}.py"
    
    try:
        # Convert notebook sang Python script
        print("ğŸ“ Äang convert notebook sang Python script...")
        subprocess.run([
            'jupyter', 'nbconvert',
            '--to', 'script',
            '--output', script_path,
            NOTEBOOK_PATH
        ], check=True)
        
        print(f"âœ… ÄÃ£ convert notebook thÃ nh: {script_path}")
        
        # Submit job lÃªn Spark cluster
        print("ğŸš€ Äang submit job lÃªn Spark cluster...")
        
        spark_submit_cmd = [
            'spark-submit',
            '--master', SPARK_MASTER_URL,
            '--deploy-mode', 'client',
            '--conf', 'spark.sql.catalog.nessie=org.apache.iceberg.spark.SparkCatalog',
            '--conf', 'spark.sql.catalog.nessie.catalog-impl=org.apache.iceberg.nessie.NessieCatalog',
            '--conf', 'spark.sql.catalog.nessie.uri=http://nessie:19120/api/v1',
            '--conf', 'spark.sql.catalog.nessie.ref=main',
            '--conf', 'spark.sql.catalog.nessie.warehouse=s3a://silver/',
            '--conf', 'spark.sql.catalog.nessie.s3.endpoint=http://minio:9000',
            '--conf', 'spark.sql.catalog.nessie.s3.access-key=admin',
            '--conf', 'spark.sql.catalog.nessie.s3.secret-key=admin123',
            '--conf', 'spark.sql.catalog.nessie.s3.path-style-access=true',
            '--conf', 'spark.hadoop.fs.s3a.endpoint=http://minio:9000',
            '--conf', 'spark.hadoop.fs.s3a.access.key=admin',
            '--conf', 'spark.hadoop.fs.s3a.secret.key=admin123',
            '--conf', 'spark.hadoop.fs.s3a.path.style.access=true',
            '--conf', 'spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem',
            '--driver-memory', '2g',
            '--executor-memory', '2g',
            '--executor-cores', '2',
            script_path
        ]
        
        # Cháº¡y spark-submit
        result = subprocess.run(
            spark_submit_cmd,
            capture_output=True,
            text=True,
            check=True
        )
        
        print("âœ… Spark job Ä‘Ã£ hoÃ n thÃ nh thÃ nh cÃ´ng!")
        print(f"ğŸ“Š Output:\n{result.stdout}")
        
        if result.stderr:
            print(f"âš ï¸  Stderr:\n{result.stderr}")
        
        # Clean up
        if os.path.exists(script_path):
            os.remove(script_path)
            print(f"ğŸ§¹ ÄÃ£ xÃ³a file táº¡m: {script_path}")
        
        return result.stdout
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ Lá»—i khi submit Spark job: {str(e)}")
        print(f"ğŸ“Š Output:\n{e.stdout}")
        print(f"ğŸ“Š Error:\n{e.stderr}")
        raise
    except Exception as e:
        print(f"âŒ Lá»—i: {str(e)}")
        raise


# ================================
# FUNCTION: Check Spark Cluster Health
# ================================
def check_spark_cluster(**context):
    """
    Kiá»ƒm tra tráº¡ng thÃ¡i Spark Cluster vÃ  notebook mount trÆ°á»›c khi cháº¡y job
    """
    import requests
    import subprocess
    
    print("ğŸ” Kiá»ƒm tra tráº¡ng thÃ¡i Spark Cluster...")
    
    # 1. Kiá»ƒm tra Spark Master UI
    spark_master_ui = "http://spark-master:8080"
    try:
        response = requests.get(f"{spark_master_ui}/json/", timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            print(f"âœ… Spark Master Ä‘ang hoáº¡t Ä‘á»™ng")
            print(f"ğŸ“Š Status: {data.get('status', 'N/A')}")
            print(f"ğŸ‘· Workers: {len(data.get('workers', []))}")
            print(f"ğŸ’» Cores: {data.get('cores', 'N/A')}")
            print(f"ğŸ’¾ Memory: {data.get('memory', 'N/A')}")
        else:
            print(f"âš ï¸  Spark Master khÃ´ng pháº£n há»“i Ä‘Ãºng: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ KhÃ´ng thá»ƒ káº¿t ná»‘i Ä‘áº¿n Spark Master: {str(e)}")
        raise
    
    # 2. Kiá»ƒm tra notebook cÃ³ Ä‘Æ°á»£c mount vÃ o Spark Master khÃ´ng
    print("\nğŸ” Kiá»ƒm tra notebook mount trong Spark Master...")
    try:
        check_cmd = ['docker', 'exec', 'spark-master', 'ls', '-la', '/opt/airflow/notebooks/bronze/']
        result = subprocess.run(check_cmd, capture_output=True, text=True, timeout=10)
        
        if result.returncode == 0:
            print("âœ… Notebooks Ä‘Ã£ Ä‘Æ°á»£c mount vÃ o Spark Master")
            print(f"ğŸ“ Files:\n{result.stdout}")
            
            # Kiá»ƒm tra file notebook cá»¥ thá»ƒ
            if "Load_Data_Bronze_To_Silver.ipynb" in result.stdout:
                print("âœ… File notebook Load_Data_Bronze_To_Silver.ipynb tá»“n táº¡i!")
            else:
                print("âš ï¸  File notebook Load_Data_Bronze_To_Silver.ipynb KHÃ”NG tÃ¬m tháº¥y!")
                return False
        else:
            print(f"âŒ KhÃ´ng thá»ƒ truy cáº­p notebooks directory: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âŒ Lá»—i khi kiá»ƒm tra notebook mount: {str(e)}")
        raise
    
    # 3. Kiá»ƒm tra papermill Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t trong Spark Master chÆ°a
    print("\nğŸ” Kiá»ƒm tra Papermill trong Spark Master...")
    try:
        check_papermill = ['docker', 'exec', 'spark-master', 'which', 'papermill']
        result = subprocess.run(check_papermill, capture_output=True, text=True, timeout=10)
        
        if result.returncode == 0:
            print(f"âœ… Papermill Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t: {result.stdout.strip()}")
        else:
            print("âš ï¸  Papermill chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t trong Spark Master!")
            print("ğŸ’¡ Cáº§n rebuild Spark Docker image vá»›i Papermill")
            return False
            
    except Exception as e:
        print(f"âŒ Lá»—i khi kiá»ƒm tra Papermill: {str(e)}")
        raise
    
    print("\nâœ… Táº¥t cáº£ kiá»ƒm tra Ä‘Ã£ PASS!")
    return True


# ================================
# FUNCTION: Verify Data in Silver Layer (Alternative - Run on Spark Master)
# ================================
def verify_silver_data(**context):
    """
    Kiá»ƒm tra dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c load vÃ o Silver layer
    Cháº¡y trÃªn Spark Master container Ä‘á»ƒ trÃ¡nh lá»—i Java gateway
    """
    import subprocess
    
    print("ğŸ” Kiá»ƒm tra dá»¯ liá»‡u trong Silver layer...")
    
    # Táº¡o script Python Ä‘á»ƒ verify data
    verify_script = """
from pyspark.sql import SparkSession

# Táº¡o Spark Session
spark = (
    SparkSession.builder
    .appName("Verify_Silver_Data")
    .master("spark://spark-master:7077")
    .config("spark.sql.catalog.nessie", "org.apache.iceberg.spark.SparkCatalog")
    .config("spark.sql.catalog.nessie.catalog-impl", "org.apache.iceberg.nessie.NessieCatalog")
    .config("spark.sql.catalog.nessie.uri", "http://nessie:19120/api/v1")
    .config("spark.sql.catalog.nessie.ref", "main")
    .config("spark.sql.catalog.nessie.warehouse", "s3a://silver/")
    .config("spark.sql.catalog.nessie.s3.endpoint", "http://minio:9000")
    .config("spark.sql.catalog.nessie.s3.access-key", "admin")
    .config("spark.sql.catalog.nessie.s3.secret-key", "admin123")
    .config("spark.sql.catalog.nessie.s3.path-style-access", "true")
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000")
    .config("spark.hadoop.fs.s3a.access.key", "admin")
    .config("spark.hadoop.fs.s3a.secret.key", "admin123")
    .config("spark.hadoop.fs.s3a.path.style.access", "true")
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
    .getOrCreate()
)

try:
    # Kiá»ƒm tra báº£ng school
    print("\\nğŸ“Š Kiá»ƒm tra báº£ng nessie.silver_tables.school:")
    df_school = spark.table("nessie.silver_tables.school")
    school_count = df_school.count()
    print(f"âœ… Sá»‘ dÃ²ng trong báº£ng school: {school_count}")
    
    # Kiá»ƒm tra báº£ng major
    print("\\nğŸ“Š Kiá»ƒm tra báº£ng nessie.silver_tables.major:")
    df_major = spark.table("nessie.silver_tables.major")
    major_count = df_major.count()
    print(f"âœ… Sá»‘ dÃ²ng trong báº£ng major: {major_count}")
    
    print(f"\\nâœ… VERIFICATION_SUCCESS: school={school_count}, major={major_count}")
except Exception as e:
    print(f"âŒ VERIFICATION_FAILED: {str(e)}")
    raise
finally:
    spark.stop()
"""
    
    try:
        # LÆ°u script vÃ o file táº¡m trong container
        script_path = "/tmp/verify_silver.py"
        
        # Write script to container
        write_cmd = ['docker', 'exec', '-i', 'spark-master', 'bash', '-c', 
                    f'cat > {script_path}']
        subprocess.run(write_cmd, input=verify_script.encode(), check=True)
        
        # Cháº¡y script trong spark-master container
        print("â³ Äang cháº¡y verification script trÃªn Spark Master...")
        cmd = [
            'docker', 'exec', 'spark-master',
            'python3', script_path
        ]
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=True,
            timeout=300  # 5 minutes timeout
        )
        
        print("âœ… Verification thÃ nh cÃ´ng!")
        print(f"\nğŸ“‹ Output:\n{result.stdout}")
        
        # Parse káº¿t quáº£
        if "VERIFICATION_SUCCESS" in result.stdout:
            # Extract counts from output
            for line in result.stdout.split('\n'):
                if "VERIFICATION_SUCCESS" in line:
                    print(f"\nğŸ‰ {line}")
        
        return result.stdout
        
    except subprocess.TimeoutExpired as e:
        print(f"â° Timeout: Verification cháº¡y quÃ¡ lÃ¢u")
        print(f"ğŸ“Š Stdout:\n{e.stdout}")
        raise
    except subprocess.CalledProcessError as e:
        print(f"âŒ Lá»—i khi verify dá»¯ liá»‡u: {str(e)}")
        if e.stdout:
            print(f"ğŸ“Š Stdout:\n{e.stdout}")
        if e.stderr:
            print(f"ğŸ“Š Stderr:\n{e.stderr}")
        raise
    except Exception as e:
        print(f"âŒ Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh: {str(e)}")
        import traceback
        traceback.print_exc()
        raise


# ================================
# DAG DEFINITION
# ================================
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    dag_id='run_notebook_bronze_to_silver',
    default_args=default_args,
    description='Cháº¡y notebook Load_Data_Bronze_To_Silver.ipynb trÃªn Spark Cluster',
    schedule_interval='@daily',  # Cháº¡y hÃ ng ngÃ y
    catchup=False,
    tags=['bronze', 'silver', 'spark', 'notebook', 'etl'],
) as dag:

    # Task 1: Kiá»ƒm tra Spark Cluster
    check_cluster_task = PythonOperator(
        task_id='check_spark_cluster',
        python_callable=check_spark_cluster,
        provide_context=True
    )    # Task 2: Thá»±c thi notebook (Chá»n 1 trong 3 phÆ°Æ¡ng phÃ¡p)
    
    # PhÆ°Æ¡ng phÃ¡p 1: Cháº¡y notebook trÃªn Spark Master container (KhuyÃªn dÃ¹ng) âœ…
    execute_notebook_task = PythonOperator(
        task_id='execute_notebook_on_spark_master',
        python_callable=execute_notebook_on_spark_master,
        provide_context=True
    )
    
    # PhÆ°Æ¡ng phÃ¡p 2: Sá»­ dá»¥ng nbconvert (Alternative - bá» comment náº¿u muá»‘n dÃ¹ng)
    # execute_notebook_task = PythonOperator(
    #     task_id='execute_notebook_nbconvert',
    #     python_callable=execute_notebook_with_nbconvert,
    #     provide_context=True
    # )
    
    # PhÆ°Æ¡ng phÃ¡p 3: Submit Spark Job trá»±c tiáº¿p (KhÃ´ng khuyÃªn dÃ¹ng - cáº§n JAVA_HOME)
    # execute_notebook_task = PythonOperator(
    #     task_id='submit_pyspark_job',
    #     python_callable=submit_pyspark_job,
    #     provide_context=True
    # )

    # Task 3: Kiá»ƒm tra dá»¯ liá»‡u sau khi load
    verify_data_task = PythonOperator(
        task_id='verify_silver_data',
        python_callable=verify_silver_data,
        provide_context=True
    )

    # Define task dependencies
    check_cluster_task >> execute_notebook_task >> verify_data_task
