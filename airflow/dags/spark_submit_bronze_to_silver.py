"""
Airflow DAG: Submit PySpark Script lÃªn Spark Cluster Ä‘á»ƒ load data Bronze -> Silver

DAG nÃ y sá»­ dá»¥ng spark-submit Ä‘á»ƒ cháº¡y file Python script trá»±c tiáº¿p trÃªn Spark Cluster,
thay vÃ¬ cháº¡y Jupyter notebook.
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import subprocess
import os


# ================================
# CONFIG
# ================================
SPARK_MASTER_URL = "spark://spark-master:7077"
PYSPARK_SCRIPT_PATH = "/opt/airflow/scripts/load_bronze_to_silver.py"
SPARK_CONTAINER = "spark-master"


# ================================
# FUNCTION: Check Spark Cluster Health
# ================================
def check_spark_cluster(**context):
    """
    Kiá»ƒm tra tráº¡ng thÃ¡i Spark Cluster trÆ°á»›c khi submit job
    """
    import requests
    
    print("ğŸ” Kiá»ƒm tra tráº¡ng thÃ¡i Spark Cluster...")
    
    # Kiá»ƒm tra Spark Master UI
    spark_master_ui = "http://spark-master:8080"
    try:
        response = requests.get(f"{spark_master_ui}/json/", timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            print(f"âœ… Spark Master Ä‘ang hoáº¡t Ä‘á»™ng")
            print(f"ğŸ“Š Status: {data.get('status', 'N/A')}")
            print(f"ğŸ‘· Workers: {len(data.get('workers', []))}")
            print(f"ğŸ’» Cores: {data.get('cores', 'N/A')}")
            print(f"ğŸ’¾ Memory: {data.get('memory', 'N/A')}")
        else:
            print(f"âš ï¸  Spark Master khÃ´ng pháº£n há»“i Ä‘Ãºng: {response.status_code}")
            raise Exception("Spark Master khÃ´ng hoáº¡t Ä‘á»™ng")
            
    except Exception as e:
        print(f"âŒ KhÃ´ng thá»ƒ káº¿t ná»‘i Ä‘áº¿n Spark Master: {str(e)}")
        raise
    
    # Kiá»ƒm tra Python script cÃ³ tá»“n táº¡i khÃ´ng
    print("\nğŸ” Kiá»ƒm tra Python script trong Spark Master container...")
    try:
        check_cmd = ['docker', 'exec', SPARK_CONTAINER, 'ls', '-la', PYSPARK_SCRIPT_PATH]
        result = subprocess.run(check_cmd, capture_output=True, text=True, timeout=10)
        
        if result.returncode == 0:
            print("âœ… Python script Ä‘Ã£ Ä‘Æ°á»£c mount vÃ o Spark Master")
            print(f"ğŸ“ File info:\n{result.stdout}")
        else:
            print(f"âŒ KhÃ´ng thá»ƒ truy cáº­p Python script: {result.stderr}")
            raise Exception(f"Python script khÃ´ng tá»“n táº¡i: {PYSPARK_SCRIPT_PATH}")
            
    except Exception as e:
        print(f"âŒ Lá»—i khi kiá»ƒm tra Python script: {str(e)}")
        raise
    
    print("\nâœ… Táº¥t cáº£ kiá»ƒm tra Ä‘Ã£ PASS!")
    return True


# ================================
# FUNCTION: Submit PySpark Job to Spark Cluster
# ================================
def submit_pyspark_job_to_cluster(**context):
    """
    Submit PySpark script lÃªn Spark Cluster báº±ng spark-submit
    Cháº¡y trá»±c tiáº¿p trong Spark Master container
    """
    execution_date = context['execution_date'].strftime('%Y%m%d_%H%M%S')
    
    print(f"ğŸš€ Äang submit PySpark job lÃªn Spark Cluster...")
    print(f"ğŸ”— Spark Master URL: {SPARK_MASTER_URL}")
    print(f"ğŸ“ Script: {PYSPARK_SCRIPT_PATH}")
    print(f"â° Execution Date: {execution_date}")
    
    try:
        # XÃ¢y dá»±ng spark-submit command
        # Cháº¡y spark-submit TRONG spark-master container
        spark_submit_cmd = [
            'docker', 'exec', SPARK_CONTAINER,
            'spark-submit',
            '--master', SPARK_MASTER_URL,
            '--deploy-mode', 'client',
            '--name', f'Load_Bronze_To_Silver_{execution_date}',
            # Iceberg & Nessie configurations
            '--conf', 'spark.sql.catalog.nessie=org.apache.iceberg.spark.SparkCatalog',
            '--conf', 'spark.sql.catalog.nessie.catalog-impl=org.apache.iceberg.nessie.NessieCatalog',
            '--conf', 'spark.sql.catalog.nessie.uri=http://nessie:19120/api/v1',
            '--conf', 'spark.sql.catalog.nessie.ref=main',
            '--conf', 'spark.sql.catalog.nessie.warehouse=s3a://silver/',
            # MinIO S3 configurations
            '--conf', 'spark.sql.catalog.nessie.s3.endpoint=http://minio:9000',
            '--conf', 'spark.sql.catalog.nessie.s3.access-key=admin',
            '--conf', 'spark.sql.catalog.nessie.s3.secret-key=admin123',
            '--conf', 'spark.sql.catalog.nessie.s3.path-style-access=true',
            '--conf', 'spark.hadoop.fs.s3a.endpoint=http://minio:9000',
            '--conf', 'spark.hadoop.fs.s3a.access.key=admin',
            '--conf', 'spark.hadoop.fs.s3a.secret.key=admin123',
            '--conf', 'spark.hadoop.fs.s3a.path.style.access=true',
            '--conf', 'spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem',
            # Resource configurations
            '--driver-memory', '2g',
            '--executor-memory', '2g',
            '--executor-cores', '2',
            '--total-executor-cores', '4',
            # Python script
            PYSPARK_SCRIPT_PATH
        ]
        
        print("\nâ³ Äang thá»±c thi spark-submit...")
        print(f"ğŸ”— Command: {' '.join(spark_submit_cmd)}")
        
        # Cháº¡y spark-submit
        result = subprocess.run(
            spark_submit_cmd,
            capture_output=True,
            text=True,
            check=True,
            timeout=3600  # 1 hour timeout
        )
        
        print("\nâœ… Spark job Ä‘Ã£ hoÃ n thÃ nh thÃ nh cÃ´ng!")
        print(f"ğŸ” Xem Spark jobs táº¡i: http://spark-master:8080")
        
        # In output
        if result.stdout:
            print(f"\nğŸ“‹ Output (last 3000 chars):\n{result.stdout[-3000:]}")
        
        if result.stderr:
            print(f"\nâš ï¸  Stderr (last 2000 chars):\n{result.stderr[-2000:]}")
        
        return result.stdout
        
    except subprocess.TimeoutExpired as e:
        print(f"â° Timeout: Spark job cháº¡y quÃ¡ lÃ¢u (> 1 giá»)")
        if e.stdout:
            print(f"ğŸ“Š Stdout (last 2000 chars):\n{e.stdout[-2000:]}")
        if e.stderr:
            print(f"ğŸ“Š Stderr (last 2000 chars):\n{e.stderr[-2000:]}")
        raise
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ Lá»—i khi submit Spark job: {str(e)}")
        print(f"ğŸ“Š Return code: {e.returncode}")
        if e.stdout:
            print(f"ğŸ“Š Stdout (last 2000 chars):\n{e.stdout[-2000:]}")
        if e.stderr:
            print(f"ğŸ“Š Stderr (last 2000 chars):\n{e.stderr[-2000:]}")
        raise
        
    except Exception as e:
        print(f"âŒ Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh: {str(e)}")
        import traceback
        traceback.print_exc()
        raise


# ================================
# FUNCTION: Verify Data in Silver Layer
# ================================
def verify_silver_data(**context):
    """
    Kiá»ƒm tra dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c load vÃ o Silver layer
    Cháº¡y trÃªn Spark Master container
    """
    print("ğŸ” Kiá»ƒm tra dá»¯ liá»‡u trong Silver layer...")
    
    # Táº¡o script Python Ä‘á»ƒ verify data
    verify_script = """
from pyspark.sql import SparkSession

# Táº¡o Spark Session
spark = (
    SparkSession.builder
    .appName("Verify_Silver_Data")
    .master("spark://spark-master:7077")
    .config("spark.sql.catalog.nessie", "org.apache.iceberg.spark.SparkCatalog")
    .config("spark.sql.catalog.nessie.catalog-impl", "org.apache.iceberg.nessie.NessieCatalog")
    .config("spark.sql.catalog.nessie.uri", "http://nessie:19120/api/v1")
    .config("spark.sql.catalog.nessie.ref", "main")
    .config("spark.sql.catalog.nessie.warehouse", "s3a://silver/")
    .config("spark.sql.catalog.nessie.s3.endpoint", "http://minio:9000")
    .config("spark.sql.catalog.nessie.s3.access-key", "admin")
    .config("spark.sql.catalog.nessie.s3.secret-key", "admin123")
    .config("spark.sql.catalog.nessie.s3.path-style-access", "true")
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000")
    .config("spark.hadoop.fs.s3a.access.key", "admin")
    .config("spark.hadoop.fs.s3a.secret.key", "admin123")
    .config("spark.hadoop.fs.s3a.path.style.access", "true")
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
    .getOrCreate()
)

spark.sparkContext.setLogLevel("ERROR")

try:
    # Kiá»ƒm tra báº£ng school
    print("\\nğŸ“Š Kiá»ƒm tra báº£ng nessie.silver_tables.school:")
    df_school = spark.table("nessie.silver_tables.school")
    school_count = df_school.count()
    print(f"âœ… Sá»‘ dÃ²ng trong báº£ng school: {school_count}")
    
    # Kiá»ƒm tra báº£ng major
    print("\\nğŸ“Š Kiá»ƒm tra báº£ng nessie.silver_tables.major:")
    df_major = spark.table("nessie.silver_tables.major")
    major_count = df_major.count()
    print(f"âœ… Sá»‘ dÃ²ng trong báº£ng major: {major_count}")
    
    # Kiá»ƒm tra báº£ng major_group
    print("\\nğŸ“Š Kiá»ƒm tra báº£ng nessie.silver_tables.major_group:")
    df_major_group = spark.table("nessie.silver_tables.major_group")
    major_group_count = df_major_group.count()
    print(f"âœ… Sá»‘ dÃ²ng trong báº£ng major_group: {major_group_count}")
    
    print(f"\\nâœ… VERIFICATION_SUCCESS: school={school_count}, major={major_count}, major_group={major_group_count}")
    
except Exception as e:
    print(f"âŒ VERIFICATION_FAILED: {str(e)}")
    import traceback
    traceback.print_exc()
    raise
finally:
    spark.stop()
"""
    
    try:
        # LÆ°u script vÃ o file táº¡m trong container
        script_path = "/tmp/verify_silver_data.py"
        
        # Write script to container
        write_cmd = ['docker', 'exec', '-i', SPARK_CONTAINER, 'bash', '-c', 
                    f'cat > {script_path}']
        subprocess.run(write_cmd, input=verify_script.encode(), check=True)
        
        # Cháº¡y script trong spark-master container
        print("â³ Äang cháº¡y verification script trÃªn Spark Master...")
        cmd = [
            'docker', 'exec', SPARK_CONTAINER,
            'python3', script_path
        ]
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=True,
            timeout=300  # 5 minutes timeout
        )
        
        print("âœ… Verification thÃ nh cÃ´ng!")
        print(f"\nğŸ“‹ Output:\n{result.stdout}")
        
        # Parse káº¿t quáº£
        if "VERIFICATION_SUCCESS" in result.stdout:
            for line in result.stdout.split('\n'):
                if "VERIFICATION_SUCCESS" in line:
                    print(f"\nğŸ‰ {line}")
        
        return result.stdout
        
    except subprocess.TimeoutExpired as e:
        print(f"â° Timeout: Verification cháº¡y quÃ¡ lÃ¢u")
        if e.stdout:
            print(f"ğŸ“Š Stdout:\n{e.stdout}")
        raise
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ Lá»—i khi verify dá»¯ liá»‡u: {str(e)}")
        if e.stdout:
            print(f"ğŸ“Š Stdout:\n{e.stdout}")
        if e.stderr:
            print(f"ğŸ“Š Stderr:\n{e.stderr}")
        raise
        
    except Exception as e:
        print(f"âŒ Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh: {str(e)}")
        import traceback
        traceback.print_exc()
        raise


# ================================
# DAG DEFINITION
# ================================
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    dag_id='spark_submit_bronze_to_silver',
    default_args=default_args,
    description='Submit PySpark script Ä‘á»ƒ load data tá»« Bronze sang Silver layer',
    schedule_interval='@daily',  # Cháº¡y hÃ ng ngÃ y
    catchup=False,
    tags=['bronze', 'silver', 'spark', 'pyspark', 'etl', 'spark-submit'],
) as dag:

    # Task 1: Kiá»ƒm tra Spark Cluster
    check_cluster_task = PythonOperator(
        task_id='check_spark_cluster',
        python_callable=check_spark_cluster,
        provide_context=True
    )

    # Task 2: Submit PySpark Job lÃªn Spark Cluster
    submit_job_task = PythonOperator(
        task_id='submit_pyspark_job',
        python_callable=submit_pyspark_job_to_cluster,
        provide_context=True
    )

    # Task 3: Kiá»ƒm tra dá»¯ liá»‡u sau khi load
    verify_data_task = PythonOperator(
        task_id='verify_silver_data',
        python_callable=verify_silver_data,
        provide_context=True
    )

    # Define task dependencies
    check_cluster_task >> submit_job_task >> verify_data_task
