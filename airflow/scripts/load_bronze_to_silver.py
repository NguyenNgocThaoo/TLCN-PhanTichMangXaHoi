#!/usr/bin/env python
# coding: utf-8

# # Load Data tá»« Bronze Layer sang Silver Layer
# 
# Notebook nÃ y sáº½ Ä‘á»c dá»¯ liá»‡u tá»« Bronze layer (MinIO) vÃ  xá»­ lÃ½ Ä‘á»ƒ load vÃ o cÃ¡c báº£ng Iceberg trong Silver layer vá»›i Nessie catalog.

# ## 1. Import Libraries vÃ  Khá»Ÿi táº¡o Spark Session

# In[ ]:


from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from datetime import datetime
import os

# Khá»Ÿi táº¡o Spark Session vá»›i Iceberg vÃ  Nessie catalog
# âš ï¸ CRITICAL: .master() pháº£i Ä‘á»©ng NGAY SAU .builder Ä‘á»ƒ avoid local mode initialization
spark = (
    SparkSession.builder
    .master("spark://spark-master:7077")  # âœ… FIRST: Connect to Spark cluster
    .appName("Load_Bronze_To_Silver")
    # ===== Iceberg Catalog qua Nessie =====
    .config("spark.sql.catalog.nessie", "org.apache.iceberg.spark.SparkCatalog")
    .config("spark.sql.catalog.nessie.catalog-impl", "org.apache.iceberg.nessie.NessieCatalog")
    .config("spark.sql.catalog.nessie.uri", "http://nessie:19120/api/v1")
    .config("spark.sql.catalog.nessie.ref", "main")
    .config("spark.sql.catalog.nessie.warehouse", "s3a://silver/")
    # ===== Cáº¥u hÃ¬nh MinIO (S3-compatible) =====
    .config("spark.sql.catalog.nessie.s3.endpoint", "http://minio:9000")
    .config("spark.sql.catalog.nessie.s3.access-key", "admin")
    .config("spark.sql.catalog.nessie.s3.secret-key", "admin123")
    .config("spark.sql.catalog.nessie.s3.path-style-access", "true")
    # ===== Spark + Hadoop S3 connector =====
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000")
    .config("spark.hadoop.fs.s3a.access.key", "admin")
    .config("spark.hadoop.fs.s3a.secret.key", "admin123")
    .config("spark.hadoop.fs.s3a.path.style.access", "true")
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
    .getOrCreate()
)

spark.sparkContext.setLogLevel("WARN")
print("âœ… Spark Session Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o vá»›i Nessie catalog!")
print(f"ğŸ”— Connected to Spark Master: {spark.sparkContext.master}")
print(f"ğŸ“Š Spark Version: {spark.version}")


# ## 2. Load Dá»¯ Liá»‡u School tá»« Bronze Layer

# In[2]:


# ÄÆ°á»ng dáº«n Ä‘áº¿n cÃ¡c file CSV trong Bronze layer trÃªn MinIO (2021-2025)
bronze_school_files = [
    "s3a://bronze/structured_data/danh sÃ¡ch cÃ¡c trÆ°á»ng Äáº¡i Há»c (2021-2025)/Danh_sÃ¡ch_cÃ¡c_trÆ°á»ng_Äáº¡i_Há»c_2021.csv",
    "s3a://bronze/structured_data/danh sÃ¡ch cÃ¡c trÆ°á»ng Äáº¡i Há»c (2021-2025)/Danh_sÃ¡ch_cÃ¡c_trÆ°á»ng_Äáº¡i_Há»c_2022.csv",
    "s3a://bronze/structured_data/danh sÃ¡ch cÃ¡c trÆ°á»ng Äáº¡i Há»c (2021-2025)/Danh_sÃ¡ch_cÃ¡c_trÆ°á»ng_Äáº¡i_Há»c_2023.csv",
    "s3a://bronze/structured_data/danh sÃ¡ch cÃ¡c trÆ°á»ng Äáº¡i Há»c (2021-2025)/Danh_sÃ¡ch_cÃ¡c_trÆ°á»ng_Äáº¡i_Há»c_2024.csv",
    "s3a://bronze/structured_data/danh sÃ¡ch cÃ¡c trÆ°á»ng Äáº¡i Há»c (2021-2025)/Danh_sÃ¡ch_cÃ¡c_trÆ°á»ng_Äáº¡i_Há»c_2025.csv"
]

# Äá»c dá»¯ liá»‡u tá»« Bronze layer
print("ğŸ“– Äang Ä‘á»c dá»¯ liá»‡u School tá»« Bronze layer (2021-2025)...")

dataframes = []
for file_path in bronze_school_files:
    try:
        year = file_path.split("_")[-1].replace(".csv", "")
        print(f"  â³ Äang Ä‘á»c file nÄƒm {year}...")

        df_temp = spark.read \
            .option("header", "true") \
            .option("inferSchema", "true") \
            .csv(file_path)

        # Chá»‰ giá»¯ 3 cá»™t: TenTruong, MaTruong, TinhThanh
        df_temp = df_temp.select(
            col("TenTruong"),
            col("MaTruong"),
            col("TinhThanh")
        )

        dataframes.append(df_temp)
        print(f"  âœ… Äá»c Ä‘Æ°á»£c {df_temp.count()} dÃ²ng tá»« nÄƒm {year}")

    except Exception as e:
        print(f"  âŒ Lá»—i khi Ä‘á»c file {file_path}: {str(e)}")

# GhÃ©p táº¥t cáº£ cÃ¡c DataFrame láº¡i vá»›i nhau
if dataframes:
    print("\nğŸ”— Äang ghÃ©p táº¥t cáº£ cÃ¡c file láº¡i...")
    df_school_bronze = dataframes[0]
    for df in dataframes[1:]:
        df_school_bronze = df_school_bronze.union(df)

    print(f"âœ… Tá»•ng sá»‘ dÃ²ng sau khi ghÃ©p: {df_school_bronze.count()}")

    # Lá»c unique theo 3 cá»™t
    print("\nğŸ”„ Äang lá»c dá»¯ liá»‡u unique...")
    df_school_bronze = df_school_bronze.dropDuplicates(["TenTruong", "MaTruong", "TinhThanh"])

    print(f"âœ… Sá»‘ dÃ²ng sau khi lá»c unique: {df_school_bronze.count()}")
    print("\nğŸ“Š Schema cá»§a dá»¯ liá»‡u Bronze:")
    df_school_bronze.printSchema()
    print("\nğŸ” Xem 10 dÃ²ng Ä‘áº§u tiÃªn:")
    df_school_bronze.show(10, truncate=False)
else:
    print("âŒ KhÃ´ng Ä‘á»c Ä‘Æ°á»£c dá»¯ liá»‡u tá»« báº¥t ká»³ file nÃ o!")


# ## 3. Xá»­ LÃ½ vÃ  Transform Dá»¯ Liá»‡u School

# In[3]:


# Transform dá»¯ liá»‡u Ä‘á»ƒ phÃ¹ há»£p vá»›i schema Silver
print("ğŸ”„ Äang xá»­ lÃ½ dá»¯ liá»‡u School...")

# ThÃªm timestamp cho created_at vÃ  updated_at
current_ts = current_timestamp()

df_school_silver = df_school_bronze.select(
    col("MaTruong").cast("string").alias("schoolId"),
    col("TenTruong").cast("string").alias("schoolName"),
    col("TinhThanh").cast("string").alias("province"),
    current_ts.alias("created_at"),
    current_ts.alias("updated_at")
)

# LÃ m sáº¡ch dá»¯ liá»‡u: loáº¡i bá» cÃ¡c dÃ²ng cÃ³ giÃ¡ trá»‹ null á»Ÿ cÃ¡c cá»™t quan trá»ng
df_school_silver = df_school_silver.filter(
    col("schoolId").isNotNull() & 
    col("schoolName").isNotNull()
)

print(f"âœ… ÄÃ£ xá»­ lÃ½ xong {df_school_silver.count()} dÃ²ng dá»¯ liá»‡u")
print("\nğŸ“Š Schema cá»§a dá»¯ liá»‡u Silver:")
df_school_silver.printSchema()
print("\nğŸ” Xem 10 dÃ²ng sau khi xá»­ lÃ½:")
df_school_silver.show(10, truncate=False)


# ## 4. Load Dá»¯ Liá»‡u vÃ o Báº£ng School trong Silver Layer

# In[4]:


# Load dá»¯ liá»‡u vÃ o báº£ng Iceberg trong Silver layer
print("ğŸ’¾ Äang ghi dá»¯ liá»‡u vÃ o báº£ng nessie.silver_tables.school...")

try:
    # Ghi dá»¯ liá»‡u vÃ o báº£ng Iceberg vá»›i mode append hoáº·c overwrite
    df_school_silver.writeTo("nessie.silver_tables.school") \
        .using("iceberg") \
        .createOrReplace()

    print("âœ… ÄÃ£ ghi dá»¯ liá»‡u thÃ nh cÃ´ng vÃ o báº£ng school!")

except Exception as e:
    print(f"âŒ Lá»—i khi ghi dá»¯ liá»‡u: {str(e)}")


# ## 5. Kiá»ƒm Tra Dá»¯ Liá»‡u ÄÃ£ Load vÃ o Silver Layer

# In[5]:


# Äá»c vÃ  kiá»ƒm tra dá»¯ liá»‡u tá»« báº£ng Silver
print("ğŸ” Kiá»ƒm tra dá»¯ liá»‡u trong báº£ng nessie.silver_tables.school...")

try:
    df_verify = spark.table("nessie.silver_tables.school")

    print(f"\nğŸ“Š Tá»•ng sá»‘ dÃ²ng trong báº£ng: {df_verify.count()}")
    print("\nğŸ” Schema cá»§a báº£ng:")
    df_verify.printSchema()

    print("\nğŸ” 10 dÃ²ng Ä‘áº§u tiÃªn:")
    df_verify.show(10, truncate=False)

    print("\nğŸ“ˆ Thá»‘ng kÃª theo tá»‰nh/thÃ nh phá»‘:")
    df_verify.groupBy("province").count().orderBy(desc("count")).show(10, truncate=False)

except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c dá»¯ liá»‡u: {str(e)}")


# ---
# 
# # LOAD Dá»® LIá»†U Báº¢NG MAJOR (NGÃ€NH Há»ŒC)
# 
# ---

# ## 6. Load Dá»¯ Liá»‡u Major tá»« Bronze Layer

# In[6]:


# ÄÆ°á»ng dáº«n Ä‘áº¿n file CSV trong Bronze layer trÃªn MinIO
bronze_major_path = "s3a://bronze/structured_data/danh sÃ¡ch cÃ¡c nhÃ³m ngÃ nh Ä‘áº¡i há»c/Danh_sÃ¡ch_cÃ¡c_ngÃ nh_theo_nhÃ³m_ngÃ nh.csv"

# Äá»c dá»¯ liá»‡u tá»« Bronze layer
print("ğŸ“– Äang Ä‘á»c dá»¯ liá»‡u Major tá»« Bronze layer...")

try:
    df_major_bronze = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .option("encoding", "UTF-8") \
        .csv(bronze_major_path)

    print(f"âœ… ÄÃ£ Ä‘á»c Ä‘Æ°á»£c {df_major_bronze.count()} dÃ²ng dá»¯ liá»‡u")
    print("\nğŸ“Š Schema cá»§a dá»¯ liá»‡u Bronze:")
    df_major_bronze.printSchema()
    print("\nğŸ” Xem 10 dÃ²ng Ä‘áº§u tiÃªn:")
    df_major_bronze.show(10, truncate=False)

    # Xem cÃ¡c cá»™t cÃ³ trong file
    print("\nğŸ“‹ CÃ¡c cá»™t trong file:")
    print(df_major_bronze.columns)

except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 7. Xá»­ LÃ½ vÃ  Transform Dá»¯ Liá»‡u Major

# In[10]:


# Transform dá»¯ liá»‡u Ä‘á»ƒ phÃ¹ há»£p vá»›i schema Silver
print("ğŸ”„ Äang xá»­ lÃ½ dá»¯ liá»‡u Major...")

try:
    # ThÃªm timestamp cho created_at vÃ  updated_at
    current_ts = current_timestamp()

    # Giáº£ sá»­ file cÃ³ cÃ¡c cá»™t: MaNganh, TenNganh, MaNhomNganh
    # Äiá»u chá»‰nh tÃªn cá»™t theo file thá»±c táº¿ cá»§a báº¡n
    df_major_silver = df_major_bronze.select(
        col(df_major_bronze.columns[0]).cast("string").alias("majorId"),          # Cá»™t Ä‘áº§u tiÃªn: MÃ£ ngÃ nh
        col(df_major_bronze.columns[1]).cast("string").alias("majorName"),        # Cá»™t thá»© 2: TÃªn ngÃ nh
        col(df_major_bronze.columns[2]).cast("int").alias("majorGroupId"),        # Cá»™t thá»© 3: MÃ£ nhÃ³m ngÃ nh
        current_ts.alias("created_at"),
        current_ts.alias("updated_at")
    )

    # LÃ m sáº¡ch dá»¯ liá»‡u: loáº¡i bá» cÃ¡c dÃ²ng cÃ³ giÃ¡ trá»‹ null á»Ÿ cÃ¡c cá»™t quan trá»ng
    df_major_silver = df_major_silver.filter(
        col("majorId").isNotNull() & 
        col("majorName").isNotNull() &
        col("majorGroupId").isNotNull()
    )

    # Lá»c unique theo majorId vÃ  majorName
    print("\nğŸ”„ Äang lá»c dá»¯ liá»‡u unique...")
    df_major_silver = df_major_silver.dropDuplicates(["majorId", "majorName"])

    print(f"âœ… ÄÃ£ xá»­ lÃ½ xong {df_major_silver.count()} dÃ²ng dá»¯ liá»‡u")
    print("\nğŸ“Š Schema cá»§a dá»¯ liá»‡u Silver:")
    df_major_silver.printSchema()
    print("\nğŸ” Xem 10 dÃ²ng sau khi xá»­ lÃ½:")
    df_major_silver.show(10, truncate=False)

    # Thá»‘ng kÃª theo nhÃ³m ngÃ nh
    print("\nğŸ“ˆ Thá»‘ng kÃª theo nhÃ³m ngÃ nh:")
    df_major_silver.groupBy("majorGroupId").count().orderBy("majorGroupId").show(truncate=False)

except Exception as e:
    print(f"âŒ Lá»—i khi xá»­ lÃ½ dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 8. Load Dá»¯ Liá»‡u vÃ o Báº£ng Major trong Silver Layer

# In[11]:


# Load dá»¯ liá»‡u vÃ o báº£ng Iceberg trong Silver layer
print("ğŸ’¾ Äang ghi dá»¯ liá»‡u vÃ o báº£ng nessie.silver_tables.major...")

try:
    # Ghi dá»¯ liá»‡u vÃ o báº£ng Iceberg vá»›i mode append hoáº·c overwrite
    df_major_silver.writeTo("nessie.silver_tables.major") \
        .using("iceberg") \
        .createOrReplace()

    print("âœ… ÄÃ£ ghi dá»¯ liá»‡u thÃ nh cÃ´ng vÃ o báº£ng major!")
    print(f"ğŸ“Š Tá»•ng sá»‘ dÃ²ng Ä‘Ã£ ghi: {df_major_silver.count()}")

except Exception as e:
    print(f"âŒ Lá»—i khi ghi dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 9. Kiá»ƒm Tra Dá»¯ Liá»‡u Major ÄÃ£ Load vÃ o Silver Layer

# In[12]:


# Äá»c vÃ  kiá»ƒm tra dá»¯ liá»‡u tá»« báº£ng Silver
print("ğŸ” Kiá»ƒm tra dá»¯ liá»‡u trong báº£ng nessie.silver_tables.major...")

try:
    df_verify_major = spark.table("nessie.silver_tables.major")

    print(f"\nğŸ“Š Tá»•ng sá»‘ dÃ²ng trong báº£ng: {df_verify_major.count()}")
    print("\nğŸ” Schema cá»§a báº£ng:")
    df_verify_major.printSchema()

    print("\nğŸ” 10 dÃ²ng Ä‘áº§u tiÃªn:")
    df_verify_major.show(10, truncate=False)

    print("\nğŸ“ˆ Thá»‘ng kÃª theo nhÃ³m ngÃ nh:")
    df_verify_major.groupBy("majorGroupId").count().orderBy("majorGroupId").show(20, truncate=False)

    print("\nğŸ” Sample má»™t vÃ i ngÃ nh:")
    df_verify_major.select("majorId", "majorName", "majorGroupId").show(20, truncate=False)

except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# In[ ]:





# ---
# 
# # LOAD Dá»® LIá»†U Báº¢NG MAJOR_GROUP (NHÃ“M NGÃ€NH)
# 
# ---

# ## 10. Load Dá»¯ Liá»‡u Major Group tá»« Bronze Layer

# In[13]:


# ÄÆ°á»ng dáº«n Ä‘áº¿n file CSV trong Bronze layer trÃªn MinIO
bronze_major_group_path = "s3a://bronze/structured_data/danh sÃ¡ch cÃ¡c nhÃ³m ngÃ nh Ä‘áº¡i há»c/Danh_sÃ¡ch_cÃ¡c_nhÃ³m_ngÃ nh_Ä‘áº¡i_há»c.csv"

# Äá»c dá»¯ liá»‡u tá»« Bronze layer
print("ğŸ“– Äang Ä‘á»c dá»¯ liá»‡u Major Group tá»« Bronze layer...")

try:
    df_major_group_bronze = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .option("encoding", "UTF-8") \
        .csv(bronze_major_group_path)

    print(f"âœ… ÄÃ£ Ä‘á»c Ä‘Æ°á»£c {df_major_group_bronze.count()} dÃ²ng dá»¯ liá»‡u")
    print("\nğŸ“Š Schema cá»§a dá»¯ liá»‡u Bronze:")
    df_major_group_bronze.printSchema()
    print("\nğŸ” Xem táº¥t cáº£ dá»¯ liá»‡u:")
    df_major_group_bronze.show(100, truncate=False)

    # Xem cÃ¡c cá»™t cÃ³ trong file
    print("\nğŸ“‹ CÃ¡c cá»™t trong file:")
    print(df_major_group_bronze.columns)

except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 11. Xá»­ LÃ½ vÃ  Transform Dá»¯ Liá»‡u Major Group

# In[14]:


# Transform dá»¯ liá»‡u Ä‘á»ƒ phÃ¹ há»£p vá»›i schema Silver
print("ğŸ”„ Äang xá»­ lÃ½ dá»¯ liá»‡u Major Group...")

try:
    # ThÃªm timestamp cho created_at vÃ  updated_at
    current_ts = current_timestamp()

    # Giáº£ sá»­ file cÃ³ cÃ¡c cá»™t: MaNhomNganh, TenNhomNganh
    # Äiá»u chá»‰nh tÃªn cá»™t theo file thá»±c táº¿ cá»§a báº¡n
    df_major_group_silver = df_major_group_bronze.select(
        col(df_major_group_bronze.columns[2]).cast("int").alias("majorGroupId"),      # Cá»™t Ä‘áº§u tiÃªn: MÃ£ nhÃ³m ngÃ nh
        col(df_major_group_bronze.columns[1]).cast("string").alias("majorGroupName"), # Cá»™t thá»© 2: TÃªn nhÃ³m ngÃ nh
        current_ts.alias("created_at"),
        current_ts.alias("updated_at")
    )

    # LÃ m sáº¡ch dá»¯ liá»‡u: loáº¡i bá» cÃ¡c dÃ²ng cÃ³ giÃ¡ trá»‹ null á»Ÿ cÃ¡c cá»™t quan trá»ng
    df_major_group_silver = df_major_group_silver.filter(
        col("majorGroupId").isNotNull() & 
        col("majorGroupName").isNotNull()
    )

    # Lá»c unique theo majorGroupId
    print("\nğŸ”„ Äang lá»c dá»¯ liá»‡u unique...")
    df_major_group_silver = df_major_group_silver.dropDuplicates(["majorGroupId"])

    print(f"âœ… ÄÃ£ xá»­ lÃ½ xong {df_major_group_silver.count()} dÃ²ng dá»¯ liá»‡u")
    print("\nğŸ“Š Schema cá»§a dá»¯ liá»‡u Silver:")
    df_major_group_silver.printSchema()
    print("\nğŸ” Xem táº¥t cáº£ dá»¯ liá»‡u sau khi xá»­ lÃ½:")
    df_major_group_silver.show(100, truncate=False)

except Exception as e:
    print(f"âŒ Lá»—i khi xá»­ lÃ½ dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 12. Load Dá»¯ Liá»‡u vÃ o Báº£ng Major Group trong Silver Layer

# In[15]:


# Load dá»¯ liá»‡u vÃ o báº£ng Iceberg trong Silver layer
print("ğŸ’¾ Äang ghi dá»¯ liá»‡u vÃ o báº£ng nessie.silver_tables.major_group...")

try:
    df_major_group_silver.writeTo("nessie.silver_tables.major_group") \
    .using("iceberg") \
    .createOrReplace()

    print(" ÄÃ£ ghi dá»¯ liá»‡u thÃ nh cÃ´ng vÃ o báº£ng major_group!")
    print(f" Tá»•ng sá»‘ dÃ²ng Ä‘Ã£ ghi: {df_major_group_silver.count()}")

except Exception as e:
    print(f"âŒ Lá»—i khi ghi dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 13. Kiá»ƒm Tra Dá»¯ Liá»‡u Major Group ÄÃ£ Load vÃ o Silver Layer

# In[16]:


# Äá»c vÃ  kiá»ƒm tra dá»¯ liá»‡u tá»« báº£ng Silver
print("ğŸ” Kiá»ƒm tra dá»¯ liá»‡u trong báº£ng nessie.silver_tables.major_group...")

try:
    df_verify_major_group = spark.table("nessie.silver_tables.major_group")

    print(f"\nğŸ“Š Tá»•ng sá»‘ dÃ²ng trong báº£ng: {df_verify_major_group.count()}")
    print("\nğŸ” Schema cá»§a báº£ng:")
    df_verify_major_group.printSchema()

    print("\nğŸ” Táº¥t cáº£ nhÃ³m ngÃ nh:")
    df_verify_major_group.orderBy("majorGroupId").show(100, truncate=False)

    # Kiá»ƒm tra xem cÃ³ nhÃ³m ngÃ nh nÃ o bá»‹ thiáº¿u khÃ´ng
    print("\nğŸ“Š Sá»‘ lÆ°á»£ng nhÃ³m ngÃ nh:")
    print(f"Total: {df_verify_major_group.count()} nhÃ³m ngÃ nh")

except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# In[ ]:





# ---
# 
# # LOAD Dá»® LIá»†U Báº¢NG SUBJECT_GROUP (NHÃ“M MÃ”N/KHá»I THI)
# 
# ---

# ## 14. Load Dá»¯ Liá»‡u Subject Group tá»« Bronze Layer

# In[27]:


# ÄÆ°á»ng dáº«n Ä‘áº¿n file CSV trong Bronze layer trÃªn MinIO
bronze_subject_group_path = "s3a://bronze/structured_data/tohop_mon.csv"

print("ğŸ“– Äang Ä‘á»c dá»¯ liá»‡u Subject Group tá»« Bronze layer...")

try:
    df_subject_group_bronze = (
        spark.read
        .format("csv")
        .option("header", "true")           # CÃ³ dÃ²ng tiÃªu Ä‘á»
        .option("inferSchema", "true")      # Tá»± Ä‘á»™ng suy luáº­n kiá»ƒu dá»¯ liá»‡u
        .option("encoding", "UTF-8")        # Äá»c tiáº¿ng Viá»‡t
        .csv(bronze_subject_group_path)
    )


    print(f"âœ… ÄÃ£ Ä‘á»c Ä‘Æ°á»£c {df_subject_group_bronze.count()} dÃ²ng dá»¯ liá»‡u")
    print("\nğŸ“Š Schema cá»§a dá»¯ liá»‡u Bronze:")
    df_subject_group_bronze.printSchema()

    print("\nğŸ” Xem dá»¯ liá»‡u máº«u:")
    df_subject_group_bronze.show(truncate=False)

    print("\nğŸ“‹ CÃ¡c cá»™t trong file:")
    print(df_subject_group_bronze.columns)

except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# In[30]:


# Transform dá»¯ liá»‡u Ä‘á»ƒ phÃ¹ há»£p vá»›i schema Silver
print("ğŸ”„ Äang xá»­ lÃ½ dá»¯ liá»‡u Subject Group...")

try:
    # ThÃªm timestamp cho created_at vÃ  updated_at
    current_ts = current_timestamp()

    # File cÃ³ 3 cá»™t: STT, Tá»• há»£p, MÃ´n chi tiáº¿t
    # Column 0: STT (subjectGroupId)
    # Column 1: Tá»• há»£p (subjectGroupName) - VD: "D01"
    # Column 2: MÃ´n chi tiáº¿t (subjectCombination) - VD: "ToÃ¡n, Ngá»¯ VÄƒn, Tiáº¿ng Anh"

    df_subject_group_silver = df_subject_group_bronze.select(
        col(df_subject_group_bronze.columns[0]).cast("int").alias("subjectGroupId"),           # STT
        col(df_subject_group_bronze.columns[1]).cast("string").alias("subjectGroupName"),      # Tá»• há»£p (VD: D01, A00, A01)
        col(df_subject_group_bronze.columns[2]).cast("string").alias("subjectCombination"),    # MÃ´n chi tiáº¿t
        current_ts.alias("created_at"),
        current_ts.alias("updated_at")
    )

    # LÃ m sáº¡ch dá»¯ liá»‡u: loáº¡i bá» cÃ¡c dÃ²ng cÃ³ giÃ¡ trá»‹ null á»Ÿ cÃ¡c cá»™t quan trá»ng
    df_subject_group_silver = df_subject_group_silver.filter(
        col("subjectGroupId").isNotNull() & 
        col("subjectGroupName").isNotNull() &
        col("subjectCombination").isNotNull()
    )

    # Lá»c unique theo subjectGroupName
    print("\nğŸ”„ Äang lá»c dá»¯ liá»‡u unique...")
    df_subject_group_silver = df_subject_group_silver.dropDuplicates(["subjectGroupName","subjectCombination"])

    print(f"âœ… ÄÃ£ xá»­ lÃ½ xong {df_subject_group_silver.count()} dÃ²ng dá»¯ liá»‡u")
    print("\nğŸ“Š Schema cá»§a dá»¯ liá»‡u Silver:")
    df_subject_group_silver.printSchema()
    print("\nğŸ” Xem 20 dÃ²ng Ä‘áº§u tiÃªn sau khi xá»­ lÃ½:")
    df_subject_group_silver.show(20, truncate=False)

    # Thá»‘ng kÃª
    print("\nğŸ“Š Thá»‘ng kÃª:")
    print(f"Tá»•ng sá»‘ tá»• há»£p mÃ´n: {df_subject_group_silver.count()}")

except Exception as e:
    print(f"âŒ Lá»—i khi xá»­ lÃ½ dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 16. Load Dá»¯ Liá»‡u vÃ o Báº£ng Subject Group trong Silver Layer

# In[31]:


# Load dá»¯ liá»‡u vÃ o báº£ng Iceberg trong Silver layer
print("ğŸ’¾ Äang ghi dá»¯ liá»‡u vÃ o báº£ng nessie.silver_tables.subject_group...")

try:
    df_subject_group_silver.writeTo("nessie.silver_tables.subject_group") \
    .using("iceberg") \
    .createOrReplace()

    print("âœ… ÄÃ£ ghi dá»¯ liá»‡u thÃ nh cÃ´ng vÃ o báº£ng subject_group!")
    print(f"ğŸ“Š Tá»•ng sá»‘ dÃ²ng Ä‘Ã£ ghi: {df_subject_group_silver.count()}")

except Exception as e:
    print(f"âŒ Lá»—i khi ghi dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 17. Kiá»ƒm Tra Dá»¯ Liá»‡u Subject Group ÄÃ£ Load vÃ o Silver Layer

# In[32]:


# Äá»c vÃ  kiá»ƒm tra dá»¯ liá»‡u tá»« báº£ng Silver
print("ğŸ” Kiá»ƒm tra dá»¯ liá»‡u trong báº£ng nessie.silver_tables.subject_group...")

try:
    df_verify_subject_group = spark.table("nessie.silver_tables.subject_group")

    print(f"\nğŸ“Š Tá»•ng sá»‘ dÃ²ng trong báº£ng: {df_verify_subject_group.count()}")
    print("\nğŸ” Schema cá»§a báº£ng:")
    df_verify_subject_group.printSchema()

    print("\nğŸ” 20 tá»• há»£p mÃ´n Ä‘áº§u tiÃªn:")
    df_verify_subject_group.orderBy("subjectGroupId").show(20, truncate=False)

    print("\nğŸ” Má»™t sá»‘ tá»• há»£p mÃ´n cá»¥ thá»ƒ:")
    df_verify_subject_group.filter(col("subjectGroupName").isin(["A00", "A01", "D01", "C00"])).show(truncate=False)

    # Thá»‘ng kÃª
    print("\nğŸ“Š Thá»‘ng kÃª:")
    print(f"Tá»•ng sá»‘ tá»• há»£p mÃ´n: {df_verify_subject_group.count()}")

except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 18. Load Dá»¯ Liá»‡u Selection Method tá»« Bronze Layer

# In[33]:


# ÄÆ°á»ng dáº«n Ä‘áº¿n file CSV Ä‘iá»ƒm chuáº©n trong Bronze layer trÃªn MinIO
# File nÃ y chá»©a thÃ´ng tin vá» phÆ°Æ¡ng thá»©c xÃ©t tuyá»ƒn trong cá»™t PhuongThuc
bronze_benchmark_path = "s3a://bronze/structured_data/Ä‘iá»ƒm chuáº©n cÃ¡c trÆ°á»ng (2021-2025)/Äiá»ƒm_chuáº©n_cÃ¡c_ngÃ nh_Ä‘áº¡i_há»c_nÄƒm(2021-2025)*.csv"

# Äá»c dá»¯ liá»‡u tá»« Bronze layer
print("ğŸ“– Äang Ä‘á»c dá»¯ liá»‡u Ä‘iá»ƒm chuáº©n Ä‘á»ƒ láº¥y Selection Method tá»« Bronze layer...")

try:
    df_benchmark_bronze = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .option("encoding", "UTF-8") \
        .csv(bronze_benchmark_path)

    print(f"âœ… ÄÃ£ Ä‘á»c Ä‘Æ°á»£c {df_benchmark_bronze.count()} dÃ²ng dá»¯ liá»‡u")
    print("\nğŸ“Š Schema cá»§a dá»¯ liá»‡u Bronze:")
    df_benchmark_bronze.printSchema()

    # Xem cÃ¡c giÃ¡ trá»‹ trong cá»™t PhuongThuc
    print("\nğŸ” CÃ¡c giÃ¡ trá»‹ trong cá»™t PhuongThuc:")
    df_benchmark_bronze.select("PhuongThuc").distinct().show(50, truncate=False)

except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 19. Xá»­ LÃ½ vÃ  Transform Dá»¯ Liá»‡u Selection Method

# In[34]:


from pyspark.sql.functions import regexp_replace, trim, row_number
from pyspark.sql.window import Window

# Transform dá»¯ liá»‡u Ä‘á»ƒ phÃ¹ há»£p vá»›i schema Silver
print("ğŸ”„ Äang xá»­ lÃ½ dá»¯ liá»‡u Selection Method...")

try:
    # ThÃªm timestamp cho created_at vÃ  updated_at
    current_ts = current_timestamp()

    # Láº¥y cá»™t PhuongThuc vÃ  loáº¡i bá» pháº§n "nÄƒm ..." 
    # VD: "XÃ©t tuyá»ƒn theo Ä‘iá»ƒm thi THPT nÄƒm 2023" -> "XÃ©t tuyá»ƒn theo Ä‘iá»ƒm thi THPT"
    df_selection_method = df_benchmark_bronze.select(
        regexp_replace(col("PhuongThuc"), r"\s*nÄƒm\s+\d{4}.*$", "").alias("selectionMethodName_raw")
    ).distinct()

    # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a vÃ  filter null
    df_selection_method = df_selection_method.withColumn(
        "selectionMethodName",
        trim(col("selectionMethodName_raw"))
    ).filter(
        col("selectionMethodName").isNotNull() & 
        (col("selectionMethodName") != "")
    ).select("selectionMethodName").distinct()

    # Táº¡o selectionMethodId tá»± Ä‘á»™ng báº±ng row_number
    window_spec = Window.orderBy("selectionMethodName")
    df_selection_method_silver = df_selection_method.withColumn(
        "selectionMethodId",
        row_number().over(window_spec)
    ).select(
        col("selectionMethodId").cast("int"),
        col("selectionMethodName").cast("string"),
        current_ts.alias("created_at"),
        current_ts.alias("updated_at")
    )

    print(f"âœ… ÄÃ£ xá»­ lÃ½ xong {df_selection_method_silver.count()} phÆ°Æ¡ng thá»©c xÃ©t tuyá»ƒn")
    print("\nğŸ“Š Schema cá»§a dá»¯ liá»‡u Silver:")
    df_selection_method_silver.printSchema()
    print("\nğŸ” Táº¥t cáº£ cÃ¡c phÆ°Æ¡ng thá»©c xÃ©t tuyá»ƒn sau khi xá»­ lÃ½:")
    df_selection_method_silver.orderBy("selectionMethodId").show(100, truncate=False)

except Exception as e:
    print(f"âŒ Lá»—i khi xá»­ lÃ½ dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 20. Load Dá»¯ Liá»‡u vÃ o Báº£ng Selection Method trong Silver Layer

# In[35]:


# Load dá»¯ liá»‡u vÃ o báº£ng Iceberg trong Silver layer
print("ğŸ’¾ Äang ghi dá»¯ liá»‡u vÃ o báº£ng nessie.silver_tables.selection_method...")

try:
    df_selection_method_silver.writeTo("nessie.silver_tables.selection_method") \
        .using("iceberg") \
        .createOrReplace()

    print("âœ… ÄÃ£ ghi dá»¯ liá»‡u thÃ nh cÃ´ng vÃ o báº£ng selection_method!")
    print(f"ğŸ“Š Tá»•ng sá»‘ dÃ²ng Ä‘Ã£ ghi: {df_selection_method_silver.count()}")

except Exception as e:
    print(f"âŒ Lá»—i khi ghi dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 21. Kiá»ƒm Tra Dá»¯ Liá»‡u Selection Method ÄÃ£ Load vÃ o Silver Layer

# In[36]:


# Äá»c vÃ  kiá»ƒm tra dá»¯ liá»‡u tá»« báº£ng Silver
print("ğŸ” Kiá»ƒm tra dá»¯ liá»‡u trong báº£ng nessie.silver_tables.selection_method...")

try:
    df_verify_selection_method = spark.table("nessie.silver_tables.selection_method")

    print(f"\nğŸ“Š Tá»•ng sá»‘ dÃ²ng trong báº£ng: {df_verify_selection_method.count()}")
    print("\nğŸ” Schema cá»§a báº£ng:")
    df_verify_selection_method.printSchema()

    print("\nğŸ” Táº¥t cáº£ cÃ¡c phÆ°Æ¡ng thá»©c xÃ©t tuyá»ƒn:")
    df_verify_selection_method.orderBy("selectionMethodId").show(100, truncate=False)

    # Thá»‘ng kÃª
    print("\nğŸ“Š Thá»‘ng kÃª:")
    print(f"Tá»•ng sá»‘ phÆ°Æ¡ng thá»©c xÃ©t tuyá»ƒn: {df_verify_selection_method.count()}")

except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# In[ ]:





# ---
# 
# # LOAD Dá»® LIá»†U Báº¢NG BENCHMARK (ÄIá»‚M CHUáº¨N)
# 
# ---

# ## 22. Load Dá»¯ Liá»‡u Benchmark tá»« Bronze Layer

# In[37]:


# ÄÆ°á»ng dáº«n Ä‘áº¿n file CSV Ä‘iá»ƒm chuáº©n trong Bronze layer trÃªn MinIO
bronze_benchmark_path = "s3a://bronze/structured_data/Ä‘iá»ƒm chuáº©n cÃ¡c trÆ°á»ng (2021-2025)/Äiá»ƒm_chuáº©n_cÃ¡c_ngÃ nh_Ä‘áº¡i_há»c_nÄƒm(2021-2025)*.csv"

# Äá»c dá»¯ liá»‡u tá»« Bronze layer
print("ğŸ“– Äang Ä‘á»c dá»¯ liá»‡u Benchmark tá»« Bronze layer...")

try:
    df_benchmark_raw = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .option("encoding", "UTF-8") \
        .csv(bronze_benchmark_path)

    print(f"âœ… ÄÃ£ Ä‘á»c Ä‘Æ°á»£c {df_benchmark_raw.count()} dÃ²ng dá»¯ liá»‡u")
    print("\nğŸ“Š Schema cá»§a dá»¯ liá»‡u Bronze:")
    df_benchmark_raw.printSchema()

    print("\nğŸ“‹ CÃ¡c cá»™t trong file:")
    print(df_benchmark_raw.columns)

    print("\nğŸ” Xem 10 dÃ²ng Ä‘áº§u tiÃªn:")
    df_benchmark_raw.show(10, truncate=False)

except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 23. Xá»­ LÃ½ vÃ  Transform Dá»¯ Liá»‡u Benchmark

# In[38]:


from pyspark.sql.functions import regexp_replace, trim, monotonically_increasing_id, lit

# Transform dá»¯ liá»‡u Ä‘á»ƒ phÃ¹ há»£p vá»›i schema Silver
print("ğŸ”„ Äang xá»­ lÃ½ dá»¯ liá»‡u Benchmark...")

try:
    # ThÃªm timestamp cho created_at vÃ  updated_at
    current_ts = current_timestamp()

    # CÃ¡c cá»™t trong file CSV:
    # STT, Nam, MaTruong, PhuongThuc, MaNganh, TenNganh, KhoiThi, DiemChuan, GhiChu

    # BÆ°á»›c 1: Xá»­ lÃ½ PhuongThuc - loáº¡i bá» pháº§n "nÄƒm..."
    df_benchmark_processed = df_benchmark_raw.withColumn(
        "PhuongThuc_cleaned",
        trim(regexp_replace(col("PhuongThuc"), r"\s*nÄƒm\s+\d{4}.*$", ""))
    )

    # BÆ°á»›c 2: Join vá»›i báº£ng selection_method Ä‘á»ƒ láº¥y selectionMethodId
    df_selection_method_lookup = spark.table("nessie.silver_tables.selection_method")

    df_benchmark_with_method = df_benchmark_processed.join(
        df_selection_method_lookup,
        df_benchmark_processed["PhuongThuc_cleaned"] == df_selection_method_lookup["selectionMethodName"],
        "left"
    )

    # BÆ°á»›c 3: Láº¥y subjectGroupId tá»« báº£ng subject_group dá»±a trÃªn KhoiThi
    df_subject_group_lookup = spark.table("nessie.silver_tables.subject_group")

    df_benchmark_with_subject = df_benchmark_with_method.join(
        df_subject_group_lookup,
        df_benchmark_with_method["KhoiThi"] == df_subject_group_lookup["subjectGroupName"],
        "left"
    )

    # BÆ°á»›c 4: Select vÃ  cast cÃ¡c cá»™t theo schema Silver
    df_benchmark_silver = df_benchmark_with_subject.select(
        col("MaTruong").cast("string").alias("schoolId"),
        col("MaNganh").cast("string").alias("majorId"),
        col("subjectGroupId").cast("int"),
        col("selectionMethodId").cast("int"),
        col("Nam").cast("int").alias("year"),
        col("DiemChuan").cast("double").alias("score"),
        current_ts.alias("created_at"),
        current_ts.alias("updated_at")
    )

    # BÆ°á»›c 5: LÃ m sáº¡ch dá»¯ liá»‡u - loáº¡i bá» cÃ¡c dÃ²ng cÃ³ giÃ¡ trá»‹ null á»Ÿ cÃ¡c cá»™t quan trá»ng
    df_benchmark_silver = df_benchmark_silver.filter(
        col("schoolId").isNotNull() & 
        col("majorId").isNotNull() &
        col("year").isNotNull() &
        col("score").isNotNull() &
        col("selectionMethodId").isNotNull() &
        col("subjectGroupId").isNotNull()
    )

    # BÆ°á»›c 6: ThÃªm benchmarkId tá»± Ä‘á»™ng tÄƒng
    df_benchmark_silver = df_benchmark_silver.withColumn(
        "benchmarkId",
        monotonically_increasing_id().cast("int")
    ).select(
        "benchmarkId",
        "schoolId",
        "majorId",
        "subjectGroupId",
        "selectionMethodId",
        "year",
        "score",
        "created_at",
        "updated_at"
    )

    # BÆ°á»›c 7: Lá»c unique Ä‘á»ƒ trÃ¡nh trÃ¹ng láº·p
    print("\nğŸ”„ Äang lá»c dá»¯ liá»‡u unique...")
    df_benchmark_silver = df_benchmark_silver.dropDuplicates([
        "schoolId", "majorId", "subjectGroupId", "selectionMethodId", "year"
    ])

    print(f"âœ… ÄÃ£ xá»­ lÃ½ xong {df_benchmark_silver.count()} dÃ²ng dá»¯ liá»‡u")
    print("\nğŸ“Š Schema cá»§a dá»¯ liá»‡u Silver:")
    df_benchmark_silver.printSchema()

    print("\nğŸ” Xem 20 dÃ²ng Ä‘áº§u tiÃªn sau khi xá»­ lÃ½:")
    df_benchmark_silver.orderBy("year", "schoolId").show(20, truncate=False)

    # Thá»‘ng kÃª theo nÄƒm
    print("\nğŸ“Š Thá»‘ng kÃª theo nÄƒm:")
    df_benchmark_silver.groupBy("year").count().orderBy("year").show()

    # Thá»‘ng kÃª Ä‘iá»ƒm chuáº©n
    print("\nğŸ“Š Thá»‘ng kÃª Ä‘iá»ƒm chuáº©n:")
    df_benchmark_silver.select(
        avg("score").alias("Äiá»ƒm TB"),
        min("score").alias("Äiá»ƒm Min"),
        max("score").alias("Äiá»ƒm Max")
    ).show()

except Exception as e:
    print(f"âŒ Lá»—i khi xá»­ lÃ½ dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 24. Load Dá»¯ Liá»‡u vÃ o Báº£ng Benchmark trong Silver Layer

# In[39]:


# Load dá»¯ liá»‡u vÃ o báº£ng Iceberg trong Silver layer
print("ğŸ’¾ Äang ghi dá»¯ liá»‡u vÃ o báº£ng nessie.silver_tables.benchmark...")

try:
    # Ghi dá»¯ liá»‡u vÃ o báº£ng Iceberg vá»›i partitioning theo nÄƒm
    df_benchmark_silver.writeTo("nessie.silver_tables.benchmark") \
        .using("iceberg") \
        .createOrReplace()

    print("âœ… ÄÃ£ ghi dá»¯ liá»‡u thÃ nh cÃ´ng vÃ o báº£ng benchmark!")
    print(f"ğŸ“Š Tá»•ng sá»‘ dÃ²ng Ä‘Ã£ ghi: {df_benchmark_silver.count()}")

except Exception as e:
    print(f"âŒ Lá»—i khi ghi dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 25. Kiá»ƒm Tra Dá»¯ Liá»‡u Benchmark ÄÃ£ Load vÃ o Silver Layer

# In[40]:


# Äá»c vÃ  kiá»ƒm tra dá»¯ liá»‡u tá»« báº£ng Silver
print("ğŸ” Kiá»ƒm tra dá»¯ liá»‡u trong báº£ng nessie.silver_tables.benchmark...")

try:
    df_verify_benchmark = spark.table("nessie.silver_tables.benchmark")

    print(f"\nğŸ“Š Tá»•ng sá»‘ dÃ²ng trong báº£ng: {df_verify_benchmark.count()}")
    print("\nğŸ” Schema cá»§a báº£ng:")
    df_verify_benchmark.printSchema()

    print("\nğŸ” 20 dÃ²ng Ä‘áº§u tiÃªn:")
    df_verify_benchmark.orderBy("year", "score").show(20, truncate=False)

    print("\nğŸ“ˆ Thá»‘ng kÃª theo nÄƒm:")
    df_verify_benchmark.groupBy("year") \
        .agg(
            count("*").alias("Sá»‘ lÆ°á»£ng"),
            avg("score").alias("Äiá»ƒm TB"),
            min("score").alias("Äiá»ƒm Min"),
            max("score").alias("Äiá»ƒm Max")
        ) \
        .orderBy("year") \
        .show()

    print("\nğŸ“ˆ Top 10 Ä‘iá»ƒm chuáº©n cao nháº¥t nÄƒm 2025:")
    df_verify_benchmark.filter(col("year") == 2025) \
        .orderBy(desc("score")) \
        .show(10, truncate=False)

    print("\nğŸ“Š PhÃ¢n bá»‘ theo phÆ°Æ¡ng thá»©c xÃ©t tuyá»ƒn:")
    df_verify_benchmark.groupBy("selectionMethodId").count() \
        .orderBy(desc("count")) \
        .show(10)

    print("\nğŸ“Š PhÃ¢n bá»‘ theo khá»‘i thi:")
    df_verify_benchmark.filter(col("subjectGroupId").isNotNull()) \
        .groupBy("subjectGroupId").count() \
        .orderBy(desc("count")) \
        .show(10)

    # Kiá»ƒm tra dá»¯ liá»‡u null
    print("\nâš ï¸ Kiá»ƒm tra dá»¯ liá»‡u null/missing:")
    print(f"Sá»‘ dÃ²ng cÃ³ subjectGroupId = null: {df_verify_benchmark.filter(col('subjectGroupId').isNull()).count()}")
    print(f"Sá»‘ dÃ²ng cÃ³ selectionMethodId = null: {df_verify_benchmark.filter(col('selectionMethodId').isNull()).count()}")

except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# In[ ]:





# ---
# 
# # LOAD Dá»® LIá»†U Báº¢NG REGION (KHU Vá»°C)
# 
# ---

# ## 26. Load Dá»¯ Liá»‡u Region tá»« Bronze Layer

# In[41]:


# ÄÆ°á»ng dáº«n Ä‘áº¿n file CSV region trong Bronze layer trÃªn MinIO
bronze_region_path = "s3a://bronze/structured_data/region.csv"

# Äá»c dá»¯ liá»‡u tá»« Bronze layer
print("ğŸ“– Äang Ä‘á»c dá»¯ liá»‡u Region tá»« Bronze layer...")

try:
    df_region_bronze = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .option("encoding", "UTF-8") \
        .csv(bronze_region_path)

    print(f"âœ… ÄÃ£ Ä‘á»c Ä‘Æ°á»£c {df_region_bronze.count()} dÃ²ng dá»¯ liá»‡u")
    print("\nğŸ“Š Schema cá»§a dá»¯ liá»‡u Bronze:")
    df_region_bronze.printSchema()

    print("\nğŸ“‹ CÃ¡c cá»™t trong file:")
    print(df_region_bronze.columns)

    print("\nğŸ” Xem táº¥t cáº£ dá»¯ liá»‡u:")
    df_region_bronze.show(100, truncate=False)

except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 27. Xá»­ LÃ½ vÃ  Transform Dá»¯ Liá»‡u Region

# In[42]:


# Transform dá»¯ liá»‡u Ä‘á»ƒ phÃ¹ há»£p vá»›i schema Silver
print("ğŸ”„ Äang xá»­ lÃ½ dá»¯ liá»‡u Region...")

try:
    # ThÃªm timestamp cho created_at vÃ  updated_at
    current_ts = current_timestamp()

    # File cÃ³ cÃ¡c cá»™t: regionId, regionName (hoáº·c tÆ°Æ¡ng tá»±)
    # Äiá»u chá»‰nh tÃªn cá»™t theo file thá»±c táº¿

    # Náº¿u file khÃ´ng cÃ³ tÃªn cá»™t chuáº©n, sá»­ dá»¥ng index
    df_region_silver = df_region_bronze.select(
        col(df_region_bronze.columns[0]).cast("string").alias("regionId"),
        col(df_region_bronze.columns[1]).cast("string").alias("regionName"),
        current_ts.alias("created_at"),
        current_ts.alias("updated_at")
    )

    # LÃ m sáº¡ch dá»¯ liá»‡u: loáº¡i bá» cÃ¡c dÃ²ng cÃ³ giÃ¡ trá»‹ null á»Ÿ cÃ¡c cá»™t quan trá»ng
    df_region_silver = df_region_silver.filter(
        col("regionId").isNotNull() & 
        col("regionName").isNotNull()
    )

    # Lá»c unique theo regionId
    print("\nğŸ”„ Äang lá»c dá»¯ liá»‡u unique...")
    df_region_silver = df_region_silver.dropDuplicates(["regionId"])

    print(f"âœ… ÄÃ£ xá»­ lÃ½ xong {df_region_silver.count()} dÃ²ng dá»¯ liá»‡u")
    print("\nğŸ“Š Schema cá»§a dá»¯ liá»‡u Silver:")
    df_region_silver.printSchema()

    print("\nğŸ” Táº¥t cáº£ cÃ¡c khu vá»±c sau khi xá»­ lÃ½:")
    df_region_silver.orderBy("regionId").show(100, truncate=False)

    # Thá»‘ng kÃª
    print("\nğŸ“Š Thá»‘ng kÃª:")
    print(f"Tá»•ng sá»‘ khu vá»±c: {df_region_silver.count()}")

except Exception as e:
    print(f"âŒ Lá»—i khi xá»­ lÃ½ dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 28. Load Dá»¯ Liá»‡u vÃ o Báº£ng Region trong Silver Layer

# In[43]:


# Load dá»¯ liá»‡u vÃ o báº£ng Iceberg trong Silver layer
print("ğŸ’¾ Äang ghi dá»¯ liá»‡u vÃ o báº£ng nessie.silver_tables.region...")

try:
    df_region_silver.writeTo("nessie.silver_tables.region") \
        .using("iceberg") \
        .createOrReplace()

    print("âœ… ÄÃ£ ghi dá»¯ liá»‡u thÃ nh cÃ´ng vÃ o báº£ng region!")
    print(f"ğŸ“Š Tá»•ng sá»‘ dÃ²ng Ä‘Ã£ ghi: {df_region_silver.count()}")

except Exception as e:
    print(f"âŒ Lá»—i khi ghi dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 29. Kiá»ƒm Tra Dá»¯ Liá»‡u Region ÄÃ£ Load vÃ o Silver Layer

# In[44]:


# Äá»c vÃ  kiá»ƒm tra dá»¯ liá»‡u tá»« báº£ng Silver
print("ğŸ” Kiá»ƒm tra dá»¯ liá»‡u trong báº£ng nessie.silver_tables.region...")

try:
    df_verify_region = spark.table("nessie.silver_tables.region")

    print(f"\nğŸ“Š Tá»•ng sá»‘ dÃ²ng trong báº£ng: {df_verify_region.count()}")
    print("\nğŸ” Schema cá»§a báº£ng:")
    df_verify_region.printSchema()

    print("\nğŸ” Táº¥t cáº£ cÃ¡c khu vá»±c:")
    df_verify_region.orderBy("regionId").show(100, truncate=False)

    # Thá»‘ng kÃª
    print("\nğŸ“Š Thá»‘ng kÃª:")
    print(f"Tá»•ng sá»‘ khu vá»±c: {df_verify_region.count()}")

    # LÆ°u DataFrame Ä‘á»ƒ sá»­ dá»¥ng cho viá»‡c map vá»›i student_scores sau nÃ y
    print("\nğŸ’¾ LÆ°u thÃ´ng tin region Ä‘á»ƒ sá»­ dá»¥ng cho mapping...")

except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ---
# 
# # LOAD Dá»® LIá»†U Báº¢NG STUDENT_SCORES (ÄIá»‚M THI SINH VIÃŠN)
# 
# ---

# ## 30. Load Dá»¯ Liá»‡u Student Scores tá»« Bronze Layer

# In[65]:


# ÄÆ°á»ng dáº«n Ä‘áº¿n cÃ¡c thÆ° má»¥c chá»©a file CSV Ä‘iá»ƒm thi tá»«ng nÄƒm
# Cáº¥u trÃºc: bronze/structured_data/Ä‘iá»ƒm tá»«ng thi sinh/[2021-2025]/Diem_thi_*.csv
years = [2021, 2022, 2023, 2024, 2025]

print("ğŸ“– Äang Ä‘á»c dá»¯ liá»‡u Student Scores tá»« Bronze layer (2021-2025)...")

try:
    all_dataframes = []

    for year in years:
        bronze_path = f"s3a://bronze/structured_data/Ä‘iá»ƒm tá»«ng thÃ­ sinh/{year}/*.csv"
        print(f"\nâ³ Äang Ä‘á»c dá»¯ liá»‡u nÄƒm {year}...")

        try:
            # Äá»c táº¥t cáº£ file CSV trong thÆ° má»¥c nÄƒm
            df_year = spark.read \
                .option("header", "true") \
                .option("inferSchema", "false") \
                .option("encoding", "UTF-8") \
                .csv(bronze_path)

            # ThÃªm cá»™t nÄƒm Ä‘á»ƒ phÃ¢n biá»‡t
            df_year = df_year.withColumn("Year", lit(year))

            count = df_year.count()
            print(f"  âœ… Äá»c Ä‘Æ°á»£c {count:,} dÃ²ng tá»« nÄƒm {year}")

            all_dataframes.append(df_year)

        except Exception as e:
            print(f"  âš ï¸ KhÃ´ng tÃ¬m tháº¥y hoáº·c lá»—i khi Ä‘á»c dá»¯ liá»‡u nÄƒm {year}: {str(e)}")

    # GhÃ©p táº¥t cáº£ cÃ¡c DataFrame láº¡i
    if all_dataframes:
        print("\nğŸ”— Äang ghÃ©p táº¥t cáº£ dá»¯ liá»‡u cÃ¡c nÄƒm...")
        df_student_scores_bronze = all_dataframes[0]
        for df in all_dataframes[1:]:
            df_student_scores_bronze = df_student_scores_bronze.union(df)

        total_count = df_student_scores_bronze.count()
        print(f"âœ… Tá»•ng sá»‘ dÃ²ng sau khi ghÃ©p: {total_count:,}")

        print("\nğŸ“Š Schema cá»§a dá»¯ liá»‡u Bronze:")
        df_student_scores_bronze.printSchema()

        print("\nğŸ“‹ CÃ¡c cá»™t trong file:")
        print(df_student_scores_bronze.columns)

        print("\nğŸ” Xem 10 dÃ²ng Ä‘áº§u tiÃªn:")
        df_student_scores_bronze.show(10, truncate=False)

    else:
        print("âŒ KhÃ´ng Ä‘á»c Ä‘Æ°á»£c dá»¯ liá»‡u tá»« báº¥t ká»³ nÄƒm nÃ o!")

except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 31. Xá»­ LÃ½ vÃ  Transform Dá»¯ Liá»‡u Student Scores

# In[66]:


from pyspark.sql.functions import split, expr, create_map, concat, lpad
from pyspark.sql.types import MapType, StringType, DoubleType
from itertools import chain

# Transform dá»¯ liá»‡u Ä‘á»ƒ phÃ¹ há»£p vá»›i schema Silver
print("ğŸ”„ Äang xá»­ lÃ½ dá»¯ liá»‡u Student Scores...")

try:
    # ThÃªm timestamp
    current_ts = current_timestamp()

    # BÆ°á»›c 1: Táº¡o studentId = SBD + Year (VD: 010000012021)
    df_processed = df_student_scores_bronze.withColumn(
        "studentId",
        concat(col("SBD"), col("Year").cast("string"))
    )
    print("\nğŸ” Xem 2 dÃ²ng sau khi táº¡o studentId:")
    df_processed.show(2, truncate=False)

    # BÆ°á»›c 2: Xá»­ lÃ½ cá»™t DiemThi - chuyá»ƒn tá»« string sang MAP
    # Format: "ToÃ¡n:2.2,VÄƒn:3.5,Sá»­:2.5,Äá»‹a:5.5"
    # Cáº§n chuyá»ƒn thÃ nh: {"ToÃ¡n": 2.2, "VÄƒn": 3.5, "Sá»­": 2.5, "Äá»‹a": 5.5}

    # Split string thÃ nh array cÃ¡c cáº·p "mÃ´n:Ä‘iá»ƒm"
    df_processed = df_processed.withColumn(
        "score_pairs",
        split(col("DiemThi"), ",")
    )


    # UDF Ä‘á»ƒ parse Ä‘iá»ƒm tá»« string sang MAP
    from pyspark.sql.functions import udf
    from typing import Dict

    def parse_scores(score_string: str) -> Dict[str, float]:
        """Parse score string to dictionary"""
        if not score_string or score_string.strip() == "":
            return {}

        scores_dict = {}
        try:
            # Split by comma
            pairs = score_string.split(",")
            for pair in pairs:
                if ":" in pair:
                    subject, score = pair.split(":")
                    subject = subject.strip()
                    try:
                        scores_dict[subject] = float(score.strip())
                    except ValueError:
                        # Náº¿u khÃ´ng parse Ä‘Æ°á»£c Ä‘iá»ƒm, bá» qua
                        pass
        except Exception:
            pass

        return scores_dict

    # ÄÄƒng kÃ½ UDF
    parse_scores_udf = udf(parse_scores, MapType(StringType(), DoubleType()))

    # Apply UDF Ä‘á»ƒ táº¡o MAP scores
    df_processed = df_processed.withColumn(
        "scores",
        parse_scores_udf(col("DiemThi"))
    )
    print("\nğŸ” Xem 2 dÃ²ng sau khi xá»­ lÃ½ cá»™t scores:")
    df_processed.show(1, truncate=False)
    # BÆ°á»›c 3: Xá»­ lÃ½ cá»™t SBD Ä‘á»ƒ láº¥y regionId
    # 2 kÃ½ tá»± Ä‘áº§u tiÃªn cá»§a SBD chÃ­nh lÃ  mÃ£ khu vá»±c (regionId)
    # VD: SBD = "01000001" -> regionId = "01"
    df_processed = df_processed.withColumn(
        "regionId",
        substring(col("SBD"), 1, 2).cast("string")
    )
    print("\nğŸ” Xem 2 dÃ²ng sau khi xá»­ lÃ½ cá»™t regionId:")
    df_processed.show(2, truncate=False)
    # BÆ°á»›c 4: Select cÃ¡c cá»™t theo schema Silver
    df_student_scores_silver = df_processed.select(
        col("studentId").cast("string"),
        col("regionId").cast("string"),
        col("Year").cast("int").alias("year"),
        col("scores"),
        current_ts.alias("created_at"),
        current_ts.alias("updated_at")
    )

    # BÆ°á»›c 5: LÃ m sáº¡ch dá»¯ liá»‡u
    df_student_scores_silver = df_student_scores_silver.filter(
        col("studentId").isNotNull() & 
        col("year").isNotNull() &
        col("scores").isNotNull()
    )

    # BÆ°á»›c 6: Lá»c unique theo studentId
    print("\nğŸ”„ Äang lá»c dá»¯ liá»‡u unique...")
    df_student_scores_silver = df_student_scores_silver.dropDuplicates(["studentId"])

    print(f"âœ… ÄÃ£ xá»­ lÃ½ xong {df_student_scores_silver.count():,} dÃ²ng dá»¯ liá»‡u")
    print("\nğŸ“Š Schema cá»§a dá»¯ liá»‡u Silver:")
    df_student_scores_silver.printSchema()

    print("\nğŸ” Xem 10 dÃ²ng Ä‘áº§u tiÃªn sau khi xá»­ lÃ½:")
    df_student_scores_silver.show(10, truncate=False)

    # Thá»‘ng kÃª theo nÄƒm
    print("\nğŸ“Š Thá»‘ng kÃª theo nÄƒm:")
    df_student_scores_silver.groupBy("year").count().orderBy("year").show()

    # Thá»‘ng kÃª vá» sá»‘ mÃ´n thi
    print("\nğŸ“Š Thá»‘ng kÃª sá»‘ mÃ´n thi:")
    df_student_scores_silver.select(
        size(col("scores")).alias("num_subjects")
    ).groupBy("num_subjects").count().orderBy("num_subjects").show()

except Exception as e:
    print(f"âŒ Lá»—i khi xá»­ lÃ½ dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 32. Load Dá»¯ Liá»‡u vÃ o Báº£ng Student Scores trong Silver Layer

# In[67]:


# Load dá»¯ liá»‡u vÃ o báº£ng Iceberg trong Silver layer
print("ğŸ’¾ Äang ghi dá»¯ liá»‡u vÃ o báº£ng nessie.silver_tables.student_scores...")

try:
    # Ghi dá»¯ liá»‡u vÃ o báº£ng Iceberg vá»›i partitioning theo nÄƒm
    df_student_scores_silver.writeTo("nessie.silver_tables.student_scores") \
        .using("iceberg") \
        .createOrReplace()

    print("âœ… ÄÃ£ ghi dá»¯ liá»‡u thÃ nh cÃ´ng vÃ o báº£ng student_scores!")
    print(f"ğŸ“Š Tá»•ng sá»‘ dÃ²ng Ä‘Ã£ ghi: {df_student_scores_silver.count():,}")

except Exception as e:
    print(f"âŒ Lá»—i khi ghi dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# In[68]:


# Äá»c vÃ  kiá»ƒm tra dá»¯ liá»‡u tá»« báº£ng Silver
print("ğŸ” Kiá»ƒm tra dá»¯ liá»‡u trong báº£ng nessie.silver_tables.student_scores...")

try:
    df_verify_student_scores = spark.table("nessie.silver_tables.student_scores")

    print(f"\nğŸ“Š Tá»•ng sá»‘ dÃ²ng trong báº£ng: {df_verify_student_scores.count():,}")
    print("\nğŸ” Schema cá»§a báº£ng:")
    df_verify_student_scores.printSchema()

    print("\nğŸ” 20 dÃ²ng Ä‘áº§u tiÃªn:")
    df_verify_student_scores.orderBy("year", "studentId").show(20, truncate=False)

    # print("\nğŸ“ˆ Thá»‘ng kÃª theo nÄƒm:")
    # df_verify_student_scores.groupBy("year") \
    #     .agg(
    #         count("*").alias("Sá»‘ thÃ­ sinh")
    #     ) \
    #     .orderBy("year") \
    #     .show()

    # print("\nğŸ“Š PhÃ¢n bá»‘ sá»‘ mÃ´n thi:")
    # df_verify_student_scores.select(
    #     size(col("scores")).alias("num_subjects")
    # ).groupBy("num_subjects") \
    #     .count() \
    #     .orderBy("num_subjects") \
    #     .show()

    print("\nğŸ” Xem chi tiáº¿t Ä‘iá»ƒm má»™t vÃ i thÃ­ sinh:")
    df_verify_student_scores.select(
        "studentId",
        "year",
        "scores"
    ).show(10, truncate=False)

    # # Thá»‘ng kÃª vá» Ä‘iá»ƒm sá»‘ (vÃ­ dá»¥: mÃ´n ToÃ¡n)
    # print("\nğŸ“Š Thá»‘ng kÃª Ä‘iá»ƒm mÃ´n ToÃ¡n (náº¿u cÃ³):")
    # df_verify_student_scores.select(
    #     col("scores")["ToÃ¡n"].alias("diem_toan")
    # ).filter(
    #     col("diem_toan").isNotNull()
    # ).select(
    #     avg("diem_toan").alias("Äiá»ƒm TB ToÃ¡n"),
    #     min("diem_toan").alias("Äiá»ƒm Min ToÃ¡n"),
    #     max("diem_toan").alias("Äiá»ƒm Max ToÃ¡n")
    # ).show()

    # # Kiá»ƒm tra dá»¯ liá»‡u null
    # print("\nâš ï¸ Kiá»ƒm tra dá»¯ liá»‡u null/missing:")
    # print(f"Sá»‘ dÃ²ng cÃ³ regionId = null: {df_verify_student_scores.filter(col('regionId').isNull()).count():,}")
    # print(f"Sá»‘ dÃ²ng cÃ³ scores rá»—ng: {df_verify_student_scores.filter(size(col('scores')) == 0).count():,}")

    # # Thá»‘ng kÃª cÃ¡c mÃ´n thi phá»• biáº¿n
    # print("\nğŸ“Š CÃ¡c mÃ´n thi cÃ³ trong dá»¯ liá»‡u (láº¥y máº«u):")
    # sample_df = df_verify_student_scores.limit(1000)
    # # Láº¥y táº¥t cáº£ keys tá»« MAP scores
    # all_keys = sample_df.select(explode(map_keys(col("scores")))).distinct()
    # print("CÃ¡c mÃ´n thi:")
    # all_keys.show(50, truncate=False)

except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# In[ ]:





# ---
# 
# # LOAD Dá»® LIá»†U Báº¢NG SUBJECT (MÃ”N Há»ŒC)
# 
# ---

# ## 34. Load vÃ  TÃ¡ch Dá»¯ Liá»‡u Subject tá»« Bronze Layer

# In[69]:


# ÄÆ°á»ng dáº«n Ä‘áº¿n file CSV trong Bronze layer
bronze_subject_group_path = "s3a://bronze/structured_data/tohop_mon.csv"

print("ğŸ“– Äang Ä‘á»c dá»¯ liá»‡u Subject tá»« Bronze layer...")

try:
    # Äá»c file tohop_mon.csv
    df_tohop_mon = spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .option("encoding", "UTF-8") \
        .csv(bronze_subject_group_path)

    print(f"âœ… ÄÃ£ Ä‘á»c Ä‘Æ°á»£c {df_tohop_mon.count()} dÃ²ng dá»¯ liá»‡u")
    print("\nğŸ“Š Schema cá»§a dá»¯ liá»‡u:")
    df_tohop_mon.printSchema()

    print("\nğŸ“‹ CÃ¡c cá»™t trong file:")
    print(df_tohop_mon.columns)

    print("\nğŸ” Xem dá»¯ liá»‡u máº«u:")
    df_tohop_mon.show(10, truncate=False)

    # Láº¥y cá»™t "MÃ´n chi tiáº¿t" (cá»™t thá»© 3)
    mon_chi_tiet_col = df_tohop_mon.columns[2]
    print(f"\nğŸ“Œ TÃªn cá»™t mÃ´n chi tiáº¿t: {mon_chi_tiet_col}")

    # Láº¥y táº¥t cáº£ cÃ¡c giÃ¡ trá»‹ mÃ´n chi tiáº¿t
    df_mon_chi_tiet = df_tohop_mon.select(col(mon_chi_tiet_col).alias("monChiTiet"))

    print("\nğŸ” CÃ¡c tá»• há»£p mÃ´n:")
    df_mon_chi_tiet.show(20, truncate=False)

except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 35. Xá»­ LÃ½ vÃ  TÃ¡ch MÃ´n Há»c tá»« Cá»™t MÃ´n Chi Tiáº¿t

# In[70]:


from pyspark.sql.functions import explode, split, trim, row_number
from pyspark.sql.window import Window

print("ğŸ”„ Äang xá»­ lÃ½ vÃ  tÃ¡ch mÃ´n há»c...")

try:
    # BÆ°á»›c 1: TÃ¡ch chuá»—i mÃ´n chi tiáº¿t thÃ nh array
    # VD: "ToÃ¡n-Ngá»¯ VÄƒn-Tiáº¿ng Anh" -> ["ToÃ¡n", "Ngá»¯ VÄƒn", "Tiáº¿ng Anh"]
    df_mon_array = df_mon_chi_tiet.withColumn(
        "mon_array",
        split(col("monChiTiet"), "-")
    )

    print("\nğŸ” Sau khi tÃ¡ch thÃ nh array:")
    df_mon_array.show(5, truncate=False)

    # BÆ°á»›c 2: Explode array thÃ nh cÃ¡c dÃ²ng riÃªng láº»
    df_mon_exploded = df_mon_array.select(
        explode(col("mon_array")).alias("subjectName")
    )

    print(f"\nğŸ“Š Tá»•ng sá»‘ dÃ²ng sau khi explode: {df_mon_exploded.count()}")

    # BÆ°á»›c 3: Trim spaces vÃ  láº¥y unique
    df_mon_unique = df_mon_exploded.withColumn(
        "subjectName",
        trim(col("subjectName"))
    ).filter(
        col("subjectName").isNotNull() & 
        (col("subjectName") != "")
    ).distinct()

    print(f"\nğŸ“Š Sá»‘ lÆ°á»£ng mÃ´n há»c unique: {df_mon_unique.count()}")

    print("\nğŸ” Danh sÃ¡ch cÃ¡c mÃ´n há»c:")
    df_mon_unique.orderBy("subjectName").show(50, truncate=False)

    # BÆ°á»›c 4: ThÃªm subjectId tá»± Ä‘á»™ng
    current_ts = current_timestamp()
    window_spec = Window.orderBy("subjectName")

    df_subject_silver = df_mon_unique.withColumn(
        "subjectId",
        row_number().over(window_spec)
    ).select(
        col("subjectId").cast("int"),
        col("subjectName").cast("string"),
        current_ts.alias("created_at"),
        current_ts.alias("updated_at")
    )

    print(f"\nâœ… ÄÃ£ xá»­ lÃ½ xong {df_subject_silver.count()} mÃ´n há»c")
    print("\nğŸ“Š Schema cá»§a dá»¯ liá»‡u Silver:")
    df_subject_silver.printSchema()

    print("\nğŸ” Danh sÃ¡ch mÃ´n há»c vá»›i ID:")
    df_subject_silver.orderBy("subjectId").show(50, truncate=False)

except Exception as e:
    print(f"âŒ Lá»—i khi xá»­ lÃ½ dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 36. Load Dá»¯ Liá»‡u vÃ o Báº£ng Subject trong Silver Layer

# In[71]:


# Load dá»¯ liá»‡u vÃ o báº£ng Iceberg trong Silver layer
print("ğŸ’¾ Äang ghi dá»¯ liá»‡u vÃ o báº£ng nessie.silver_tables.subject...")

try:
    df_subject_silver.writeTo("nessie.silver_tables.subject") \
        .using("iceberg") \
        .createOrReplace()

    print("âœ… ÄÃ£ ghi dá»¯ liá»‡u thÃ nh cÃ´ng vÃ o báº£ng subject!")
    print(f"ğŸ“Š Tá»•ng sá»‘ dÃ²ng Ä‘Ã£ ghi: {df_subject_silver.count()}")

except Exception as e:
    print(f"âŒ Lá»—i khi ghi dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# ## 37. Kiá»ƒm Tra Dá»¯ Liá»‡u Subject ÄÃ£ Load vÃ o Silver Layer

# In[72]:


# Äá»c vÃ  kiá»ƒm tra dá»¯ liá»‡u tá»« báº£ng Silver
print("ğŸ” Kiá»ƒm tra dá»¯ liá»‡u trong báº£ng nessie.silver_tables.subject...")

try:
    df_verify_subject = spark.table("nessie.silver_tables.subject")

    print(f"\nğŸ“Š Tá»•ng sá»‘ dÃ²ng trong báº£ng: {df_verify_subject.count()}")
    print("\nğŸ” Schema cá»§a báº£ng:")
    df_verify_subject.printSchema()

    print("\nğŸ” Táº¥t cáº£ cÃ¡c mÃ´n há»c:")
    df_verify_subject.orderBy("subjectId").show(50, truncate=False)

    # Thá»‘ng kÃª
    print("\nğŸ“Š Thá»‘ng kÃª:")
    print(f"Tá»•ng sá»‘ mÃ´n há»c: {df_verify_subject.count()}")

    # Liá»‡t kÃª cÃ¡c mÃ´n há»c theo thá»© tá»± alphabet
    print("\nğŸ“‹ Danh sÃ¡ch mÃ´n há»c theo thá»© tá»± alphabet:")
    df_verify_subject.orderBy("subjectName").show(50, truncate=False)

except Exception as e:
    print(f"âŒ Lá»—i khi Ä‘á»c dá»¯ liá»‡u: {str(e)}")
    import traceback
    traceback.print_exc()


# In[ ]:




