# ğŸ‰ CHUYá»‚N Äá»”I Tá»ª NOTEBOOK SANG PYSPARK SCRIPT

## ğŸ“ TÃ³m Táº¯t Nhá»¯ng GÃ¬ ÄÃ£ LÃ m

### 1. âœ… Táº¡o PySpark Script tá»« Notebook

**File má»›i:** `airflow/scripts/load_bronze_to_silver.py`

- Chuyá»ƒn Ä‘á»•i notebook `Load_Data_Bronze_To_Silver.ipynb` thÃ nh Python script
- Tá»• chá»©c code thÃ nh cÃ¡c functions:
  - `create_spark_session()`: Khá»Ÿi táº¡o Spark
  - `load_school_data()`: Load dá»¯ liá»‡u School
  - `load_major_data()`: Load dá»¯ liá»‡u Major  
  - `load_major_group_data()`: Load dá»¯ liá»‡u Major Group
  - `main()`: Main function

**Æ¯u Ä‘iá»ƒm:**
- âœ… Production-ready
- âœ… Dá»… debug vÃ  maintain
- âœ… Better performance
- âœ… Clean logs

### 2. âœ… Táº¡o Airflow DAG Má»›i

**File má»›i:** `airflow/dags/spark_submit_bronze_to_silver.py`

**3 Tasks:**
1. `check_spark_cluster`: Kiá»ƒm tra Spark cluster health
2. `submit_pyspark_job`: Submit script lÃªn Spark cluster báº±ng `spark-submit`
3. `verify_silver_data`: Verify dá»¯ liá»‡u Ä‘Ã£ load thÃ nh cÃ´ng

**PhÆ°Æ¡ng phÃ¡p:**
- Sá»­ dá»¥ng `docker exec` Ä‘á»ƒ cháº¡y `spark-submit` TRONG Spark Master container
- Truyá»n Ä‘áº§y Ä‘á»§ Spark configs (Iceberg, Nessie, MinIO)
- Capture output vÃ  errors Ä‘á»ƒ debug

### 3. âœ… Cáº­p Nháº­t Docker Compose

**File cáº­p nháº­t:** `docker-compose.yaml`

**Thay Ä‘á»•i:**

#### Spark Master:
```yaml
volumes:
  - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
  - ./notebooks:/opt/airflow/notebooks  # â† Má»›i
  - ./airflow/scripts:/opt/airflow/scripts  # â† Má»›i
```

#### Airflow:
```yaml
volumes:
  - ./airflow/dags:/opt/airflow/dags
  - ./airflow/logs:/opt/airflow/logs
  - ./airflow/plugins:/opt/airflow/plugins
  - ./airflow/entrypoint.sh:/opt/airflow/entrypoint.sh
  - ./notebooks:/opt/airflow/notebooks
  - ./airflow/scripts:/opt/airflow/scripts  # â† Má»›i
  - /var/run/docker.sock:/var/run/docker.sock  # â† Má»›i (Ä‘á»ƒ Airflow control Docker)
```

### 4. âœ… Cáº­p Nháº­t Airflow Dockerfile

**File cáº­p nháº­t:** `airflow/Dockerfile`

**ThÃªm Docker CLI:**
```dockerfile
# CÃ i Ä‘áº·t Docker CLI Ä‘á»ƒ Airflow cÃ³ thá»ƒ cháº¡y docker exec
RUN apt-get update && \
    apt-get install -y docker-ce-cli
```

### 5. âœ… Táº¡o TÃ i Liá»‡u

**Files má»›i:**
- `airflow/README_SPARK_SUBMIT.md`: HÆ°á»›ng dáº«n chi tiáº¿t
- `setup_spark_submit.sh`: Script tá»± Ä‘á»™ng setup

---

## ğŸš€ CÃ¡ch Sá»­ Dá»¥ng

### Quick Start

```bash
# 1. Cháº¡y script setup
bash setup_spark_submit.sh

# 2. Má»Ÿ Airflow UI
# http://localhost:8088

# 3. Enable vÃ  trigger DAG: spark_submit_bronze_to_silver

# 4. Monitor Spark jobs
# http://localhost:8080
```

### Manual Steps

```bash
# 1. Stop containers
docker-compose down

# 2. Build láº¡i
docker-compose build --no-cache airflow spark-master

# 3. Start services
docker-compose up -d

# 4. Kiá»ƒm tra mount
docker exec spark-master ls -la /opt/airflow/scripts/
docker exec airflow ls -la /opt/airflow/scripts/

# 5. Trigger DAG
docker exec airflow airflow dags trigger spark_submit_bronze_to_silver

# 6. Xem logs
docker exec airflow airflow tasks logs spark_submit_bronze_to_silver submit_pyspark_job <execution_date>
```

---

## ğŸ“Š So SÃ¡nh: CÅ© vs Má»›i

### CÃ¡ch CÅ© (Notebook)
```
Airflow â†’ Papermill â†’ Execute Notebook (trong Spark Master)
```

**Váº¥n Ä‘á»:**
- âŒ Notebook overhead
- âŒ KhÃ³ debug
- âŒ KhÃ´ng production-ready
- âŒ Logs lá»™n xá»™n

### CÃ¡ch Má»›i (PySpark Script)
```
Airflow â†’ spark-submit â†’ Python Script (trÃªn Spark Cluster)
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… Hiá»‡u suáº¥t cao hÆ¡n
- âœ… Dá»… debug
- âœ… Production-ready
- âœ… Logs rÃµ rÃ ng
- âœ… Scalable

---

## ğŸ”§ Kiáº¿n TrÃºc Hoáº¡t Äá»™ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Airflow DAG    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ (1) Check cluster health
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spark Master    â”‚ â† Check port 8080 (REST API)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ (2) docker exec spark-master spark-submit ...
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ spark-submit                                â”‚
â”‚   --master spark://spark-master:7077       â”‚
â”‚   --deploy-mode client                      â”‚
â”‚   /opt/airflow/scripts/load_bronze_to_silver.py â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ (3) Submit jobs to cluster
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spark Master    â”‚â”€â”€â”€â”€â”€â”€â”‚ Spark Worker(s) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ (4) Read from Bronze / Write to Silver
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MinIO (S3)     â”‚      â”‚  Nessie Catalog â”‚
â”‚  - Bronze       â”‚      â”‚  - silver_tablesâ”‚
â”‚  - Silver       â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ (5) Verify data
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Verification    â”‚
â”‚ Script          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Cáº¥u TrÃºc ThÆ° Má»¥c

```
TLCN-Source/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ spark_submit_bronze_to_silver.py    # â† DAG má»›i âœ¨
â”‚   â”‚   â””â”€â”€ run_notebook_bronze_to_silver.py    # â† DAG cÅ©
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ load_bronze_to_silver.py            # â† PySpark script âœ¨
â”‚   â”œâ”€â”€ Dockerfile                              # â† Updated (Docker CLI) âœ¨
â”‚   â”œâ”€â”€ README_SPARK_SUBMIT.md                  # â† HÆ°á»›ng dáº«n âœ¨
â”‚   â””â”€â”€ ...
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ bronze/
â”‚       â””â”€â”€ Load_Data_Bronze_To_Silver.ipynb    # â† Original notebook
â”œâ”€â”€ docker-compose.yaml                         # â† Updated (mounts) âœ¨
â”œâ”€â”€ setup_spark_submit.sh                       # â† Setup script âœ¨
â””â”€â”€ ...
```

---

## âœ… Checklist Sau Khi Deploy

- [ ] Services Ä‘ang cháº¡y: `docker-compose ps`
- [ ] Spark Master healthy: http://localhost:8080
- [ ] Airflow UI accessible: http://localhost:8088
- [ ] Scripts Ä‘Æ°á»£c mount: `docker exec spark-master ls /opt/airflow/scripts/`
- [ ] Docker CLI hoáº¡t Ä‘á»™ng: `docker exec airflow docker --version`
- [ ] DAG xuáº¥t hiá»‡n trong Airflow UI
- [ ] Test cháº¡y DAG thÃ nh cÃ´ng
- [ ] Verify data trong Silver layer

---

## ğŸ› Troubleshooting Common Issues

### Issue 1: Script khÃ´ng tÃ¬m tháº¥y
```bash
# Check mount
docker exec spark-master ls -la /opt/airflow/scripts/

# Fix: Restart container
docker-compose restart spark-master
```

### Issue 2: Docker command not found trong Airflow
```bash
# Check Docker CLI
docker exec airflow docker --version

# Fix: Rebuild Airflow
docker-compose build --no-cache airflow
```

### Issue 3: Permission denied on docker.sock
```bash
# Check permissions
docker exec airflow ls -la /var/run/docker.sock

# Fix: Add airflow user to docker group (in Dockerfile)
# hoáº·c chmod docker.sock
sudo chmod 666 /var/run/docker.sock
```

### Issue 4: Spark job timeout
```bash
# TÄƒng timeout trong DAG
timeout=7200  # 2 hours

# Hoáº·c optimize script (cache, repartition, etc.)
```

---

## ğŸ“ Best Practices ÄÃ£ Ãp Dá»¥ng

1. âœ… **Separation of Concerns**: DAG chá»‰ orchestrate, logic á»Ÿ script
2. âœ… **Error Handling**: Try-catch á»Ÿ má»i nÆ¡i cÃ³ thá»ƒ fail
3. âœ… **Logging**: Print rÃµ rÃ ng tá»«ng bÆ°á»›c
4. âœ… **Validation**: Check cluster health trÆ°á»›c khi submit
5. âœ… **Verification**: Verify data sau khi load
6. âœ… **Resource Management**: Set memory/cores appropriately
7. âœ… **Timeout**: Set timeout Ä‘á»ƒ trÃ¡nh jobs cháº¡y mÃ£i
8. âœ… **Documentation**: README chi tiáº¿t

---

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o

- [Spark Submit Guide](https://spark.apache.org/docs/latest/submitting-applications.html)
- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)
- [Docker in Docker](https://docs.docker.com/engine/security/rootless/)
- [Iceberg + Spark](https://iceberg.apache.org/docs/latest/spark/)

---

## ğŸ‰ Káº¿t Luáº­n

Báº¡n Ä‘Ã£ thÃ nh cÃ´ng chuyá»ƒn Ä‘á»•i tá»« Jupyter Notebook sang PySpark Script production-ready!

**Key Achievements:**
- âœ… Converted notebook to Python script
- âœ… Created new Airflow DAG with spark-submit
- âœ… Updated Docker configs for proper mounts
- âœ… Added Docker CLI to Airflow
- âœ… Comprehensive documentation

**Next Steps:**
- Scale up Spark workers
- Add more data quality checks
- Implement monitoring & alerting
- Optimize performance
- Add more tables/transformations

**Happy Data Engineering! ğŸš€**
