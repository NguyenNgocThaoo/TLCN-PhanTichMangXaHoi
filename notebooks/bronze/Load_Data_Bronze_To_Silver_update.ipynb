{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c24213a7",
   "metadata": {},
   "source": [
    "# Load Data từ Bronze Layer sang Silver Layer\n",
    "\n",
    "Notebook này sẽ đọc dữ liệu từ Bronze layer (MinIO) và xử lý để load vào các bảng Iceberg trong Silver layer với Nessie catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063eaad0",
   "metadata": {},
   "source": [
    "## 1. Import Libraries và Khởi tạo Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c718b51-ae4c-4fcf-b0b4-fcf8b57dfad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark Session initialized | Master: spark://spark-master:7077 | App ID: app-20251202184326-0000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import csv, io, os, re\n",
    "from datetime import datetime\n",
    "from typing import Dict\n",
    "\n",
    "# Cấu hình AWS/MinIO credentials\n",
    "os.environ.update({\n",
    "    'AWS_REGION': 'us-east-1',\n",
    "    'AWS_ACCESS_KEY_ID': 'admin',\n",
    "    'AWS_SECRET_ACCESS_KEY': 'admin123'\n",
    "})\n",
    "\n",
    "# Khởi tạo Spark Session với Nessie Catalog\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Load_Bronze_To_Silver\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    # Nessie Catalog\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v2\")\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://silver/\")\n",
    "    .config(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    # S3/MinIO Config\n",
    "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.access-key-id\", \"admin\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.secret-access-key\", \"admin123\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.region\", \"us-east-1\")\n",
    "    # Hadoop S3A Config\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.region\", \"us-east-1\")\n",
    "    # Executor Environment\n",
    "    .config(\"spark.executorEnv.AWS_REGION\", \"us-east-1\")\n",
    "    .config(\"spark.executorEnv.AWS_ACCESS_KEY_ID\", \"admin\")\n",
    "    .config(\"spark.executorEnv.AWS_SECRET_ACCESS_KEY\", \"admin123\")\n",
    "    # Local JAR files\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/hadoop-aws-3.3.4.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS nessie.silver_tables\")\n",
    "spark.sql(\"USE nessie.silver_tables\")\n",
    "print(f\"✅ Spark Session initialized | Master: {spark.sparkContext.master} | App ID: {spark.sparkContext.applicationId}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e957d424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/02 05:20:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session đã được khởi tạo với Nessie catalog!\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql.types import *\n",
    "# from pyspark.sql.window import Window\n",
    "# from datetime import datetime\n",
    "# import os\n",
    "# import hashlib  # Để dùng trong batch processing nếu cần\n",
    "# import csv\n",
    "# import io\n",
    "\n",
    "\n",
    "# # Khởi tạo Spark Session với Iceberg và Nessie catalog\n",
    "# spark = (\n",
    "#     SparkSession.builder\n",
    "#     # .master(\"spark://spark-master:7077\") # để chạy DAG bên Spark Cluster\n",
    "#     .appName(\"Load_Bronze_To_Silver\")\n",
    "#     .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "#     .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "#     .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v2\")\n",
    "#     .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "#     .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://silver/\")\n",
    "#     .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\")\n",
    "#     .config(\"spark.sql.catalog.nessie.s3.access-key\", \"admin\")\n",
    "#     .config(\"spark.sql.catalog.nessie.s3.secret-key\", \"admin123\")\n",
    "#     .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\")\n",
    "#     .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "#     .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "#     .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\")\n",
    "#     .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "#     .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "# spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "# print(\"Spark Session đã được khởi tạo với Nessie catalog!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43664703",
   "metadata": {},
   "source": [
    "## 2. Load Bảng SCHOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88066c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOAD BẢNG SCHOOL\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã ghi 265 dòng vào school\n",
      "+--------+--------------------------------------+-----------+--------------------------+--------------------------+\n",
      "|schoolId|schoolName                            |province   |created_at                |updated_at                |\n",
      "+--------+--------------------------------------+-----------+--------------------------+--------------------------+\n",
      "|DHF     |Đại học Ngoại Ngữ - Đại học Huế       |Huế        |2025-12-01 15:39:04.602019|2025-12-01 15:39:04.602019|\n",
      "|DVB     |Đại học Việt Bắc                      |Thái Nguyên|2025-12-01 15:39:04.602019|2025-12-01 15:39:04.602019|\n",
      "|DCQ     |Đại học Công Nghệ và Quản Lý Hữu Nghị |Hà Nội     |2025-12-01 15:39:04.602019|2025-12-01 15:39:04.602019|\n",
      "|NTT     |Đại học Nguyễn Tất Thành              |TP HCM     |2025-12-01 15:39:04.602019|2025-12-01 15:39:04.602019|\n",
      "|KGH     |Trường Sĩ Quan Không Quân - Hệ Đại học|Khánh Hòa  |2025-12-01 15:39:04.602019|2025-12-01 15:39:04.602019|\n",
      "+--------+--------------------------------------+-----------+--------------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG SCHOOL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Đọc và merge tất cả các năm\n",
    "years = [2021, 2022, 2023, 2024, 2025]\n",
    "base_path = \"s3a://bronze/structured_data/danh sách các trường Đại Học (2021-2025)/Danh_sách_các_trường_Đại_Học_\"\n",
    "df_school = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv([f\"{base_path}{year}.csv\" for year in years]).select(\"TenTruong\", \"MaTruong\", \"TinhThanh\").dropDuplicates()\n",
    "\n",
    "# Transform\n",
    "df_school_silver = df_school.select(\n",
    "    col(\"MaTruong\").cast(\"string\").alias(\"schoolId\"),\n",
    "    col(\"TenTruong\").cast(\"string\").alias(\"schoolName\"),\n",
    "    col(\"TinhThanh\").cast(\"string\").alias(\"province\"),\n",
    "    current_timestamp().alias(\"created_at\"),\n",
    "    current_timestamp().alias(\"updated_at\")\n",
    ").filter(col(\"schoolId\").isNotNull() & col(\"schoolName\").isNotNull())\n",
    "\n",
    "# Ghi vào Silver\n",
    "df_school_silver.writeTo(\"nessie.silver_tables.school\").using(\"iceberg\").createOrReplace()\n",
    "print(f\"Đã ghi {df_school_silver.count()} dòng vào school\")\n",
    "\n",
    "# Verify\n",
    "spark.table(\"nessie.silver_tables.school\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0235c2c",
   "metadata": {},
   "source": [
    "## 3. Load Bảng MAJOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf9d446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã ghi 3085 dòng vào major\n",
      "+-------+------------------------------------------------------------+-------------------------+-------------------------+\n",
      "|majorId|majorName                                                   |created_at               |updated_at               |\n",
      "+-------+------------------------------------------------------------+-------------------------+-------------------------+\n",
      "|106    |Khoa học Máy tính                                           |2025-12-01 15:39:08.82324|2025-12-01 15:39:08.82324|\n",
      "|107    |Kỹ thuật Máy tính                                           |2025-12-01 15:39:08.82324|2025-12-01 15:39:08.82324|\n",
      "|108    |Điện - Điện tử - Viễn Thông - Tự động hoá - Thiết kế vi mạch|2025-12-01 15:39:08.82324|2025-12-01 15:39:08.82324|\n",
      "|109    |Kỹ Thuật Cơ khí                                             |2025-12-01 15:39:08.82324|2025-12-01 15:39:08.82324|\n",
      "|110    |Kỹ Thuật Cơ Điện tử                                         |2025-12-01 15:39:08.82324|2025-12-01 15:39:08.82324|\n",
      "+-------+------------------------------------------------------------+-------------------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lower, trim, regexp_replace, current_timestamp\n",
    "\n",
    "df_major = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .csv(\"s3a://bronze/structured_data/danh sách các ngành đại học/Danh_sách_các_ngành.csv\")\n",
    "\n",
    "df_major_clean = df_major.select(\n",
    "    regexp_replace(trim(col(df_major.columns[0])).cast(\"string\"), r\"\\.0$\", \"\").alias(\"majorId\"),\n",
    "    trim(col(df_major.columns[1])).cast(\"string\").alias(\"majorName\")\n",
    ").filter(\n",
    "    (col(\"majorId\").isNotNull()) &\n",
    "    (col(\"majorName\").isNotNull()) &\n",
    "    (col(\"majorId\") != \"\") &\n",
    "    (col(\"majorName\") != \"\") &\n",
    "    (lower(col(\"majorId\")) != \"nan\")\n",
    ")\n",
    "\n",
    "# Chuẩn hoá để dedupe theo lowercase\n",
    "df_major_silver = df_major_clean \\\n",
    "    .withColumn(\"majorId_lower\", lower(col(\"majorId\"))) \\\n",
    "    .dropDuplicates([\"majorId_lower\"]) \\\n",
    "    .select(\n",
    "        col(\"majorId\"),\n",
    "        col(\"majorName\"),\n",
    "        current_timestamp().alias(\"created_at\"),\n",
    "        current_timestamp().alias(\"updated_at\")\n",
    "    )\n",
    "\n",
    "df_major_silver.writeTo(\"nessie.silver_tables.major\").using(\"iceberg\").createOrReplace()\n",
    "\n",
    "print(f\"Đã ghi {df_major_silver.count()} dòng vào major\")\n",
    "spark.table(\"nessie.silver_tables.major\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d64a8",
   "metadata": {},
   "source": [
    "## 4. Load Bảng SUBJECT_GROUP và SUBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c053cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOAD BẢNG SUBJECT_GROUP và SUBJECT\n",
      "================================================================================\n",
      "Đã ghi 232 dòng vào subject_group\n",
      "Đã ghi 51 dòng vào subject\n",
      "+--------------+----------------+------------------+--------------------------+--------------------------+\n",
      "|subjectGroupId|subjectGroupName|subjectCombination|created_at                |updated_at                |\n",
      "+--------------+----------------+------------------+--------------------------+--------------------------+\n",
      "|1             |A00             |Toán-Lí-Hóa       |2025-12-01 15:39:10.880774|2025-12-01 15:39:10.880774|\n",
      "|2             |A01             |Toán-Lí-Ngoại ngữ |2025-12-01 15:39:10.880774|2025-12-01 15:39:10.880774|\n",
      "|3             |A02             |Toán-Lí-Sinh      |2025-12-01 15:39:10.880774|2025-12-01 15:39:10.880774|\n",
      "|4             |A03             |Toán-Lí-Sử        |2025-12-01 15:39:10.880774|2025-12-01 15:39:10.880774|\n",
      "|5             |A04             |Toán-Lí-Địa       |2025-12-01 15:39:10.880774|2025-12-01 15:39:10.880774|\n",
      "+--------------+----------------+------------------+--------------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------------------+--------------------------+--------------------------+\n",
      "|subjectId|subjectName          |created_at                |updated_at                |\n",
      "+---------+---------------------+--------------------------+--------------------------+\n",
      "|1        |biểu diễn nghệ thuật |2025-12-01 15:39:12.000187|2025-12-01 15:39:12.000187|\n",
      "|2        |Công nghệ công nghiệp|2025-12-01 15:39:12.000187|2025-12-01 15:39:12.000187|\n",
      "|3        |Công nghệ nông nghiệp|2025-12-01 15:39:12.000187|2025-12-01 15:39:12.000187|\n",
      "|4        |GDCD                 |2025-12-01 15:39:12.000187|2025-12-01 15:39:12.000187|\n",
      "|5        |Hát                  |2025-12-01 15:39:12.000187|2025-12-01 15:39:12.000187|\n",
      "+---------+---------------------+--------------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG SUBJECT_GROUP và SUBJECT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Đọc file tohop_mon_fixed.csv\n",
    "df_tohop = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"encoding\", \"UTF-8\").csv(\"s3a://bronze/structured_data/tohop_mon_fixed.csv\")\n",
    "\n",
    "# --- SUBJECT_GROUP ---\n",
    "df_subject_group_silver = df_tohop.select(\n",
    "    col(df_tohop.columns[0]).cast(\"int\").alias(\"subjectGroupId\"),\n",
    "    col(df_tohop.columns[1]).cast(\"string\").alias(\"subjectGroupName\"),\n",
    "    col(df_tohop.columns[2]).cast(\"string\").alias(\"subjectCombination\"),\n",
    "    current_timestamp().alias(\"created_at\"),\n",
    "    current_timestamp().alias(\"updated_at\")\n",
    ").filter(col(\"subjectGroupId\").isNotNull() & col(\"subjectGroupName\").isNotNull() & col(\"subjectCombination\").isNotNull()).dropDuplicates([\"subjectGroupName\", \"subjectCombination\"])\n",
    "df_subject_group_silver.writeTo(\"nessie.silver_tables.subject_group\").using(\"iceberg\").createOrReplace()\n",
    "print(f\"Đã ghi {df_subject_group_silver.count()} dòng vào subject_group\")\n",
    "\n",
    "# --- SUBJECT ---\n",
    "df_subject = (\n",
    "    df_tohop.select(explode(split(col(df_tohop.columns[2]), \"-\")).alias(\"subjectName\"))\n",
    "            .withColumn(\"subjectName\", trim(col(\"subjectName\")))\n",
    "            .filter(col(\"subjectName\").isNotNull() & (col(\"subjectName\") != \"\"))\n",
    "            .withColumn(\"subjectName_lower\", lower(col(\"subjectName\")))\n",
    "            # loại bỏ trùng theo chữ thường\n",
    "            .dropDuplicates([\"subjectName_lower\"])\n",
    ")\n",
    "\n",
    "window_spec = Window.orderBy(\"subjectName_lower\")\n",
    "df_subject_silver = df_subject.withColumn(\"subjectId\", row_number().over(window_spec)).select(\n",
    "    col(\"subjectId\").cast(\"int\"),\n",
    "    col(\"subjectName\").cast(\"string\"),\n",
    "    current_timestamp().alias(\"created_at\"),\n",
    "    current_timestamp().alias(\"updated_at\")\n",
    ")\n",
    "df_subject_silver.writeTo(\"nessie.silver_tables.subject\").using(\"iceberg\").createOrReplace()\n",
    "print(f\"Đã ghi {df_subject_silver.count()} dòng vào subject\")\n",
    "\n",
    "# Verify\n",
    "spark.table(\"nessie.silver_tables.subject_group\").orderBy(\"subjectGroupId\").show(5, truncate=False)\n",
    "spark.table(\"nessie.silver_tables.subject\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e6236f",
   "metadata": {},
   "source": [
    "## 5. Load Bảng SELECTION_METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d204061d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOAD BẢNG SELECTION_METHOD\n",
      "================================================================================\n",
      "Đã ghi 10 dòng vào selection_method\n",
      "+-----------------+------------------------------------------------------+--------------------------+--------------------------+\n",
      "|selectionMethodId|selectionMethodName                                   |created_at                |updated_at                |\n",
      "+-----------------+------------------------------------------------------+--------------------------+--------------------------+\n",
      "|1                |Điểm chuẩn theo phương thức Điểm học bạ               |2025-12-01 15:39:14.369252|2025-12-01 15:39:14.369252|\n",
      "|2                |Điểm chuẩn theo phương thức Điểm thi THPT             |2025-12-01 15:39:14.369252|2025-12-01 15:39:14.369252|\n",
      "|3                |Điểm chuẩn theo phương thức Điểm xét tuyển kết hợp    |2025-12-01 15:39:14.369252|2025-12-01 15:39:14.369252|\n",
      "|4                |Điểm chuẩn theo phương thức Điểm xét tốt nghiệp THPT  |2025-12-01 15:39:14.369252|2025-12-01 15:39:14.369252|\n",
      "|5                |Điểm chuẩn theo phương thức Điểm ĐGNL HCM             |2025-12-01 15:39:14.369252|2025-12-01 15:39:14.369252|\n",
      "|6                |Điểm chuẩn theo phương thức Điểm ĐGNL HN              |2025-12-01 15:39:14.369252|2025-12-01 15:39:14.369252|\n",
      "|7                |Điểm chuẩn theo phương thức Điểm ĐGNL ĐH Sư phạm HN   |2025-12-01 15:39:14.369252|2025-12-01 15:39:14.369252|\n",
      "|8                |Điểm chuẩn theo phương thức Điểm ĐGNL ĐH Sư phạm TPHCM|2025-12-01 15:39:14.369252|2025-12-01 15:39:14.369252|\n",
      "|9                |Điểm chuẩn theo phương thức Điểm Đánh giá Tư duy      |2025-12-01 15:39:14.369252|2025-12-01 15:39:14.369252|\n",
      "|10               |Điểm chuẩn theo phương thức ƯTXT, XT thẳng            |2025-12-01 15:39:14.369252|2025-12-01 15:39:14.369252|\n",
      "+-----------------+------------------------------------------------------+--------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG SELECTION_METHOD\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Đọc từ file benchmark để lấy các phương thức xét tuyển\n",
    "df_benchmark = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"encoding\", \"UTF-8\").csv(\"s3a://bronze/structured_data/điểm chuẩn các trường (2021-2025)/Điểm_chuẩn_các_ngành_đại_học_năm(2021-2025)*.csv\")\n",
    "\n",
    "# Lấy PhuongThuc và loại bỏ \"năm ...\"\n",
    "df_selection = df_benchmark.select(trim(regexp_replace(col(\"PhuongThuc\"), r\"\\s*năm\\s+\\d{4}.*$\", \"\")).alias(\"selectionMethodName\")).filter(col(\"selectionMethodName\").isNotNull() & (col(\"selectionMethodName\") != \"\")).distinct()\n",
    "\n",
    "window_spec = Window.orderBy(\"selectionMethodName\")\n",
    "df_selection_method_silver = df_selection.withColumn(\"selectionMethodId\", row_number().over(window_spec)).select(\n",
    "    col(\"selectionMethodId\").cast(\"int\"),\n",
    "    col(\"selectionMethodName\").cast(\"string\"),\n",
    "    current_timestamp().alias(\"created_at\"),\n",
    "    current_timestamp().alias(\"updated_at\")\n",
    ")\n",
    "df_selection_method_silver.writeTo(\"nessie.silver_tables.selection_method\").using(\"iceberg\").createOrReplace()\n",
    "print(f\"Đã ghi {df_selection_method_silver.count()} dòng vào selection_method\")\n",
    "\n",
    "# Verify\n",
    "spark.table(\"nessie.silver_tables.selection_method\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911622a",
   "metadata": {},
   "source": [
    "## 6. Load Bảng GradingScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9ca8916-830b-4f32-bbbb-d0ac527e6568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOAD BẢNG GRADING_SCALE TỪ PHANLOAITHANGDIEM\n",
      "================================================================================\n",
      "Đã ghi 10 dòng vào grading_scale\n",
      "+--------------+------+---------------+--------------------------+--------------------------+\n",
      "|gradingScaleId|value |description    |created_at                |updated_at                |\n",
      "+--------------+------+---------------+--------------------------+--------------------------+\n",
      "|0             |30.0  |Thang điểm 30  |2025-12-01 15:39:16.761752|2025-12-01 15:39:16.761752|\n",
      "|1             |1200.0|Thang điểm 1200|2025-12-01 15:39:16.761752|2025-12-01 15:39:16.761752|\n",
      "|2             |40.0  |Thang điểm 40  |2025-12-01 15:39:16.761752|2025-12-01 15:39:16.761752|\n",
      "|3             |50.0  |Thang điểm 50  |2025-12-01 15:39:16.761752|2025-12-01 15:39:16.761752|\n",
      "|4             |150.0 |Thang điểm 150 |2025-12-01 15:39:16.761752|2025-12-01 15:39:16.761752|\n",
      "|5             |10.0  |Thang điểm 10  |2025-12-01 15:39:16.761752|2025-12-01 15:39:16.761752|\n",
      "|6             |100.0 |Thang điểm 100 |2025-12-01 15:39:16.761752|2025-12-01 15:39:16.761752|\n",
      "|7             |120.0 |Thang điểm 120 |2025-12-01 15:39:16.761752|2025-12-01 15:39:16.761752|\n",
      "|8             |35.0  |Thang điểm 35  |2025-12-01 15:39:16.761752|2025-12-01 15:39:16.761752|\n",
      "|9             |90.0  |Thang điểm 90  |2025-12-01 15:39:16.761752|2025-12-01 15:39:16.761752|\n",
      "+--------------+------+---------------+--------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG GRADING_SCALE TỪ PHANLOAITHANGDIEM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Đọc dữ liệu gốc từ file CSV (giống benchmark)\n",
    "df_raw = (\n",
    "    spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .option(\"encoding\", \"UTF-8\")\n",
    "        .csv(\"s3a://bronze/structured_data/điểm chuẩn các trường (2021-2025)/Điểm_chuẩn_các_ngành_đại_học_năm(2021-2025)*.csv\")\n",
    ")\n",
    "\n",
    "# 2. Lấy unique PhanLoaiThangDiem\n",
    "df_grading_raw = (\n",
    "    df_raw\n",
    "        .select(trim(col(\"PhanLoaiThangDiem\")).alias(\"description\"))\n",
    "        .filter(col(\"description\").isNotNull() & (col(\"description\") != \"\"))\n",
    "        .dropDuplicates([\"description\"])\n",
    ")\n",
    "\n",
    "# 3. Tách giá trị số trong description làm \"value\" (nếu có, vd: \"thang 40\" -> 40)\n",
    "df_grading = (\n",
    "    df_grading_raw\n",
    "        .withColumn(\n",
    "            \"value\",\n",
    "            regexp_extract(col(\"description\"), r\"(\\d+(?:\\.\\d+)?)\", 1).cast(\"float\")\n",
    "        )\n",
    "        .withColumn(\"gradingScaleId\", monotonically_increasing_id().cast(\"int\"))\n",
    "        .withColumn(\"created_at\", current_timestamp())\n",
    "        .withColumn(\"updated_at\", current_timestamp())\n",
    "        .select(\n",
    "            \"gradingScaleId\",\n",
    "            \"value\",\n",
    "            \"description\",\n",
    "            \"created_at\",\n",
    "            \"updated_at\"\n",
    "        )\n",
    ")\n",
    "\n",
    "# 4. Ghi vào bảng Iceberg grading_scale đã tạo trước đó\n",
    "df_grading.writeTo(\"nessie.silver_tables.grading_scale\") \\\n",
    "          .using(\"iceberg\") \\\n",
    "          .createOrReplace()\n",
    "\n",
    "print(f\"Đã ghi {df_grading.count()} dòng vào grading_scale\")\n",
    "\n",
    "# 5. Verify\n",
    "spark.table(\"nessie.silver_tables.grading_scale\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036e97a-c555-43a0-a6d4-fb1e673edf0f",
   "metadata": {},
   "source": [
    "## 6. Load Bảng BENCHMARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24de0240-da7e-4d00-8870-78bfa419516a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOAD BẢNG BENCHMARK\n",
      "================================================================================\n",
      "Bảng nessie.silver_tables.benchmark đã tồn tại → dùng MERGE (upsert).\n",
      "Đã MERGE dữ liệu mới vào bảng benchmark\n",
      "+--------------------+--------+--------+--------------+-----------------+--------------+----+-----+--------------------------+--------------------------+\n",
      "|benchmarkId         |schoolId|majorId |subjectGroupId|selectionMethodId|gradingScaleId|year|score|created_at                |updated_at                |\n",
      "+--------------------+--------+--------+--------------+-----------------+--------------+----+-----+--------------------------+--------------------------+\n",
      "|676054869222677887  |HHK     |7510302A|2             |2                |0             |2025|18.0 |2025-12-01 16:00:01.747504|2025-12-01 16:00:01.747504|\n",
      "|-4276703068313169330|HIU     |7340122 |35            |2                |0             |2025|15.0 |2025-12-01 16:00:01.747504|2025-12-01 16:00:01.747504|\n",
      "|-1430534741141047354|DCT     |7480201 |NULL          |5                |1             |2025|740.0|2025-12-01 16:00:01.747504|2025-12-01 16:00:01.747504|\n",
      "|7856740622250900629 |DHL     |7620305 |18            |1                |0             |2025|18.0 |2025-12-01 16:00:01.747504|2025-12-01 16:00:01.747504|\n",
      "|2912808173307302652 |HSU     |7210404 |2             |6                |4             |2025|67.0 |2025-12-01 16:00:01.747504|2025-12-01 16:00:01.747504|\n",
      "+--------------------+--------+--------+--------------+-----------------+--------------+----+-----+--------------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+-----+\n",
      "|year|count|\n",
      "+----+-----+\n",
      "|2021|23215|\n",
      "|2022|27251|\n",
      "|2023|31775|\n",
      "|2024|40734|\n",
      "|2025|40424|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, trim, regexp_replace, current_timestamp,\n",
    "    avg, round, expr\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG BENCHMARK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =========================\n",
    "# 1. ĐỌC & CHUẨN HÓA DỮ LIỆU BRONZE\n",
    "# =========================\n",
    "\n",
    "df_benchmark = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"encoding\", \"UTF-8\")\n",
    "    .csv(\"s3a://bronze/structured_data/điểm chuẩn các trường (2021-2025)/Điểm_chuẩn_các_ngành_đại_học_năm(2021-2025)*.csv\")\n",
    ")\n",
    "\n",
    "# Chuẩn hóa cột PhuongThuc: bỏ phần \"năm XXXX ...\"\n",
    "df_benchmark = df_benchmark.withColumn(\n",
    "    \"PhuongThuc_cleaned\",\n",
    "    trim(regexp_replace(col(\"PhuongThuc\"), r\"\\s*năm\\s+\\d{4}.*$\", \"\"))\n",
    ")\n",
    "\n",
    "# Lookup tables từ Silver\n",
    "df_selection_lookup     = spark.table(\"nessie.silver_tables.selection_method\")\n",
    "df_subject_group_lookup = spark.table(\"nessie.silver_tables.subject_group\")\n",
    "df_grading_scale_lookup = spark.table(\"nessie.silver_tables.grading_scale\")\n",
    "\n",
    "# Join lookup + chuẩn hóa\n",
    "df_benchmark_base = (\n",
    "    df_benchmark\n",
    "    .join(\n",
    "        df_selection_lookup,\n",
    "        df_benchmark[\"PhuongThuc_cleaned\"] == df_selection_lookup[\"selectionMethodName\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    .join(\n",
    "        df_subject_group_lookup,\n",
    "        df_benchmark[\"KhoiThi\"] == df_subject_group_lookup[\"subjectGroupName\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    .join(\n",
    "        df_grading_scale_lookup,\n",
    "        trim(df_benchmark[\"PhanLoaiThangDiem\"]) == df_grading_scale_lookup[\"description\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"MaTruong\").cast(\"string\").alias(\"schoolId\"),\n",
    "        col(\"MaNganh\").cast(\"string\").alias(\"majorId\"),\n",
    "        col(\"subjectGroupId\").cast(\"int\"),\n",
    "        col(\"selectionMethodId\").cast(\"int\"),\n",
    "        col(\"gradingScaleId\").cast(\"int\"),\n",
    "        col(\"Nam\").cast(\"int\").alias(\"year\"),\n",
    "        col(\"DiemChuan\").cast(\"double\").alias(\"score\"),\n",
    "    )\n",
    "    .filter(\n",
    "        col(\"schoolId\").isNotNull() &\n",
    "        col(\"majorId\").isNotNull() &\n",
    "        col(\"gradingScaleId\").isNotNull() &\n",
    "        col(\"year\").isNotNull() &\n",
    "        col(\"score\").isNotNull() &\n",
    "        col(\"selectionMethodId\").isNotNull()\n",
    "        # col(\"subjectGroupId\").isNotNull()  # nếu muốn bắt buộc khối thi thì mở dòng này\n",
    "    )\n",
    "    .dropDuplicates([\n",
    "        \"schoolId\",\n",
    "        \"majorId\",\n",
    "        \"subjectGroupId\",\n",
    "        \"selectionMethodId\",\n",
    "        \"year\",\n",
    "        \"gradingScaleId\",\n",
    "        \"score\"\n",
    "    ])\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 2. GROUP BY & LẤY ĐIỂM TRUNG BÌNH\n",
    "# =========================\n",
    "\n",
    "df_benchmark_grouped = (\n",
    "    df_benchmark_base\n",
    "    .groupBy(\n",
    "        \"schoolId\",\n",
    "        \"majorId\",\n",
    "        \"subjectGroupId\",\n",
    "        \"selectionMethodId\",\n",
    "        \"gradingScaleId\",\n",
    "        \"year\"\n",
    "    )\n",
    "    .agg(\n",
    "        round(avg(\"score\"), 2).alias(\"score\")\n",
    "    )\n",
    ")\n",
    "\n",
    "table_name = \"nessie.silver_tables.benchmark\"\n",
    "\n",
    "# =========================\n",
    "# 3. CHECK BẢNG SILVER ĐÃ TỒN TẠI CHƯA\n",
    "# =========================\n",
    "\n",
    "try:\n",
    "    spark.table(table_name)\n",
    "    table_exists = True\n",
    "    print(f\"Bảng {table_name} đã tồn tại → dùng MERGE (upsert).\")\n",
    "except Exception:\n",
    "    table_exists = False\n",
    "    print(f\"Bảng {table_name} chưa tồn tại → tạo mới full-load.\")\n",
    "\n",
    "# =========================\n",
    "# 4. LẦN ĐẦU: TẠO BẢNG FULL (DÙNG xxhash64 LÀM benchmarkId)\n",
    "# =========================\n",
    "\n",
    "if not table_exists:\n",
    "    df_benchmark_silver = (\n",
    "        df_benchmark_grouped\n",
    "        .withColumn(\n",
    "            \"benchmarkId\",\n",
    "            expr(\n",
    "                \"\"\"\n",
    "                CAST(\n",
    "                    xxhash64(\n",
    "                        schoolId,\n",
    "                        majorId,\n",
    "                        COALESCE(subjectGroupId, -1),\n",
    "                        selectionMethodId,\n",
    "                        gradingScaleId,\n",
    "                        year\n",
    "                    ) AS BIGINT\n",
    "                )\n",
    "                \"\"\"\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"created_at\", current_timestamp())\n",
    "        .withColumn(\"updated_at\", current_timestamp())\n",
    "        .select(\n",
    "            \"benchmarkId\",\n",
    "            \"schoolId\",\n",
    "            \"majorId\",\n",
    "            \"subjectGroupId\",\n",
    "            \"selectionMethodId\",\n",
    "            \"gradingScaleId\",\n",
    "            \"year\",\n",
    "            \"score\",\n",
    "            \"created_at\",\n",
    "            \"updated_at\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_benchmark_silver.writeTo(table_name).using(\"iceberg\").createOrReplace()\n",
    "    print(f\"Đã tạo mới benchmark với {df_benchmark_silver.count()} dòng\")\n",
    "\n",
    "# =========================\n",
    "# 5. CÁC LẦN SAU: MERGE / UPSERT\n",
    "# =========================\n",
    "\n",
    "else:\n",
    "    # Staging từ bronze sau khi chuẩn hóa + group\n",
    "    df_staging = (\n",
    "        df_benchmark_grouped\n",
    "        .withColumn(\"created_at\", current_timestamp())\n",
    "        .withColumn(\"updated_at\", current_timestamp())\n",
    "    )\n",
    "\n",
    "    df_staging.createOrReplaceTempView(\"benchmark_staging\")\n",
    "\n",
    "    # MERGE:\n",
    "    # - MATCHED: update score + updated_at\n",
    "    # - NOT MATCHED: insert bản ghi mới với benchmarkId = hash(business key)\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {table_name} AS t\n",
    "        USING benchmark_staging AS s\n",
    "        ON  t.schoolId          = s.schoolId\n",
    "        AND t.majorId           = s.majorId\n",
    "        AND COALESCE(t.subjectGroupId,  -1) = COALESCE(s.subjectGroupId,  -1)\n",
    "        AND t.selectionMethodId = s.selectionMethodId\n",
    "        AND t.gradingScaleId    = s.gradingScaleId\n",
    "        AND t.year              = s.year\n",
    "\n",
    "        WHEN MATCHED THEN UPDATE SET\n",
    "            t.score      = s.score,\n",
    "            t.updated_at = current_timestamp()\n",
    "\n",
    "        WHEN NOT MATCHED THEN INSERT (\n",
    "            benchmarkId,\n",
    "            schoolId,\n",
    "            majorId,\n",
    "            subjectGroupId,\n",
    "            selectionMethodId,\n",
    "            gradingScaleId,\n",
    "            year,\n",
    "            score,\n",
    "            created_at,\n",
    "            updated_at\n",
    "        ) VALUES (\n",
    "            CAST(\n",
    "                xxhash64(\n",
    "                    s.schoolId,\n",
    "                    s.majorId,\n",
    "                    COALESCE(s.subjectGroupId, -1),\n",
    "                    s.selectionMethodId,\n",
    "                    s.gradingScaleId,\n",
    "                    s.year\n",
    "                ) AS BIGINT\n",
    "            ),\n",
    "            s.schoolId,\n",
    "            s.majorId,\n",
    "            s.subjectGroupId,\n",
    "            s.selectionMethodId,\n",
    "            s.gradingScaleId,\n",
    "            s.year,\n",
    "            s.score,\n",
    "            s.created_at,\n",
    "            s.updated_at\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Đã MERGE dữ liệu mới vào bảng benchmark\")\n",
    "\n",
    "# =========================\n",
    "# 6. VERIFY\n",
    "# =========================\n",
    "\n",
    "spark.table(table_name).show(5, truncate=False)\n",
    "spark.table(table_name).groupBy(\"year\").count().orderBy(\"year\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18094e01",
   "metadata": {},
   "source": [
    "## 7. Load Bảng REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dbdfc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOAD BẢNG REGION\n",
      "================================================================================\n",
      "Đã ghi 64 dòng vào region\n",
      "+--------+-----------------------+-------------------------+-------------------------+\n",
      "|regionId|regionName             |created_at               |updated_at               |\n",
      "+--------+-----------------------+-------------------------+-------------------------+\n",
      "|01      |Sở GDĐT Hà Nội         |2025-12-01 15:39:24.67929|2025-12-01 15:39:24.67929|\n",
      "|02      |Sở GDĐT TP. Hồ Chí Minh|2025-12-01 15:39:24.67929|2025-12-01 15:39:24.67929|\n",
      "|03      |Sở GDĐT Hải Phòng      |2025-12-01 15:39:24.67929|2025-12-01 15:39:24.67929|\n",
      "|04      |Sở GDĐT Đà Nẵng        |2025-12-01 15:39:24.67929|2025-12-01 15:39:24.67929|\n",
      "|05      |Sở GDĐT Hà Giang       |2025-12-01 15:39:24.67929|2025-12-01 15:39:24.67929|\n",
      "|06      |Sở GDĐT Cao Bằng       |2025-12-01 15:39:24.67929|2025-12-01 15:39:24.67929|\n",
      "|07      |Sở GDĐT Lai Châu       |2025-12-01 15:39:24.67929|2025-12-01 15:39:24.67929|\n",
      "|08      |Sở GDĐT Lào Cai        |2025-12-01 15:39:24.67929|2025-12-01 15:39:24.67929|\n",
      "|09      |Sở GDĐT Tuyên Quang    |2025-12-01 15:39:24.67929|2025-12-01 15:39:24.67929|\n",
      "|10      |Sở GDĐT Lạng Sơn       |2025-12-01 15:39:24.67929|2025-12-01 15:39:24.67929|\n",
      "+--------+-----------------------+-------------------------+-------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG REGION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_region = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"encoding\", \"UTF-8\").csv(\"s3a://bronze/structured_data/region.csv\")\n",
    "df_region_silver = df_region.select(\n",
    "    lpad(col(df_region.columns[0]).cast(\"string\"), 2, \"0\").alias(\"regionId\"),  # Format thành 2 chữ số: \"1\" -> \"01\"\n",
    "    col(df_region.columns[1]).cast(\"string\").alias(\"regionName\"),\n",
    "    current_timestamp().alias(\"created_at\"),\n",
    "    current_timestamp().alias(\"updated_at\")\n",
    ").filter(col(\"regionId\").isNotNull() & col(\"regionName\").isNotNull()).dropDuplicates([\"regionId\"])\n",
    "\n",
    "df_region_silver.writeTo(\"nessie.silver_tables.region\").using(\"iceberg\").createOrReplace()\n",
    "print(f\"Đã ghi {df_region_silver.count()} dòng vào region\")\n",
    "\n",
    "# Verify\n",
    "spark.table(\"nessie.silver_tables.region\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6217cf",
   "metadata": {},
   "source": [
    "## 8. Load Bảng STUDENT_SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d52c6fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOAD BẢNG STUDENT_SCORES - INCREMENTAL BY FILE (DELETE + APPEND)\n",
      "================================================================================\n",
      "❌ Không có file mới nào, dừng job.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, trim, regexp_replace, current_timestamp, lit,\n",
    "    concat, substring, udf, input_file_name, regexp_extract\n",
    ")\n",
    "from pyspark.sql.types import MapType, IntegerType, DoubleType\n",
    "from typing import Dict\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG STUDENT_SCORES - INCREMENTAL BY FILE (DELETE + APPEND)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =====================================================\n",
    "# 0. TẠO BẢNG LOG INGEST (LƯU FILE ĐÃ XỬ LÝ) NẾU CHƯA CÓ\n",
    "# =====================================================\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nessie.silver_tables.student_scores_ingest_log (\n",
    "    path STRING,\n",
    "    year INT,\n",
    "    processed_at TIMESTAMP\n",
    ") USING iceberg\n",
    "\"\"\")\n",
    "\n",
    "# =====================================================\n",
    "# 1. LẤY DANH SÁCH TẤT CẢ FILE CSV HIỆN CÓ TRONG BRONZE\n",
    "#    + TRỪ ĐI NHỮNG FILE ĐÃ INGEST (log)\n",
    "# =====================================================\n",
    "\n",
    "df_files = (\n",
    "    spark.read.format(\"binaryFile\")\n",
    "    .option(\"pathGlobFilter\", \"*.csv\")\n",
    "    .load(\"s3a://bronze/structured_data/điểm từng thí sinh/*/*.csv\")\n",
    "    .select(\"path\")\n",
    ")\n",
    "\n",
    "df_log = spark.table(\"nessie.silver_tables.student_scores_ingest_log\")\n",
    "\n",
    "df_new_files = df_files.join(df_log, on=\"path\", how=\"left_anti\")\n",
    "new_files = [r.path for r in df_new_files.collect()]\n",
    "\n",
    "if not new_files:\n",
    "    print(\"❌ Không có file mới nào, dừng job.\")\n",
    "else:\n",
    "    print(f\"✅ Phát hiện {len(new_files)} file mới cần xử lý.\")\n",
    "\n",
    "    # =====================================================\n",
    "    # 2. ĐỌC CHỈ CÁC FILE MỚI + THÊM CỘT YEAR\n",
    "    # =====================================================\n",
    "\n",
    "    df_scores_raw = (\n",
    "        spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"false\")\n",
    "        .option(\"encoding\", \"UTF-8\")\n",
    "        .csv(new_files)\n",
    "        .withColumn(\"path\", input_file_name())\n",
    "    )\n",
    "\n",
    "    df_scores_raw = df_scores_raw.withColumn(\n",
    "        \"Year\",\n",
    "        regexp_extract(col(\"path\"), r\"/(\\d{4})/\", 1).cast(\"int\")\n",
    "    )\n",
    "\n",
    "    # =====================================================\n",
    "    # 3. LOAD LOOKUP MÔN HỌC\n",
    "    # =====================================================\n",
    "\n",
    "    df_subject_lookup = spark.table(\"nessie.silver_tables.subject\").select(\"subjectId\", \"subjectName\")\n",
    "    subject_map = {row.subjectName: row.subjectId for row in df_subject_lookup.collect()}\n",
    "    print(f\"\\nĐã load {len(subject_map)} môn học để mapping\")\n",
    "\n",
    "    # =====================================================\n",
    "    # 4. UDF PARSE ĐIỂM → Map<subjectId, score>\n",
    "    # =====================================================\n",
    "\n",
    "    def parse_scores_with_subject_id(score_string: str) -> Dict[int, float]:\n",
    "        if not score_string or score_string.strip() == \"\":\n",
    "            return {}\n",
    "        scores_dict = {}\n",
    "        try:\n",
    "            pairs = score_string.split(\",\")\n",
    "            for pair in pairs:\n",
    "                if \":\" in pair:\n",
    "                    subject_name, score = pair.split(\":\")\n",
    "                    subject_name = subject_name.strip()\n",
    "                    # Map tên môn -> subjectId\n",
    "                    if subject_name in subject_map:\n",
    "                        subject_id = subject_map[subject_name]\n",
    "                        try:\n",
    "                            scores_dict[subject_id] = float(score.strip())\n",
    "                        except:\n",
    "                            pass\n",
    "        except:\n",
    "            pass\n",
    "        return scores_dict\n",
    "\n",
    "    parse_scores_udf = udf(parse_scores_with_subject_id, MapType(IntegerType(), DoubleType()))\n",
    "\n",
    "    # =====================================================\n",
    "    # 5. TRANSFORM → DATAFRAME STAGING (KHÔNG MERGE)\n",
    "    # =====================================================\n",
    "\n",
    "    # 1️⃣ Biến đầy đủ để append vào silver\n",
    "    df_student_scores_stage = (\n",
    "        df_scores_raw\n",
    "        .withColumn(\"studentId\", concat(col(\"SBD\"), col(\"Year\").cast(\"string\")))\n",
    "        .withColumn(\"scores\", parse_scores_udf(col(\"DiemThi\")))   # UDF ở đây\n",
    "        .withColumn(\"regionId\", substring(col(\"SBD\"), 1, 2).cast(\"string\"))\n",
    "        .select(\n",
    "            col(\"studentId\").cast(\"string\"),\n",
    "            col(\"regionId\").cast(\"string\"),\n",
    "            col(\"Year\").cast(\"int\").alias(\"year\"),\n",
    "            col(\"scores\")\n",
    "        )\n",
    "        .filter(\n",
    "            col(\"studentId\").isNotNull() &\n",
    "            col(\"year\").isNotNull() &\n",
    "            col(\"scores\").isNotNull()\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 2️⃣ Biến thứ hai chỉ có studentId — KHÔNG UDF → dùng để DELETE\n",
    "    df_student_ids = (\n",
    "        df_scores_raw\n",
    "        .withColumn(\"studentId\", concat(col(\"SBD\"), col(\"Year\").cast(\"string\")))\n",
    "        .select(\"studentId\")\n",
    "        .filter(col(\"studentId\").isNotNull())\n",
    "        # .dropDuplicates([\"studentId\"])\n",
    "    )\n",
    "    \n",
    "    df_student_ids.createOrReplaceTempView(\"student_scores_new_ids\")\n",
    "\n",
    "\n",
    "    staging_count = df_student_scores_stage.count()\n",
    "    print(f\"Staging có {staging_count:,} dòng.\")\n",
    "\n",
    "    table_name = \"nessie.silver_tables.student_scores\"\n",
    "\n",
    "        # =====================================================\n",
    "    # 6. XOÁ studentId CŨ BẰNG CÁCH COLLECT RA PYTHON + DELETE IN (...)\n",
    "    # =====================================================\n",
    "\n",
    "    # Lấy list studentId distinct trong batch mới\n",
    "    new_ids = [\n",
    "        row.studentId\n",
    "        for row in df_student_scores_stage.select(\"studentId\").distinct().collect()\n",
    "    ]\n",
    "\n",
    "    print(f\"Số studentId distinct trong batch mới: {len(new_ids):,}\")\n",
    "\n",
    "    # Kiểm tra bảng silver đã tồn tại chưa\n",
    "    try:\n",
    "        spark.table(table_name)\n",
    "        table_exists = True\n",
    "        print(f\"Bảng {table_name} đã tồn tại → DELETE theo list studentId + APPEND.\")\n",
    "    except Exception:\n",
    "        table_exists = False\n",
    "        print(f\"Bảng {table_name} chưa tồn tại → tạo mới từ batch, không cần xoá.\")\n",
    "\n",
    "    silver_count = spark.table(table_name).count() if table_exists else 0\n",
    "    print(f\"Số dòng trong bảng silver hiện tại: {silver_count:,}\")\n",
    "\n",
    "    if not table_exists:\n",
    "        # 1️⃣ BẢNG CHƯA TỒN TẠI → TẠO MỚI\n",
    "        (\n",
    "            df_student_scores_stage\n",
    "            .withColumn(\"created_at\", current_timestamp())\n",
    "            .withColumn(\"updated_at\", current_timestamp())\n",
    "            .writeTo(table_name)\n",
    "            .using(\"iceberg\")\n",
    "            .createOrReplace()\n",
    "        )\n",
    "        print(f\"✅ Đã tạo mới bảng {table_name} với {staging_count:,} dòng.\")\n",
    "    \n",
    "    elif silver_count == 0:\n",
    "        # 2️⃣ BẢNG TỒN TẠI NHƯNG RỖNG → KHÔNG XOÁ, CHỈ APPEND\n",
    "        print(\"⚠️ Bảng silver đã tồn tại nhưng rỗng → chỉ append, không xoá.\")\n",
    "    \n",
    "        (\n",
    "            df_student_scores_stage\n",
    "            .withColumn(\"created_at\", current_timestamp())\n",
    "            .withColumn(\"updated_at\", current_timestamp())\n",
    "            .writeTo(table_name)\n",
    "            .using(\"iceberg\")\n",
    "            .append()\n",
    "        )\n",
    "        print(f\"✅ Đã append {staging_count:,} dòng mới vào {table_name}.\")\n",
    "    \n",
    "    elif new_ids:\n",
    "        # 3️⃣ BẢNG TỒN TẠI VÀ new_ids KHÔNG RỖNG → DELETE + APPEND\n",
    "        print(\"Bảng silver có dữ liệu → DELETE + APPEND.\")\n",
    "    \n",
    "        chunk_size = 500\n",
    "        from math import ceil\n",
    "    \n",
    "        num_chunks = ceil(len(new_ids) / chunk_size)\n",
    "        print(f\"Chia studentId thành {num_chunks} chunk để xoá...\")\n",
    "    \n",
    "        for i in range(num_chunks):\n",
    "            chunk = new_ids[i * chunk_size:(i + 1) * chunk_size]\n",
    "            escaped_ids = [sid.replace(\"'\", \"''\") for sid in chunk]\n",
    "            in_list = \",\".join([f\"'{sid}'\" for sid in escaped_ids])\n",
    "    \n",
    "            sql_delete = f\"\"\"\n",
    "                DELETE FROM {table_name}\n",
    "                WHERE studentId IN ({in_list})\n",
    "            \"\"\"\n",
    "            spark.sql(sql_delete)\n",
    "    \n",
    "        print(\"✅ Đã xoá xong các studentId cũ trong silver.\")\n",
    "    \n",
    "        (\n",
    "            df_student_scores_stage\n",
    "            .withColumn(\"created_at\", current_timestamp())\n",
    "            .withColumn(\"updated_at\", current_timestamp())\n",
    "            .writeTo(table_name)\n",
    "            .using(\"iceberg\")\n",
    "            .append()\n",
    "        )\n",
    "        print(f\"✅ Đã append {staging_count:,} dòng mới.\")\n",
    "    \n",
    "    else:\n",
    "        # 4️⃣ new_ids rỗng → không xoá, không append\n",
    "        print(\"⚠️ Batch mới không có studentId nào hợp lệ → không làm gì cả.\")\n",
    "\n",
    "\n",
    "    # =====================================================\n",
    "    # 7. GHI LOG FILE ĐÃ XỬ LÝ\n",
    "    # =====================================================\n",
    "\n",
    "    from pyspark.sql.functions import array, explode\n",
    "\n",
    "    df_new_files_log = (\n",
    "        df_new_files\n",
    "        .withColumn(\"year\", regexp_extract(col(\"path\"), r\"/(\\d{4})/\", 1).cast(\"int\"))\n",
    "        .withColumn(\"processed_at\", current_timestamp())\n",
    "    )\n",
    "\n",
    "    (\n",
    "        df_new_files_log\n",
    "        .writeTo(\"nessie.silver_tables.student_scores_ingest_log\")\n",
    "        .using(\"iceberg\")\n",
    "        .append()\n",
    "    )\n",
    "\n",
    "    print(f\"Đã ghi log {df_new_files_log.count():,} file đã xử lý.\")\n",
    "\n",
    "    # =====================================================\n",
    "    # 8. VERIFY\n",
    "    # =====================================================\n",
    "\n",
    "    print(\"\\nMẫu dữ liệu student_scores:\")\n",
    "    spark.table(table_name).show(5, truncate=False)\n",
    "    spark.table(table_name).groupBy(\"year\").count().orderBy(\"year\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce95322d",
   "metadata": {},
   "source": [
    "## 9. Load Bảng ARTICLE và COMMENT từ TikTok Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d2b30ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOAD BẢNG ARTICLE VÀ COMMENT TỪ TIKTOK DATA (INCREMENTAL)\n",
      "================================================================================\n",
      "Đọc danh sách file từ: s3a://bronze/MangXaHoi/tiktok-data/comments/*.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số file comment mới: 1\n",
      "max articleID hiện có: 1769\n",
      "max commentID hiện có: 212891\n",
      "\n",
      "Xử lý file: s3a://bronze/MangXaHoi/tiktok-data/comments/tiktok_comments_2025-09-10T15-05-15.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video @lafbeeday/video/7379584620364352769 là mới → tạo articleID=1770\n",
      "  Tìm thấy header comment ở dòng 16\n",
      "  Header: STT,Tên,Tag tên,URL,Comment,Time,Likes,Level Comment,Replied To Tag Name,Number of Replies\n",
      "  Tìm thấy 23 comment (sau khi parse CSV)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Đã ghi thêm 23 comment cho articleID=1770 (tổng tạm: 23)\n",
      "  ==> Tổng cộng ghi 23 comment cho articleID=1770\n",
      "Đã xử lý xong file s3a://bronze/MangXaHoi/tiktok-data/comments/tiktok_comments_2025-09-10T15-05-15.csv\n",
      "\n",
      "================================================================================\n",
      "TỔNG ARTICLE MỚI:  1\n",
      "TỔNG COMMENT MỚI:  23\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import re, csv, io\n",
    "from datetime import datetime\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import input_file_name, col, trim, current_timestamp\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, IntegerType, StringType, TimestampType\n",
    ")\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# ===== Helpers =====\n",
    "def clean_url(url: str):\n",
    "    \"\"\"Bỏ prefix 'https://www.tiktok.com/' trong urlUser\"\"\"\n",
    "    if not url:\n",
    "        return url\n",
    "    prefix = \"https://www.tiktok.com/\"\n",
    "    if url.startswith(prefix):\n",
    "        return url[len(prefix):]\n",
    "    return url\n",
    "\n",
    "def clean_reply_to(name: str):\n",
    "    \"\"\"Bỏ suffix '?lang=vi-VN' trong replyTo\"\"\"\n",
    "    if not name:\n",
    "        return name\n",
    "    suffix = \"?lang=vi-VN\"\n",
    "    if name.endswith(suffix):\n",
    "        return name[:-len(suffix)]\n",
    "    return name\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG ARTICLE VÀ COMMENT TỪ TIKTOK DATA (INCREMENTAL)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ===== 0. Tạo bảng log file nếu chưa có =====\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nessie.silver_tables.tiktok_comment_files_log (\n",
    "  file_path STRING,\n",
    "  load_time TIMESTAMP\n",
    ") USING iceberg\n",
    "\"\"\")\n",
    "\n",
    "# ===== 1. Xác định các file comment MỚI =====\n",
    "tiktok_path = \"s3a://bronze/MangXaHoi/tiktok-data/comments/*.csv\"\n",
    "print(f\"Đọc danh sách file từ: {tiktok_path}\")\n",
    "\n",
    "df_all_files = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"false\")\n",
    "         .option(\"encoding\", \"UTF-8\")\n",
    "         .csv(tiktok_path)\n",
    "         .select(input_file_name().alias(\"file_path\"))\n",
    "         .distinct()\n",
    ")\n",
    "\n",
    "try:\n",
    "    df_processed = spark.table(\"nessie.silver_tables.tiktok_comment_files_log\") \\\n",
    "                        .select(\"file_path\").distinct()\n",
    "except AnalysisException:\n",
    "    df_processed = spark.createDataFrame([], \"file_path STRING\")\n",
    "\n",
    "df_new_files = df_all_files.join(df_processed, \"file_path\", \"left_anti\")\n",
    "\n",
    "tiktok_files = [r.file_path for r in df_new_files.collect()]\n",
    "print(f\"Số file comment mới: {len(tiktok_files)}\")\n",
    "\n",
    "if not tiktok_files:\n",
    "    print(\"Không có file comment mới, bỏ qua phần load comment.\")\n",
    "else:\n",
    "    # ===== 2. Lấy max articleID & commentID hiện có =====\n",
    "    try:\n",
    "        max_article_id = spark.table(\"nessie.silver_tables.article\") \\\n",
    "                              .agg(F.max(\"articleID\")).collect()[0][0]\n",
    "        if max_article_id is None:\n",
    "            max_article_id = 0\n",
    "    except AnalysisException:\n",
    "        max_article_id = 0\n",
    "\n",
    "    try:\n",
    "        max_comment_id = spark.table(\"nessie.silver_tables.comment\") \\\n",
    "                              .agg(F.max(\"commentID\")).collect()[0][0]\n",
    "        if max_comment_id is None:\n",
    "            max_comment_id = 0\n",
    "    except AnalysisException:\n",
    "        max_comment_id = 0\n",
    "\n",
    "    article_counter = max_article_id\n",
    "    comment_counter = max_comment_id\n",
    "\n",
    "    print(f\"max articleID hiện có: {article_counter}\")\n",
    "    print(f\"max commentID hiện có: {comment_counter}\")\n",
    "\n",
    "    # ===== Schema article & comment =====\n",
    "    article_schema = StructType([\n",
    "        StructField(\"articleID\", IntegerType(), False),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"description\", StringType(), True),\n",
    "        StructField(\"author\", StringType(), True),\n",
    "        StructField(\"url\", StringType(), True),\n",
    "        StructField(\"timePublish\", TimestampType(), True),\n",
    "        StructField(\"likeCount\", IntegerType(), True),\n",
    "        StructField(\"commentCount\", IntegerType(), True),\n",
    "        StructField(\"shareCount\", IntegerType(), True),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "        StructField(\"updated_at\", TimestampType(), True)\n",
    "    ])\n",
    "\n",
    "    comment_schema = StructType([\n",
    "        StructField(\"commentID\", IntegerType(), False),\n",
    "        StructField(\"articleID\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"tagName\", StringType(), True),\n",
    "        StructField(\"urlUser\", StringType(), True),\n",
    "        StructField(\"comment\", StringType(), True),\n",
    "        StructField(\"commentTime\", TimestampType(), True),\n",
    "        StructField(\"commentLike\", IntegerType(), True),\n",
    "        StructField(\"levelComment\", IntegerType(), True),\n",
    "        StructField(\"replyTo\", StringType(), True),\n",
    "        StructField(\"numberOfReply\", IntegerType(), True),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "        StructField(\"updated_at\", TimestampType(), True)\n",
    "    ])\n",
    "\n",
    "    total_article_new = 0\n",
    "    total_comment_new = 0\n",
    "\n",
    "    # ===== 3. Xử lý từng file comment MỚI =====\n",
    "    for file_path in tiktok_files:\n",
    "        try:\n",
    "            print(f\"\\nXử lý file: {file_path}\")\n",
    "\n",
    "            # Đọc raw để lấy metadata\n",
    "            df_raw = (spark.read\n",
    "                      .option(\"header\", \"false\")\n",
    "                      .option(\"encoding\", \"UTF-8\")\n",
    "                      .csv(file_path))\n",
    "            rows = df_raw.collect()\n",
    "\n",
    "            # ----- Parse metadata bài post -----\n",
    "            post_url = \"\"\n",
    "            author = \"\"\n",
    "            tag_name = \"\"\n",
    "            author_url = \"\"\n",
    "            time_publish = \"\"\n",
    "            like_count = 0\n",
    "            comment_count_meta = 0\n",
    "            share_count = 0\n",
    "            title = \"\"\n",
    "\n",
    "            for r in rows[:20]:\n",
    "                line = r[0] if r[0] else \"\"\n",
    "\n",
    "                if \"Post URL:\" in line:\n",
    "                    post_url = line.split(\"Post URL:\")[1].strip()\n",
    "                elif \"Người đăng:\" in line:\n",
    "                    author = line.split(\"Người đăng:\")[1].strip()\n",
    "                elif \"Tag người đăng:\" in line:\n",
    "                    tag_name = line.split(\"Tag người đăng:\")[1].strip()\n",
    "                elif \"URL người đăng:\" in line:\n",
    "                    author_url = line.split(\"URL người đăng:\")[1].strip()\n",
    "                elif \"Thời gian đăng:\" in line:\n",
    "                    time_str = line.split(\"Thời gian đăng:\")[1].strip()\n",
    "                    try:\n",
    "                        time_publish = datetime.strptime(time_str, \"%d-%m-%Y\")\n",
    "                    except:\n",
    "                        time_publish = datetime.now()\n",
    "                elif \"Số lượt tym:\" in line:\n",
    "                    tym_str = line.split(\"Số lượt tym:\")[1].strip()\n",
    "                    try:\n",
    "                        if \"K\" in tym_str:\n",
    "                            like_count = int(float(tym_str.replace(\"K\", \"\")) * 1000)\n",
    "                        else:\n",
    "                            like_count = int(tym_str)\n",
    "                    except:\n",
    "                        like_count = 0\n",
    "                elif \"Số lượt comment:\" in line:\n",
    "                    try:\n",
    "                        c_str = line.split(\"Số lượt comment:\")[1].strip()\n",
    "                        if \"K\" in c_str:\n",
    "                            comment_count_meta = int(float(c_str.replace(\"K\", \"\")) * 1000)\n",
    "                        else:\n",
    "                            comment_count_meta = int(c_str)\n",
    "                    except:\n",
    "                        comment_count_meta = 0\n",
    "                elif \"Số lượt share:\" in line:\n",
    "                    share_str = line.split(\"Số lượt share:\")[1].strip()\n",
    "                    try:\n",
    "                        if share_str != \"N/A\":\n",
    "                            if \"K\" in share_str:\n",
    "                                share_count = int(float(share_str.replace(\"K\", \"\")) * 1000)\n",
    "                            else:\n",
    "                                share_count = int(share_str)\n",
    "                        else:\n",
    "                            share_count = 0\n",
    "                    except:\n",
    "                        share_count = 0\n",
    "                elif \"Mô tả của bài đăng:\" in line:\n",
    "                    first_part = line.split(\"Mô tả của bài đăng:\")[1].strip()\n",
    "\n",
    "                    if first_part.endswith('\"') and first_part.count('\"') >= 2:\n",
    "                        title = first_part.strip('\"')\n",
    "                    else:\n",
    "                        desc_lines = []\n",
    "                        if first_part != \"\":\n",
    "                            desc_lines.append(first_part.lstrip('\"'))\n",
    "\n",
    "                        j = rows.index(r) + 1\n",
    "                        while j < len(rows):\n",
    "                            next_line = rows[j][0] if rows[j][0] else \"\"\n",
    "                            desc_lines.append(next_line)\n",
    "                            if next_line.endswith('\"'):\n",
    "                                break\n",
    "                            j += 1\n",
    "\n",
    "                        full_desc = \"\\n\".join(desc_lines).strip()\n",
    "                        title = full_desc.strip('\"').strip()\n",
    "\n",
    "            video_url = clean_url(post_url)\n",
    "\n",
    "            # ----- Xác định articleID (cũ → dùng lại, mới → tăng counter) -----\n",
    "            try:\n",
    "                df_article_exist = (spark.table(\"nessie.silver_tables.article\")\n",
    "                                         .select(\"articleID\", \"url\")\n",
    "                                         .filter(col(\"url\") == video_url))\n",
    "                exist_rows = df_article_exist.collect()\n",
    "            except AnalysisException:\n",
    "                exist_rows = []\n",
    "\n",
    "            if exist_rows:\n",
    "                article_id = exist_rows[0].articleID\n",
    "                is_new_article = False\n",
    "                print(f\"Video {video_url} đã có articleID={article_id} → xoá comment cũ rồi insert lại\")\n",
    "                # Xoá comment cũ\n",
    "                spark.sql(f\"\"\"\n",
    "                    DELETE FROM nessie.silver_tables.comment\n",
    "                    WHERE articleID = {article_id}\n",
    "                \"\"\")\n",
    "            else:\n",
    "                article_counter += 1\n",
    "                article_id = article_counter\n",
    "                is_new_article = True\n",
    "                print(f\"Video {video_url} là mới → tạo articleID={article_id}\")\n",
    "\n",
    "            # ----- Nếu là article mới → insert vào bảng article -----\n",
    "            if is_new_article:\n",
    "                article_data = {\n",
    "                    \"articleID\": article_id,\n",
    "                    \"title\": title,\n",
    "                    \"description\": None,   # Description sẽ được cập nhật sau từ sub\n",
    "                    \"author\": author,\n",
    "                    \"url\": video_url,\n",
    "                    \"timePublish\": time_publish,\n",
    "                    \"likeCount\": like_count,\n",
    "                    \"commentCount\": comment_count_meta,\n",
    "                    \"shareCount\": share_count,\n",
    "                    \"type\": \"TikTok\",\n",
    "                    \"created_at\": datetime.now(),\n",
    "                    \"updated_at\": datetime.now()\n",
    "                }\n",
    "                df_article_one = spark.createDataFrame([article_data], article_schema)\n",
    "                (df_article_one.writeTo(\"nessie.silver_tables.article\")\n",
    "                               .using(\"iceberg\")\n",
    "                               .append())\n",
    "                total_article_new += 1\n",
    "\n",
    "            # ----- Đọc lại file dạng text để parse comment chuẩn CSV -----\n",
    "            df_text = spark.read.text(file_path)\n",
    "            text_lines = [r.value if r.value is not None else \"\" for r in df_text.collect()]\n",
    "\n",
    "            header_row_index = -1\n",
    "            header_line = None\n",
    "\n",
    "            for i, line in enumerate(text_lines):\n",
    "                line_norm = line.strip()\n",
    "                if (\n",
    "                    line_norm.startswith(\"STT,\")\n",
    "                    and \"Tên\" in line_norm\n",
    "                    and \"Tag tên\" in line_norm\n",
    "                    and \"URL\" in line_norm\n",
    "                    and \"Comment\" in line_norm\n",
    "                    and \"Time\" in line_norm\n",
    "                    and \"Likes\" in line_norm\n",
    "                    and \"Level Comment\" in line_norm\n",
    "                    and \"Replied To Tag Name\" in line_norm\n",
    "                    and \"Number of Replies\" in line_norm\n",
    "                ):\n",
    "                    header_row_index = i\n",
    "                    header_line = line_norm\n",
    "                    break\n",
    "\n",
    "            if header_row_index >= 0:\n",
    "                print(f\"  Tìm thấy header comment ở dòng {header_row_index}\")\n",
    "                print(f\"  Header: {header_line}\")\n",
    "\n",
    "                csv_lines = text_lines[header_row_index:]\n",
    "                csv_str = \"\\n\".join(csv_lines)\n",
    "\n",
    "                reader = csv.DictReader(io.StringIO(csv_str))\n",
    "                comment_dict_rows = list(reader)\n",
    "                print(f\"  Tìm thấy {len(comment_dict_rows)} comment (sau khi parse CSV)\")\n",
    "\n",
    "                file_comments = []\n",
    "\n",
    "                for r in comment_dict_rows:\n",
    "                    try:\n",
    "                        comment_time_str = r.get(\"Time\", \"\") or \"\"\n",
    "                        try:\n",
    "                            comment_time = datetime.strptime(comment_time_str, \"%d-%m-%Y\")\n",
    "                        except:\n",
    "                            comment_time = datetime.now()\n",
    "\n",
    "                        level_comment = 2 if (r.get(\"Level Comment\") == \"Yes\") else 1\n",
    "\n",
    "                        reply_to_raw = r.get(\"Replied To Tag Name\", \"\")\n",
    "                        if reply_to_raw and reply_to_raw != \"---\":\n",
    "                            reply_to = clean_reply_to(reply_to_raw)\n",
    "                        else:\n",
    "                            reply_to = None\n",
    "\n",
    "                        likes_raw = r.get(\"Likes\", \"\")\n",
    "                        comment_like = int(likes_raw) if (likes_raw and str(likes_raw).isdigit()) else 0\n",
    "\n",
    "                        num_reply_raw = r.get(\"Number of Replies\", \"\")\n",
    "                        number_of_reply = int(num_reply_raw) if (num_reply_raw and str(num_reply_raw).isdigit()) else 0\n",
    "\n",
    "                        comment_counter += 1\n",
    "                        comment_data = {\n",
    "                            \"commentID\": comment_counter,\n",
    "                            \"articleID\": article_id,\n",
    "                            \"name\": r.get(\"Tên\", \"\") or \"\",\n",
    "                            \"tagName\": r.get(\"Tag tên\", \"\") or \"\",\n",
    "                            \"urlUser\": clean_url(r.get(\"URL\", \"\") or \"\"),\n",
    "                            \"comment\": r.get(\"Comment\", \"\") or \"\",\n",
    "                            \"commentTime\": comment_time,\n",
    "                            \"commentLike\": comment_like,\n",
    "                            \"levelComment\": level_comment,\n",
    "                            \"replyTo\": reply_to,\n",
    "                            \"numberOfReply\": number_of_reply,\n",
    "                            \"created_at\": datetime.now(),\n",
    "                            \"updated_at\": datetime.now()\n",
    "                        }\n",
    "                        file_comments.append(comment_data)\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Lỗi xử lý comment: {e}\")\n",
    "                        continue\n",
    "\n",
    "                # if file_comments:\n",
    "                #     # tạo DF từ list\n",
    "                #     df_comment_batch = spark.createDataFrame(file_comments, comment_schema)\n",
    "                \n",
    "                #     # CHIA NHỎ ra nhiều partition để mỗi task ghi ít dữ liệu hơn\n",
    "                #     df_comment_batch = df_comment_batch.repartition(16)  # thử 4, nếu vẫn OOM thì tăng 8, 16\n",
    "                \n",
    "                #     # số bản ghi đã biết sẵn từ list, không cần .count()\n",
    "                #     # written = len(file_comments)\n",
    "                #     written = 0\n",
    "                \n",
    "                #     (df_comment_batch.writeTo(\"nessie.silver_tables.comment\")\n",
    "                #                      .using(\"iceberg\")\n",
    "                #                      .append())\n",
    "                \n",
    "                #     total_comment_new += written\n",
    "                #     print(f\"  Đã ghi {written} comment mới cho articleID={article_id}\")\n",
    "                # else:\n",
    "                #     print(\"  File này không có comment nào để ghi\")\n",
    "                # chỗ này nên sửa lại nếu chạy trên spark cluster ( hiện đang tối ưu nhỏ hơn để chạy trên jupyter)\n",
    "                if file_comments:\n",
    "                    # CHUNK_SIZE: số comment tối đa mỗi lần ghi\n",
    "                    CHUNK_SIZE = 1000  # nếu vẫn OOM thì giảm còn 2000 hoặc 1000\n",
    "                \n",
    "                    total_for_article = 0\n",
    "                \n",
    "                    for i in range(0, len(file_comments), CHUNK_SIZE):\n",
    "                        chunk = file_comments[i:i + CHUNK_SIZE]\n",
    "                \n",
    "                        # Tạo DataFrame cho chunk hiện tại\n",
    "                        df_chunk = spark.createDataFrame(chunk, comment_schema)\n",
    "                \n",
    "                        # Chia nhỏ thêm trong JVM (nếu muốn)\n",
    "                        df_chunk = df_chunk.repartition(2)  # có thể thử 2 / 4 / 8\n",
    "                \n",
    "                        # Ghi chunk vào Iceberg\n",
    "                        (df_chunk.writeTo(\"nessie.silver_tables.comment\")\n",
    "                                .using(\"iceberg\")\n",
    "                                .append())\n",
    "                \n",
    "                        total_for_article += len(chunk)\n",
    "                        print(f\"  Đã ghi thêm {len(chunk)} comment cho articleID={article_id} (tổng tạm: {total_for_article})\")\n",
    "                \n",
    "                    total_comment_new += total_for_article\n",
    "                    print(f\"  ==> Tổng cộng ghi {total_for_article} comment cho articleID={article_id}\")\n",
    "                else:\n",
    "                    print(\"  File này không có comment nào để ghi\")\n",
    "\n",
    "            else:\n",
    "                print(\"  Không tìm thấy header comment trong file\")\n",
    "\n",
    "            # ----- Ghi log file đã xử lý -----\n",
    "            df_log = spark.createDataFrame(\n",
    "                [(file_path, datetime.now())],\n",
    "                \"file_path STRING, load_time TIMESTAMP\"\n",
    "            )\n",
    "            (df_log.writeTo(\"nessie.silver_tables.tiktok_comment_files_log\")\n",
    "                  .using(\"iceberg\")\n",
    "                  .append())\n",
    "\n",
    "            print(f\"Đã xử lý xong file {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi xử lý file {file_path}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"TỔNG ARTICLE MỚI:  {total_article_new}\")\n",
    "    print(f\"TỔNG COMMENT MỚI:  {total_comment_new}\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ea45b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo DataFrame cho article\n",
    "# if all_articles:\n",
    "#     article_schema = StructType([\n",
    "#         StructField(\"articleID\", IntegerType(), False),\n",
    "#         StructField(\"title\", StringType(), True),\n",
    "#         StructField(\"description\", StringType(), True),\n",
    "#         StructField(\"author\", StringType(), True),\n",
    "#         StructField(\"url\", StringType(), True),\n",
    "#         StructField(\"timePublish\", TimestampType(), True),\n",
    "#         StructField(\"likeCount\", IntegerType(), True),\n",
    "#         StructField(\"commentCount\", IntegerType(), True),\n",
    "#         StructField(\"shareCount\", IntegerType(), True),\n",
    "#         StructField(\"type\", StringType(), True),\n",
    "#         StructField(\"created_at\", TimestampType(), True),\n",
    "#         StructField(\"updated_at\", TimestampType(), True)\n",
    "#     ])\n",
    "    \n",
    "#     df_article_silver = spark.createDataFrame(all_articles, article_schema)\n",
    "    \n",
    "#     # Ghi vào bảng article\n",
    "#     df_article_silver.writeTo(\"nessie.silver_tables.article\").using(\"iceberg\").createOrReplace()\n",
    "#     print(f\"Đã ghi {df_article_silver.count()} dòng vào bảng article\")\n",
    "    \n",
    "#     # Verify\n",
    "#     # spark.table(\"nessie.silver_tables.article\").show(5, truncate=False)\n",
    "# else:\n",
    "#     print(\"Không có dữ liệu article để ghi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e8dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo DataFrame cho comment\n",
    "# if all_comments:\n",
    "#     comment_schema = StructType([\n",
    "#         StructField(\"commentID\", IntegerType(), False),\n",
    "#         StructField(\"articleID\", IntegerType(), True),\n",
    "#         StructField(\"name\", StringType(), True),\n",
    "#         StructField(\"tagName\", StringType(), True),\n",
    "#         StructField(\"urlUser\", StringType(), True),\n",
    "#         StructField(\"comment\", StringType(), True),\n",
    "#         StructField(\"commentTime\", TimestampType(), True),\n",
    "#         StructField(\"commentLike\", IntegerType(), True),\n",
    "#         StructField(\"levelComment\", IntegerType(), True),\n",
    "#         StructField(\"replyTo\", StringType(), True),\n",
    "#         StructField(\"numberOfReply\", IntegerType(), True),\n",
    "#         StructField(\"created_at\", TimestampType(), True),\n",
    "#         StructField(\"updated_at\", TimestampType(), True)\n",
    "#     ])\n",
    "    \n",
    "#     df_comment_silver = spark.createDataFrame(all_comments, comment_schema)\n",
    "    \n",
    "#     # Ghi vào bảng comment\n",
    "#     df_comment_silver.writeTo(\"nessie.silver_tables.comment\").using(\"iceberg\").createOrReplace()\n",
    "#     print(f\"Đã ghi {df_comment_silver.count()} dòng vào bảng comment\")\n",
    "    \n",
    "#     # Verify\n",
    "#     spark.table(\"nessie.silver_tables.comment\").show(5, truncate=False)\n",
    "    \n",
    "#     # Thống kê theo article\n",
    "#     print(\"\\nThống kê comment theo article:\")\n",
    "#     spark.table(\"nessie.silver_tables.comment\").groupBy(\"articleID\").count().orderBy(\"articleID\").show()\n",
    "# else:\n",
    "#     print(\"Không có dữ liệu comment để ghi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3cac25-11fb-4ba3-9412-7ed9afa1ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load sub vào cột description cho tiktok posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bc8996c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CẬP NHẬT TITLE & DESCRIPTION CHO BẢNG ARTICLE TỪ FILE SUB (INCREMENTAL)\n",
      "================================================================================\n",
      "Số file sub mới: 1\n",
      "Đọc được 1 dòng từ các file sub mới\n",
      "Số dòng dùng để cập nhật: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã cập nhật title & description cho các article có url trùng trong bảng article!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CẬP NHẬT TITLE & DESCRIPTION CHO BẢNG ARTICLE TỪ FILE SUB (INCREMENTAL)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Tạo bảng log file sub nếu chưa có\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nessie.silver_tables.tiktok_sub_files_log (\n",
    "  file_path STRING,\n",
    "  load_time TIMESTAMP\n",
    ") USING iceberg\n",
    "\"\"\")\n",
    "\n",
    "SUB_PATH = \"s3a://bronze/MangXaHoi/tiktok-data/sub/*.csv\"\n",
    "\n",
    "# ----- Lấy danh sách file sub hiện có -----\n",
    "df_all_sub_files = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"false\")\n",
    "         .option(\"encoding\", \"UTF-8\")\n",
    "         .csv(SUB_PATH)\n",
    "         .select(input_file_name().alias(\"file_path\"))\n",
    "         .distinct()\n",
    ")\n",
    "\n",
    "try:\n",
    "    df_sub_processed = spark.table(\"nessie.silver_tables.tiktok_sub_files_log\") \\\n",
    "                            .select(\"file_path\").distinct()\n",
    "except AnalysisException:\n",
    "    df_sub_processed = spark.createDataFrame([], \"file_path STRING\")\n",
    "\n",
    "df_new_sub_files = df_all_sub_files.join(df_sub_processed, \"file_path\", \"left_anti\")\n",
    "new_sub_files = [r.file_path for r in df_new_sub_files.collect()]\n",
    "\n",
    "print(f\"Số file sub mới: {len(new_sub_files)}\")\n",
    "\n",
    "if not new_sub_files:\n",
    "    print(\"Không có file sub mới, bỏ qua bước cập nhật title/description.\")\n",
    "else:\n",
    "    # ----- Đọc data từ các file sub MỚI -----\n",
    "    df_desc_raw = (\n",
    "        spark.read\n",
    "             .option(\"header\", \"true\")\n",
    "             .option(\"inferSchema\", \"false\")\n",
    "             .option(\"encoding\", \"UTF-8\")\n",
    "             .csv(new_sub_files)\n",
    "    )\n",
    "\n",
    "    print(f\"Đọc được {df_desc_raw.count():,} dòng từ các file sub mới\")\n",
    "\n",
    "    # Chuẩn hoá cột url, title, description\n",
    "    # new_title: thường là caption / mô tả ngắn\n",
    "    # new_description: toàn bộ sub\n",
    "    df_desc = (\n",
    "        df_desc_raw\n",
    "            .select(\n",
    "                trim(col(\"url\")).alias(\"url\"),\n",
    "                trim(col(\"description\")).alias(\"new_title\"),\n",
    "                trim(col(\"sub\")).alias(\"new_description\")\n",
    "            )\n",
    "            .filter(col(\"url\").isNotNull() & (col(\"url\") != \"\"))\n",
    "    )\n",
    "\n",
    "    print(f\"Số dòng dùng để cập nhật: {df_desc.count():,}\")\n",
    "\n",
    "    df_desc.createOrReplaceTempView(\"article_desc_update\")\n",
    "\n",
    "    # ----- CHỈ UPDATE các article đã tồn tại, KHÔNG insert mới -----\n",
    "    spark.sql(\"\"\"\n",
    "    MERGE INTO nessie.silver_tables.article AS a\n",
    "    USING article_desc_update AS d\n",
    "    ON a.url = d.url\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "      a.title       = d.new_title,\n",
    "      a.description = d.new_description,\n",
    "      a.updated_at  = current_timestamp()\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Đã cập nhật title & description cho các article có url trùng trong bảng article!\")\n",
    "\n",
    "    # Ghi log các file sub đã xử lý\n",
    "    df_sub_log = spark.createDataFrame(\n",
    "        [(p, datetime.now()) for p in new_sub_files],\n",
    "        \"file_path STRING, load_time TIMESTAMP\"\n",
    "    )\n",
    "    (df_sub_log.writeTo(\"nessie.silver_tables.tiktok_sub_files_log\")\n",
    "              .using(\"iceberg\")\n",
    "              .append())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88959aa4-a9f3-4a7d-904e-9d859de6d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load Facebook data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0e2331c-4b1b-4bdc-be81-b2cc07c9dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from datetime import datetime\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types import StringType\n",
    "\n",
    "# # Map tháng tiếng Việt -> số\n",
    "# MONTH_MAP = {\n",
    "#     \"Tháng 1\": \"01\", \"Tháng 2\": \"02\", \"Tháng 3\": \"03\", \"Tháng 4\": \"04\",\n",
    "#     \"Tháng 5\": \"05\", \"Tháng 6\": \"06\", \"Tháng 7\": \"07\", \"Tháng 8\": \"08\",\n",
    "#     \"Tháng 9\": \"09\", \"Tháng 10\": \"10\", \"Tháng 11\": \"11\", \"Tháng 12\": \"12\"\n",
    "# }\n",
    "\n",
    "# def parse_vietnam_datetime(dt_str):\n",
    "#     \"\"\"\n",
    "#     Convert:\n",
    "#     'Thứ Sáu, 1 Tháng 8, 2025 lúc 20:18'\n",
    "#           ↓\n",
    "#     '2025-08-01 20:18:00'\n",
    "#     \"\"\"\n",
    "#     if not dt_str:\n",
    "#         return None\n",
    "    \n",
    "#     try:\n",
    "#         # Bỏ tiền tố thứ ngày\n",
    "#         # Ví dụ: \"Thứ Sáu, \" → \"\"\n",
    "#         dt_str = dt_str.split(\",\", 1)[1].strip()\n",
    "\n",
    "#         # dt_str còn lại:\n",
    "#         # \"1 Tháng 8, 2025 lúc 20:18\"\n",
    "\n",
    "#         # Tách ngày – tháng tiếng Việt\n",
    "#         # 1 Tháng 8\n",
    "#         match = re.search(r\"(\\d+)\\s+(Tháng\\s+\\d+)\", dt_str)\n",
    "#         if not match:\n",
    "#             return None\n",
    "\n",
    "#         day = match.group(1)\n",
    "#         month_text = match.group(2)\n",
    "#         month = MONTH_MAP.get(month_text)\n",
    "\n",
    "#         # Lấy năm\n",
    "#         year_match = re.search(r\",\\s*(\\d{4})\", dt_str)\n",
    "#         if not year_match:\n",
    "#             return None\n",
    "#         year = year_match.group(1)\n",
    "\n",
    "#         # Lấy giờ phút\n",
    "#         time_match = re.search(r\"lúc\\s+(\\d{1,2}:\\d{2})\", dt_str)\n",
    "#         if time_match:\n",
    "#             time_str = time_match.group(1)\n",
    "#         else:\n",
    "#             time_str = \"00:00\"\n",
    "\n",
    "#         # Format chuẩn: yyyy-MM-dd HH:mm:ss\n",
    "#         final_str = f\"{year}-{month}-{int(day):02d} {time_str}:00\"\n",
    "#         return final_str\n",
    "\n",
    "#     except Exception:\n",
    "#         return None\n",
    "\n",
    "\n",
    "# parse_vn_time_udf = udf(parse_vietnam_datetime, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8ddb7bd-e1ce-470d-8cf0-ff3a927dd117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql(\"\"\"\n",
    "# DELETE FROM nessie.silver_tables.article\n",
    "# WHERE type = 'facebook'\n",
    "# \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10b5c831-3d1a-49a2-8577-38cb7f3f89bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql(\"\"\"\n",
    "# DELETE FROM nessie.silver_tables.comment\n",
    "# WHERE articleID NOT IN (\n",
    "#     SELECT articleID FROM nessie.silver_tables.article\n",
    "# )\n",
    "# \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2876f3da-c616-4b51-81ca-6fc65bc0f841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql(\"\"\"\n",
    "# DELETE FROM nessie.silver_tables.fb_posts_files_log\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9114de25-0782-4902-96c4-4e72bfe800f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, trim, current_timestamp, to_timestamp, row_number, lit,\n",
    "    max as F_max, input_file_name\n",
    ")\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85aae2a5-07a5-4c26-8b24-32f59a791240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOAD DỮ LIỆU FACEBOOK VÀO BẢNG ARTICLE & COMMENT (INCREMENTAL)\n",
      "================================================================================\n",
      "base_article_id (offset) = 3065\n",
      "base_comment_id (offset) = 304070\n",
      "\n",
      "=== XỬ LÝ POSTS FACEBOOK (ARTICLE) ===\n",
      "Tổng dòng posts đọc được: 1,296\n",
      "Số file posts Facebook mới: 0\n",
      "Không có file posts Facebook mới.\n",
      "Không có file posts Facebook mới.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====================================================\n",
    "# 0. HÀM PARSE THỜI GIAN TIẾNG VIỆT (GIỮ NGUYÊN)\n",
    "# ====================================================\n",
    "MONTH_MAP = {\n",
    "    \"Tháng 1\": \"01\", \"Tháng 2\": \"02\", \"Tháng 3\": \"03\", \"Tháng 4\": \"04\",\n",
    "    \"Tháng 5\": \"05\", \"Tháng 6\": \"06\", \"Tháng 7\": \"07\", \"Tháng 8\": \"08\",\n",
    "    \"Tháng 9\": \"09\", \"Tháng 10\": \"10\", \"Tháng 11\": \"11\", \"Tháng 12\": \"12\"\n",
    "}\n",
    "\n",
    "def parse_vietnam_datetime(dt_str):\n",
    "    if not dt_str:\n",
    "        return None\n",
    "    try:\n",
    "        dt_str = dt_str.split(\",\", 1)[1].strip()\n",
    "        match = re.search(r\"(\\d+)\\s+(Tháng\\s+\\d+)\", dt_str)\n",
    "        if not match:\n",
    "            return None\n",
    "        day = match.group(1)\n",
    "        month_text = match.group(2)\n",
    "        month = MONTH_MAP.get(month_text)\n",
    "\n",
    "        year_match = re.search(r\",\\s*(\\d{4})\", dt_str)\n",
    "        if not year_match:\n",
    "            return None\n",
    "        year = year_match.group(1)\n",
    "\n",
    "        time_match = re.search(r\"lúc\\s+(\\d{1,2}:\\d{2})\", dt_str)\n",
    "        if time_match:\n",
    "            time_str = time_match.group(1)\n",
    "        else:\n",
    "            time_str = \"00:00\"\n",
    "\n",
    "        final_str = f\"{year}-{month}-{int(day):02d} {time_str}:00\"\n",
    "        return final_str\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "parse_vn_time_udf = udf(parse_vietnam_datetime, StringType())\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOAD DỮ LIỆU FACEBOOK VÀO BẢNG ARTICLE & COMMENT (INCREMENTAL)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "posts_glob    = \"s3a://bronze/MangXaHoi/Face-data/posts/*.csv\"\n",
    "comments_glob = \"s3a://bronze/MangXaHoi/Face-data/comments/*.csv\"\n",
    "\n",
    "# ====================================================\n",
    "# 1. TẠO LOG FILE POSTS / COMMENTS NẾU CHƯA CÓ\n",
    "# ====================================================\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nessie.silver_tables.fb_posts_files_log (\n",
    "  file_path STRING,\n",
    "  load_time TIMESTAMP\n",
    ") USING iceberg\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nessie.silver_tables.fb_comments_files_log (\n",
    "  file_path STRING,\n",
    "  load_time TIMESTAMP\n",
    ") USING iceberg\n",
    "\"\"\")\n",
    "\n",
    "# ====================================================\n",
    "# 2. LẤY MAX articleID / commentID HIỆN CÓ\n",
    "# ====================================================\n",
    "try:\n",
    "    max_article_row = (\n",
    "        spark.table(\"nessie.silver_tables.article\")\n",
    "             .agg(F_max(\"articleID\").alias(\"maxID\"))\n",
    "             .collect()[0]\n",
    "    )\n",
    "    base_article_id = max_article_row[\"maxID\"] or 0\n",
    "except Exception:\n",
    "    base_article_id = 0\n",
    "\n",
    "try:\n",
    "    max_comment_row = (\n",
    "        spark.table(\"nessie.silver_tables.comment\")\n",
    "             .agg(F_max(\"commentID\").alias(\"maxID\"))\n",
    "             .collect()[0]\n",
    "    )\n",
    "    base_comment_id = max_comment_row[\"maxID\"] or 0\n",
    "except Exception:\n",
    "    base_comment_id = 0\n",
    "\n",
    "print(f\"base_article_id (offset) = {base_article_id}\")\n",
    "print(f\"base_comment_id (offset) = {base_comment_id}\")\n",
    "\n",
    "# ====================================================\n",
    "# 3. ĐỌC CÁC FILE POSTS MỚI -> UPSERT VÀO ARTICLE\n",
    "# ====================================================\n",
    "print(\"\\n=== XỬ LÝ POSTS FACEBOOK (ARTICLE) ===\")\n",
    "\n",
    "# 3.1 Đọc tất cả file posts + gắn file_path\n",
    "df_posts_all = (\n",
    "    spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"false\")\n",
    "        .option(\"encoding\", \"UTF-8\")\n",
    "        .csv(posts_glob)\n",
    "        .withColumn(\"file_path\", input_file_name())\n",
    ")\n",
    "\n",
    "print(f\"Tổng dòng posts đọc được: {df_posts_all.count():,}\")\n",
    "\n",
    "# 3.2 Lọc ra file mới theo log\n",
    "try:\n",
    "    df_posts_processed = (\n",
    "        spark.table(\"nessie.silver_tables.fb_posts_files_log\")\n",
    "             .select(\"file_path\").distinct()\n",
    "    )\n",
    "except AnalysisException:\n",
    "    df_posts_processed = spark.createDataFrame([], \"file_path STRING\")\n",
    "\n",
    "df_posts_new = df_posts_all.join(df_posts_processed, \"file_path\", \"left_anti\")\n",
    "\n",
    "# Lấy list file mới (từ log)\n",
    "new_post_files = [r.file_path for r in df_posts_new.select(\"file_path\").distinct().collect()]\n",
    "print(f\"Số file posts Facebook mới: {len(new_post_files)}\")\n",
    "\n",
    "if not new_post_files:\n",
    "    print(\"Không có file posts Facebook mới.\")\n",
    "else:\n",
    "    # Đọc lại dữ liệu posts chỉ từ các file mới này\n",
    "    df_posts_new2 = (\n",
    "        spark.read\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"false\")\n",
    "            .option(\"encoding\", \"UTF-8\")\n",
    "            .csv(new_post_files)\n",
    "    )\n",
    "\n",
    "    print(f\"Tổng dòng posts đọc được từ file MỚI: {df_posts_new2.count():,}\")\n",
    "\n",
    "if df_posts_new.rdd.isEmpty():\n",
    "    print(\"Không có file posts Facebook mới.\")\n",
    "else:\n",
    "    print(f\"Số dòng posts thuộc file MỚI: {df_posts_new.count():,}\")\n",
    "\n",
    "    # 3.3 Chuẩn hoá dữ liệu posts mới\n",
    "    df_posts_clean = df_posts_new2.select(\n",
    "    trim(col(\"ID\")).alias(\"url_post\"),\n",
    "    trim(col(\"Title\")).alias(\"title\"),\n",
    "    trim(col(\"Description\")).alias(\"description\"),\n",
    "    trim(col(\"Author\")).alias(\"author\"),\n",
    "    trim(col(\"Url\")).alias(\"url_extra\"),\n",
    "    trim(col(\"TimePublish\")).alias(\"TimePublish_raw\"),\n",
    "    trim(col(\"Like\")).alias(\"Like_raw\"),\n",
    "    trim(col(\"Share\")).alias(\"Share_raw\"),\n",
    "    trim(col(\"Comment\")).alias(\"Comment_raw\")\n",
    "    )\n",
    "\n",
    "    df_posts_parsed = (\n",
    "        df_posts_clean\n",
    "            .withColumn(\"TimePublish_clean\", parse_vn_time_udf(col(\"TimePublish_raw\")))\n",
    "            .withColumn(\"timePublish\", to_timestamp(col(\"TimePublish_clean\")))\n",
    "            .withColumn(\"likeCount\",    col(\"Like_raw\").cast(\"int\"))\n",
    "            .withColumn(\"shareCount\",   col(\"Share_raw\").cast(\"int\"))\n",
    "            .withColumn(\"commentCount\", col(\"Comment_raw\").cast(\"int\"))\n",
    "    )\n",
    "\n",
    "    if not df_posts_parsed.rdd.isEmpty():\n",
    "        df_update_view = df_posts_parsed.select(\n",
    "            col(\"url_post\").alias(\"url\"),\n",
    "            col(\"title\"),\n",
    "            col(\"description\"),\n",
    "            col(\"author\"),\n",
    "            col(\"timePublish\"),\n",
    "            col(\"likeCount\"),\n",
    "            col(\"commentCount\"),\n",
    "            col(\"shareCount\")\n",
    "        ).distinct()\n",
    "\n",
    "        df_update_view.createOrReplaceTempView(\"fb_posts_update\")\n",
    "\n",
    "        spark.sql(\"\"\"\n",
    "        MERGE INTO nessie.silver_tables.article AS t\n",
    "        USING fb_posts_update AS s\n",
    "        ON  t.url = s.url and t.type = 'facebook'\n",
    "        WHEN MATCHED THEN UPDATE SET\n",
    "          t.title        = s.title,\n",
    "          t.description  = s.description,\n",
    "          t.author       = s.author,\n",
    "          t.timePublish  = s.timePublish,\n",
    "          t.likeCount    = s.likeCount,\n",
    "          t.commentCount = s.commentCount,\n",
    "          t.shareCount   = s.shareCount,\n",
    "          t.updated_at   = current_timestamp()\n",
    "        \"\"\")\n",
    "        print(f\"Đã UPDATE {df_update_view.count():,} bài post facebook.\") # nó lấy nguyên file để merge nên số hiển thị chưa  đúng\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ---------- INSERT các bài post mới ----------\n",
    "    # ----- Lấy lại danh sách URL facebook hiện có để xác định bản ghi mới -----\n",
    "    try:\n",
    "        df_article_fb_urls = (\n",
    "            spark.table(\"nessie.silver_tables.article\")\n",
    "                 .filter(col(\"type\") == \"facebook\")\n",
    "                 .select(\"url\")\n",
    "                 .distinct()\n",
    "        )\n",
    "    except AnalysisException:\n",
    "        df_article_fb_urls = spark.createDataFrame([], \"url STRING\")\n",
    "    \n",
    "    # anti-join: chỉ giữ các post trong batch mà chưa có trong article\n",
    "    df_posts_to_insert = (\n",
    "        df_posts_parsed.alias(\"p\")\n",
    "            .join(df_article_fb_urls.alias(\"a\"),\n",
    "                  col(\"p.url_post\") == col(\"a.url\"),\n",
    "                  \"left_anti\")\n",
    "    )\n",
    "\n",
    "    if not df_posts_to_insert.rdd.isEmpty():\n",
    "        w_new_article = Window.orderBy(\"url_post\")\n",
    "        df_insert_final = (\n",
    "            df_posts_to_insert\n",
    "                .withColumn(\n",
    "                    \"articleID\",\n",
    "                    (row_number().over(w_new_article) + lit(base_article_id)).cast(\"int\")\n",
    "                )\n",
    "                .select(\n",
    "                    col(\"articleID\"),\n",
    "                    col(\"title\"),\n",
    "                    col(\"description\"),\n",
    "                    col(\"author\"),\n",
    "                    col(\"url_post\").alias(\"url\"),\n",
    "                    col(\"timePublish\"),\n",
    "                    col(\"likeCount\"),\n",
    "                    col(\"commentCount\"),\n",
    "                    col(\"shareCount\"),\n",
    "                    lit(\"facebook\").alias(\"type\"),\n",
    "                    current_timestamp().alias(\"created_at\"),\n",
    "                    current_timestamp().alias(\"updated_at\")\n",
    "                )\n",
    "        )\n",
    "    \n",
    "        df_insert_final.writeTo(\"nessie.silver_tables.article\") \\\n",
    "                       .using(\"iceberg\") \\\n",
    "                       .append()\n",
    "        print(f\"Đã INSERT {df_insert_final.count():,} bài post facebook mới.\")\n",
    "    \n",
    "        base_article_id += df_insert_final.count()\n",
    "    else:\n",
    "        print(\"Không có bài post facebook mới để INSERT.\")\n",
    "\n",
    "\n",
    "    # ---------- Ghi log các file posts đã xử lý ----------\n",
    "    df_posts_new.select(\"file_path\").distinct() \\\n",
    "        .withColumn(\"load_time\", current_timestamp()) \\\n",
    "        .writeTo(\"nessie.silver_tables.fb_posts_files_log\") \\\n",
    "        .using(\"iceberg\") \\\n",
    "        .append()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92136cd3-95a0-461c-8ea2-a380dbc1c03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XỬ LÝ COMMENTS FACEBOOK (COMMENT) ===\n",
      "Tổng dòng comments đọc được: 91,255\n",
      "Số dòng comment thuộc file MỚI: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số comment match được article facebook: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã xoá comment cũ của 1 bài post facebook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã INSERT 29 comment facebook mới.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# 4. ĐỌC CÁC FILE COMMENTS MỚI -> REPLACE COMMENT THEO POST\n",
    "# ====================================================\n",
    "print(\"\\n=== XỬ LÝ COMMENTS FACEBOOK (COMMENT) ===\")\n",
    "df_cmt_all = (\n",
    "    spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"false\")\n",
    "        .option(\"encoding\", \"UTF-8\")\n",
    "        .csv(comments_glob)\n",
    "        .withColumn(\"file_path\", input_file_name())\n",
    ")\n",
    "\n",
    "print(f\"Tổng dòng comments đọc được: {df_cmt_all.count():,}\")\n",
    "\n",
    "try:\n",
    "    df_cmt_processed = (\n",
    "        spark.table(\"nessie.silver_tables.fb_comments_files_log\")\n",
    "             .select(\"file_path\").distinct()\n",
    "    )\n",
    "except AnalysisException:\n",
    "    df_cmt_processed = spark.createDataFrame([], \"file_path STRING\")\n",
    "\n",
    "df_cmt_new = df_cmt_all.join(df_cmt_processed, \"file_path\", \"left_anti\")\n",
    "\n",
    "if df_cmt_new.rdd.isEmpty():\n",
    "    print(\"Không có file comment Facebook mới.\")\n",
    "else:\n",
    "    print(f\"Số dòng comment thuộc file MỚI: {df_cmt_new.count():,}\")\n",
    "\n",
    "    df_cmt_clean = df_cmt_new.select(\n",
    "        trim(col(\"STT\")).alias(\"stt\"),\n",
    "        trim(col(\"Id_post\")).alias(\"post_url\"),\n",
    "        trim(col(\"Comment\")).alias(\"comment_text\")\n",
    "    ).filter(col(\"post_url\").isNotNull() & (col(\"post_url\") != \"\"))\n",
    "\n",
    "    # Join với article facebook (sau khi đã update/insert ở bước 3)\n",
    "    df_article_fb_latest = (\n",
    "        spark.table(\"nessie.silver_tables.article\")\n",
    "             .filter(col(\"type\") == \"facebook\")\n",
    "             .select(\"articleID\", \"url\")\n",
    "    )\n",
    "\n",
    "    df_cmt_joined = (\n",
    "        df_cmt_clean.alias(\"c\")\n",
    "            .join(df_article_fb_latest.alias(\"a\"),\n",
    "                  col(\"c.post_url\") == col(\"a.url\"),\n",
    "                  \"inner\")\n",
    "    )\n",
    "\n",
    "    print(f\"Số comment match được article facebook: {df_cmt_joined.count():,}\")\n",
    "\n",
    "    if not df_cmt_joined.rdd.isEmpty():\n",
    "        # Lấy danh sách articleID có trong batch comment mới\n",
    "        article_ids = [r.articleID for r in\n",
    "                       df_cmt_joined.select(\"a.articleID\").distinct().collect()]\n",
    "\n",
    "        if article_ids:\n",
    "            id_list = \",\".join(str(i) for i in article_ids)\n",
    "            spark.sql(f\"\"\"\n",
    "                DELETE FROM nessie.silver_tables.comment\n",
    "                WHERE articleID IN ({id_list})\n",
    "            \"\"\")\n",
    "            print(f\"Đã xoá comment cũ của {len(article_ids)} bài post facebook.\")\n",
    "\n",
    "        # Gán commentID mới (offset theo base_comment_id)\n",
    "        w_cmt = Window.orderBy(col(\"a.articleID\"), col(\"c.stt\"))\n",
    "        df_comment_silver = (\n",
    "            df_cmt_joined\n",
    "                .withColumn(\n",
    "                    \"commentID\",\n",
    "                    (row_number().over(w_cmt) + lit(base_comment_id)).cast(\"int\")\n",
    "                )\n",
    "                .select(\n",
    "                    col(\"commentID\"),\n",
    "                    col(\"a.articleID\").alias(\"articleID\"),\n",
    "                    lit(None).cast(\"string\").alias(\"name\"),\n",
    "                    lit(None).cast(\"string\").alias(\"tagName\"),\n",
    "                    lit(None).cast(\"string\").alias(\"urlUser\"),\n",
    "                    col(\"comment_text\").alias(\"comment\"),\n",
    "                    lit(None).cast(\"timestamp\").alias(\"commentTime\"),\n",
    "                    lit(None).cast(\"int\").alias(\"commentLike\"),\n",
    "                    lit(1).cast(\"int\").alias(\"levelComment\"),\n",
    "                    lit(None).cast(\"string\").alias(\"replyTo\"),\n",
    "                    lit(0).cast(\"int\").alias(\"numberOfReply\"),\n",
    "                    current_timestamp().alias(\"created_at\"),\n",
    "                    current_timestamp().alias(\"updated_at\")\n",
    "                )\n",
    "        )\n",
    "        \n",
    "        df_comment_silver = df_comment_silver.coalesce(8)\n",
    "        df_comment_silver.writeTo(\"nessie.silver_tables.comment\") \\\n",
    "                         .using(\"iceberg\") \\\n",
    "                         .append()\n",
    "        print(f\"Đã INSERT {df_comment_silver.count():,} comment facebook mới.\")\n",
    "\n",
    "        base_comment_id += df_comment_silver.count()\n",
    "\n",
    "    # Ghi log các file comments đã xử lý\n",
    "    df_cmt_new.select(\"file_path\").distinct() \\\n",
    "        .withColumn(\"load_time\", current_timestamp()) \\\n",
    "        .writeTo(\"nessie.silver_tables.fb_comments_files_log\") \\\n",
    "        .using(\"iceberg\") \\\n",
    "        .append()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a912ba56-87cf-49f7-8531-24ce95b487dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 5. CHECK NHANH\n",
    "# ====================================================\n",
    "print(\"\\nCHECK lại 5 bài post facebook gần nhất:\")\n",
    "spark.table(\"nessie.silver_tables.article\") \\\n",
    "     .where(\"type = 'facebook'\") \\\n",
    "     .orderBy(col(\"articleID\").desc()) \\\n",
    "     .show(5, truncate=False)\n",
    "\n",
    "print(\"\\nCHECK lại 5 comment facebook:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT c.commentID, c.articleID, a.title, c.comment\n",
    "FROM nessie.silver_tables.comment c\n",
    "JOIN nessie.silver_tables.article a\n",
    "  ON c.articleID = a.articleID\n",
    "WHERE a.type = 'facebook'\n",
    "ORDER BY c.commentID DESC\n",
    "LIMIT 5\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4c11d8e-7930-4437-bcc7-d868553d19dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Spark Session đã được dừng!\n"
     ]
    }
   ],
   "source": [
    "# Dừng Spark Session để giải phóng resources\n",
    "spark.stop()\n",
    "print(\" Spark Session đã được dừng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82517e0-e8c9-40b2-9e2f-50ecf8b01bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
