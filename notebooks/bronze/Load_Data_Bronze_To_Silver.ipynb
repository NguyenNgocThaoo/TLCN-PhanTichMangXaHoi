{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c24213a7",
   "metadata": {},
   "source": [
    "# Load Data từ Bronze Layer sang Silver Layer\n",
    "\n",
    "Notebook này sẽ đọc dữ liệu từ Bronze layer (MinIO) và xử lý để load vào các bảng Iceberg trong Silver layer với Nessie catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063eaad0",
   "metadata": {},
   "source": [
    "## 1. Import Libraries và Khởi tạo Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c718b51-ae4c-4fcf-b0b4-fcf8b57dfad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Spark Session initialized | Master: spark://spark-master:7077 | App ID: app-20251206150307-0002\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import csv, io, os, re\n",
    "from datetime import datetime\n",
    "from typing import Dict\n",
    "\n",
    "# Cấu hình AWS/MinIO credentials\n",
    "os.environ.update({\n",
    "    'AWS_REGION': 'us-east-1',\n",
    "    'AWS_ACCESS_KEY_ID': 'admin',\n",
    "    'AWS_SECRET_ACCESS_KEY': 'admin123'\n",
    "})\n",
    "\n",
    "# Khởi tạo Spark Session với Nessie Catalog\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Load_Bronze_To_Silver\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.memory\", \"1536m\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    # Nessie Catalog\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v2\")\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://silver/\")\n",
    "    .config(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    # S3/MinIO Config\n",
    "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.access-key-id\", \"admin\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.secret-access-key\", \"admin123\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.region\", \"us-east-1\")\n",
    "    # Hadoop S3A Config\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.region\", \"us-east-1\")\n",
    "    # Executor Environment\n",
    "    .config(\"spark.executorEnv.AWS_REGION\", \"us-east-1\")\n",
    "    .config(\"spark.executorEnv.AWS_ACCESS_KEY_ID\", \"admin\")\n",
    "    .config(\"spark.executorEnv.AWS_SECRET_ACCESS_KEY\", \"admin123\")\n",
    "    # Local JAR files\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/hadoop-aws-3.3.4.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS nessie.silver_tables\")\n",
    "spark.sql(\"USE nessie.silver_tables\")\n",
    "print(f\" Spark Session initialized | Master: {spark.sparkContext.master} | App ID: {spark.sparkContext.applicationId}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43664703",
   "metadata": {},
   "source": [
    "## 2. Load Bảng SCHOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88066c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOAD BẢNG SCHOOL\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã ghi 265 dòng vào school\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------------+---------+--------------------------+--------------------------+\n",
      "|schoolId|schoolName                            |province |created_at                |updated_at                |\n",
      "+--------+--------------------------------------+---------+--------------------------+--------------------------+\n",
      "|DHF     |Đại học Ngoại Ngữ - Đại học Huế       |Huế      |2025-12-06 14:55:32.023036|2025-12-06 14:55:32.023036|\n",
      "|DCQ     |Đại học Công Nghệ và Quản Lý Hữu Nghị |Hà Nội   |2025-12-06 14:55:32.023036|2025-12-06 14:55:32.023036|\n",
      "|NTT     |Đại học Nguyễn Tất Thành              |TP HCM   |2025-12-06 14:55:32.023036|2025-12-06 14:55:32.023036|\n",
      "|KGH     |Trường Sĩ Quan Không Quân - Hệ Đại học|Khánh Hòa|2025-12-06 14:55:32.023036|2025-12-06 14:55:32.023036|\n",
      "|DHL     |Đại học Nông Lâm - Đại học Huế        |Huế      |2025-12-06 14:55:32.023036|2025-12-06 14:55:32.023036|\n",
      "+--------+--------------------------------------+---------+--------------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG SCHOOL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Đọc và merge tất cả các năm\n",
    "years = [2021, 2022, 2023, 2024, 2025]\n",
    "base_path = \"s3a://bronze/structured_data/danh sách các trường Đại Học (2021-2025)/Danh_sách_các_trường_Đại_Học_\"\n",
    "df_school = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv([f\"{base_path}{year}.csv\" for year in years]).select(\"TenTruong\", \"MaTruong\", \"TinhThanh\").dropDuplicates()\n",
    "\n",
    "# Transform\n",
    "df_school_silver = df_school.select(\n",
    "    col(\"MaTruong\").cast(\"string\").alias(\"schoolId\"),\n",
    "    col(\"TenTruong\").cast(\"string\").alias(\"schoolName\"),\n",
    "    col(\"TinhThanh\").cast(\"string\").alias(\"province\"),\n",
    "    current_timestamp().alias(\"created_at\"),\n",
    "    current_timestamp().alias(\"updated_at\")\n",
    ").filter(col(\"schoolId\").isNotNull() & col(\"schoolName\").isNotNull())\n",
    "\n",
    "# Ghi vào Silver\n",
    "df_school_silver.writeTo(\"nessie.silver_tables.school\").using(\"iceberg\").createOrReplace()\n",
    "print(f\"Đã ghi {df_school_silver.count()} dòng vào school\")\n",
    "\n",
    "# Verify\n",
    "spark.table(\"nessie.silver_tables.school\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0235c2c",
   "metadata": {},
   "source": [
    "## 3. Load Bảng MAJOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf9d446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã ghi 3085 dòng vào major\n",
      "+-------+------------------------------------------------------------+--------------------------+--------------------------+\n",
      "|majorId|majorName                                                   |created_at                |updated_at                |\n",
      "+-------+------------------------------------------------------------+--------------------------+--------------------------+\n",
      "|106    |Khoa học Máy tính                                           |2025-12-06 14:55:40.803072|2025-12-06 14:55:40.803072|\n",
      "|107    |Kỹ thuật Máy tính                                           |2025-12-06 14:55:40.803072|2025-12-06 14:55:40.803072|\n",
      "|108    |Điện - Điện tử - Viễn Thông - Tự động hoá - Thiết kế vi mạch|2025-12-06 14:55:40.803072|2025-12-06 14:55:40.803072|\n",
      "|109    |Kỹ Thuật Cơ khí                                             |2025-12-06 14:55:40.803072|2025-12-06 14:55:40.803072|\n",
      "|110    |Kỹ Thuật Cơ Điện tử                                         |2025-12-06 14:55:40.803072|2025-12-06 14:55:40.803072|\n",
      "+-------+------------------------------------------------------------+--------------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lower, trim, regexp_replace, current_timestamp\n",
    "\n",
    "df_major = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .csv(\"s3a://bronze/structured_data/danh sách các ngành đại học/Danh_sách_các_ngành.csv\")\n",
    "\n",
    "df_major_clean = df_major.select(\n",
    "    regexp_replace(trim(col(df_major.columns[0])).cast(\"string\"), r\"\\.0$\", \"\").alias(\"majorId\"),\n",
    "    trim(col(df_major.columns[1])).cast(\"string\").alias(\"majorName\")\n",
    ").filter(\n",
    "    (col(\"majorId\").isNotNull()) &\n",
    "    (col(\"majorName\").isNotNull()) &\n",
    "    (col(\"majorId\") != \"\") &\n",
    "    (col(\"majorName\") != \"\") &\n",
    "    (lower(col(\"majorId\")) != \"nan\")\n",
    ")\n",
    "\n",
    "# Chuẩn hoá để dedupe theo lowercase\n",
    "df_major_silver = df_major_clean \\\n",
    "    .withColumn(\"majorId_lower\", lower(col(\"majorId\"))) \\\n",
    "    .dropDuplicates([\"majorId_lower\"]) \\\n",
    "    .select(\n",
    "        col(\"majorId\"),\n",
    "        col(\"majorName\"),\n",
    "        current_timestamp().alias(\"created_at\"),\n",
    "        current_timestamp().alias(\"updated_at\")\n",
    "    )\n",
    "\n",
    "df_major_silver.writeTo(\"nessie.silver_tables.major\").using(\"iceberg\").createOrReplace()\n",
    "\n",
    "print(f\"Đã ghi {df_major_silver.count()} dòng vào major\")\n",
    "spark.table(\"nessie.silver_tables.major\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d64a8",
   "metadata": {},
   "source": [
    "## 4. Load Bảng SUBJECT_GROUP và SUBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c053cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOAD BẢNG SUBJECT_GROUP và SUBJECT\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã ghi 232 dòng vào subject_group\n",
      "Đã ghi 51 dòng vào subject\n",
      "+--------------+----------------+------------------+--------------------------+--------------------------+\n",
      "|subjectGroupId|subjectGroupName|subjectCombination|created_at                |updated_at                |\n",
      "+--------------+----------------+------------------+--------------------------+--------------------------+\n",
      "|1             |A00             |Toán-Lí-Hóa       |2025-12-06 14:55:44.071023|2025-12-06 14:55:44.071023|\n",
      "|2             |A01             |Toán-Lí-Ngoại ngữ |2025-12-06 14:55:44.071023|2025-12-06 14:55:44.071023|\n",
      "|3             |A02             |Toán-Lí-Sinh      |2025-12-06 14:55:44.071023|2025-12-06 14:55:44.071023|\n",
      "|4             |A03             |Toán-Lí-Sử        |2025-12-06 14:55:44.071023|2025-12-06 14:55:44.071023|\n",
      "|5             |A04             |Toán-Lí-Địa       |2025-12-06 14:55:44.071023|2025-12-06 14:55:44.071023|\n",
      "+--------------+----------------+------------------+--------------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------------------+-------------------------+-------------------------+\n",
      "|subjectId|subjectName          |created_at               |updated_at               |\n",
      "+---------+---------------------+-------------------------+-------------------------+\n",
      "|1        |biểu diễn nghệ thuật |2025-12-06 14:55:47.46713|2025-12-06 14:55:47.46713|\n",
      "|2        |Công nghệ công nghiệp|2025-12-06 14:55:47.46713|2025-12-06 14:55:47.46713|\n",
      "|3        |Công nghệ nông nghiệp|2025-12-06 14:55:47.46713|2025-12-06 14:55:47.46713|\n",
      "|4        |GDCD                 |2025-12-06 14:55:47.46713|2025-12-06 14:55:47.46713|\n",
      "|5        |Hát                  |2025-12-06 14:55:47.46713|2025-12-06 14:55:47.46713|\n",
      "+---------+---------------------+-------------------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG SUBJECT_GROUP và SUBJECT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Đọc file tohop_mon_fixed.csv\n",
    "df_tohop = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"encoding\", \"UTF-8\").csv(\"s3a://bronze/structured_data/tohop_mon_fixed.csv\")\n",
    "\n",
    "# --- SUBJECT_GROUP ---\n",
    "df_subject_group_silver = df_tohop.select(\n",
    "    col(df_tohop.columns[0]).cast(\"int\").alias(\"subjectGroupId\"),\n",
    "    col(df_tohop.columns[1]).cast(\"string\").alias(\"subjectGroupName\"),\n",
    "    col(df_tohop.columns[2]).cast(\"string\").alias(\"subjectCombination\"),\n",
    "    current_timestamp().alias(\"created_at\"),\n",
    "    current_timestamp().alias(\"updated_at\")\n",
    ").filter(col(\"subjectGroupId\").isNotNull() & col(\"subjectGroupName\").isNotNull() & col(\"subjectCombination\").isNotNull()).dropDuplicates([\"subjectGroupName\", \"subjectCombination\"])\n",
    "df_subject_group_silver.writeTo(\"nessie.silver_tables.subject_group\").using(\"iceberg\").createOrReplace()\n",
    "print(f\"Đã ghi {df_subject_group_silver.count()} dòng vào subject_group\")\n",
    "\n",
    "# --- SUBJECT ---\n",
    "df_subject = (\n",
    "    df_tohop.select(explode(split(col(df_tohop.columns[2]), \"-\")).alias(\"subjectName\"))\n",
    "            .withColumn(\"subjectName\", trim(col(\"subjectName\")))\n",
    "            .filter(col(\"subjectName\").isNotNull() & (col(\"subjectName\") != \"\"))\n",
    "            .withColumn(\"subjectName_lower\", lower(col(\"subjectName\")))\n",
    "            # loại bỏ trùng theo chữ thường\n",
    "            .dropDuplicates([\"subjectName_lower\"])\n",
    ")\n",
    "\n",
    "window_spec = Window.orderBy(\"subjectName_lower\")\n",
    "df_subject_silver = df_subject.withColumn(\"subjectId\", row_number().over(window_spec)).select(\n",
    "    col(\"subjectId\").cast(\"int\"),\n",
    "    col(\"subjectName\").cast(\"string\"),\n",
    "    current_timestamp().alias(\"created_at\"),\n",
    "    current_timestamp().alias(\"updated_at\")\n",
    ")\n",
    "df_subject_silver.writeTo(\"nessie.silver_tables.subject\").using(\"iceberg\").createOrReplace()\n",
    "print(f\"Đã ghi {df_subject_silver.count()} dòng vào subject\")\n",
    "\n",
    "# Verify\n",
    "spark.table(\"nessie.silver_tables.subject_group\").orderBy(\"subjectGroupId\").show(5, truncate=False)\n",
    "spark.table(\"nessie.silver_tables.subject\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e6236f",
   "metadata": {},
   "source": [
    "## 5. Load Bảng SELECTION_METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d204061d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOAD BẢNG SELECTION_METHOD\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã ghi 10 dòng vào selection_method\n",
      "+-----------------+------------------------------------------------------+--------------------------+--------------------------+\n",
      "|selectionMethodId|selectionMethodName                                   |created_at                |updated_at                |\n",
      "+-----------------+------------------------------------------------------+--------------------------+--------------------------+\n",
      "|1                |Điểm chuẩn theo phương thức Điểm học bạ               |2025-12-06 14:55:51.823519|2025-12-06 14:55:51.823519|\n",
      "|2                |Điểm chuẩn theo phương thức Điểm thi THPT             |2025-12-06 14:55:51.823519|2025-12-06 14:55:51.823519|\n",
      "|3                |Điểm chuẩn theo phương thức Điểm xét tuyển kết hợp    |2025-12-06 14:55:51.823519|2025-12-06 14:55:51.823519|\n",
      "|4                |Điểm chuẩn theo phương thức Điểm xét tốt nghiệp THPT  |2025-12-06 14:55:51.823519|2025-12-06 14:55:51.823519|\n",
      "|5                |Điểm chuẩn theo phương thức Điểm ĐGNL HCM             |2025-12-06 14:55:51.823519|2025-12-06 14:55:51.823519|\n",
      "|6                |Điểm chuẩn theo phương thức Điểm ĐGNL HN              |2025-12-06 14:55:51.823519|2025-12-06 14:55:51.823519|\n",
      "|7                |Điểm chuẩn theo phương thức Điểm ĐGNL ĐH Sư phạm HN   |2025-12-06 14:55:51.823519|2025-12-06 14:55:51.823519|\n",
      "|8                |Điểm chuẩn theo phương thức Điểm ĐGNL ĐH Sư phạm TPHCM|2025-12-06 14:55:51.823519|2025-12-06 14:55:51.823519|\n",
      "|9                |Điểm chuẩn theo phương thức Điểm Đánh giá Tư duy      |2025-12-06 14:55:51.823519|2025-12-06 14:55:51.823519|\n",
      "|10               |Điểm chuẩn theo phương thức ƯTXT, XT thẳng            |2025-12-06 14:55:51.823519|2025-12-06 14:55:51.823519|\n",
      "+-----------------+------------------------------------------------------+--------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG SELECTION_METHOD\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Đọc từ file benchmark để lấy các phương thức xét tuyển\n",
    "df_benchmark = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"encoding\", \"UTF-8\").csv(\"s3a://bronze/structured_data/điểm chuẩn các trường (2021-2025)/Điểm_chuẩn_các_ngành_đại_học_năm(2021-2025)*.csv\")\n",
    "\n",
    "# Lấy PhuongThuc và loại bỏ \"năm ...\"\n",
    "df_selection = df_benchmark.select(trim(regexp_replace(col(\"PhuongThuc\"), r\"\\s*năm\\s+\\d{4}.*$\", \"\")).alias(\"selectionMethodName\")).filter(col(\"selectionMethodName\").isNotNull() & (col(\"selectionMethodName\") != \"\")).distinct()\n",
    "\n",
    "window_spec = Window.orderBy(\"selectionMethodName\")\n",
    "df_selection_method_silver = df_selection.withColumn(\"selectionMethodId\", row_number().over(window_spec)).select(\n",
    "    col(\"selectionMethodId\").cast(\"int\"),\n",
    "    col(\"selectionMethodName\").cast(\"string\"),\n",
    "    current_timestamp().alias(\"created_at\"),\n",
    "    current_timestamp().alias(\"updated_at\")\n",
    ")\n",
    "df_selection_method_silver.writeTo(\"nessie.silver_tables.selection_method\").using(\"iceberg\").createOrReplace()\n",
    "print(f\"Đã ghi {df_selection_method_silver.count()} dòng vào selection_method\")\n",
    "\n",
    "# Verify\n",
    "spark.table(\"nessie.silver_tables.selection_method\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911622a",
   "metadata": {},
   "source": [
    "## 6. Load Bảng GradingScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9ca8916-830b-4f32-bbbb-d0ac527e6568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOAD BẢNG GRADING_SCALE TỪ PHANLOAITHANGDIEM\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã ghi 10 dòng vào grading_scale\n",
      "+--------------+------+---------------+--------------------------+--------------------------+\n",
      "|gradingScaleId|value |description    |created_at                |updated_at                |\n",
      "+--------------+------+---------------+--------------------------+--------------------------+\n",
      "|0             |30.0  |Thang điểm 30  |2025-12-06 14:55:57.731411|2025-12-06 14:55:57.731411|\n",
      "|1             |50.0  |Thang điểm 50  |2025-12-06 14:55:57.731411|2025-12-06 14:55:57.731411|\n",
      "|2             |1200.0|Thang điểm 1200|2025-12-06 14:55:57.731411|2025-12-06 14:55:57.731411|\n",
      "|3             |150.0 |Thang điểm 150 |2025-12-06 14:55:57.731411|2025-12-06 14:55:57.731411|\n",
      "|4             |40.0  |Thang điểm 40  |2025-12-06 14:55:57.731411|2025-12-06 14:55:57.731411|\n",
      "|5             |10.0  |Thang điểm 10  |2025-12-06 14:55:57.731411|2025-12-06 14:55:57.731411|\n",
      "|6             |100.0 |Thang điểm 100 |2025-12-06 14:55:57.731411|2025-12-06 14:55:57.731411|\n",
      "|7             |35.0  |Thang điểm 35  |2025-12-06 14:55:57.731411|2025-12-06 14:55:57.731411|\n",
      "|8             |90.0  |Thang điểm 90  |2025-12-06 14:55:57.731411|2025-12-06 14:55:57.731411|\n",
      "|9             |120.0 |Thang điểm 120 |2025-12-06 14:55:57.731411|2025-12-06 14:55:57.731411|\n",
      "+--------------+------+---------------+--------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG GRADING_SCALE TỪ PHANLOAITHANGDIEM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Đọc dữ liệu gốc từ file CSV (giống benchmark)\n",
    "df_raw = (\n",
    "    spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .option(\"encoding\", \"UTF-8\")\n",
    "        .csv(\"s3a://bronze/structured_data/điểm chuẩn các trường (2021-2025)/Điểm_chuẩn_các_ngành_đại_học_năm(2021-2025)*.csv\")\n",
    ")\n",
    "\n",
    "# 2. Lấy unique PhanLoaiThangDiem\n",
    "df_grading_raw = (\n",
    "    df_raw\n",
    "        .select(trim(col(\"PhanLoaiThangDiem\")).alias(\"description\"))\n",
    "        .filter(col(\"description\").isNotNull() & (col(\"description\") != \"\"))\n",
    "        .dropDuplicates([\"description\"])\n",
    ")\n",
    "\n",
    "# 3. Tách giá trị số trong description làm \"value\" (nếu có, vd: \"thang 40\" -> 40)\n",
    "df_grading = (\n",
    "    df_grading_raw\n",
    "        .withColumn(\n",
    "            \"value\",\n",
    "            regexp_extract(col(\"description\"), r\"(\\d+(?:\\.\\d+)?)\", 1).cast(\"float\")\n",
    "        )\n",
    "        .withColumn(\"gradingScaleId\", monotonically_increasing_id().cast(\"int\"))\n",
    "        .withColumn(\"created_at\", current_timestamp())\n",
    "        .withColumn(\"updated_at\", current_timestamp())\n",
    "        .select(\n",
    "            \"gradingScaleId\",\n",
    "            \"value\",\n",
    "            \"description\",\n",
    "            \"created_at\",\n",
    "            \"updated_at\"\n",
    "        )\n",
    ")\n",
    "\n",
    "# 4. Ghi vào bảng Iceberg grading_scale đã tạo trước đó\n",
    "df_grading.writeTo(\"nessie.silver_tables.grading_scale\") \\\n",
    "          .using(\"iceberg\") \\\n",
    "          .createOrReplace()\n",
    "\n",
    "print(f\"Đã ghi {df_grading.count()} dòng vào grading_scale\")\n",
    "\n",
    "# 5. Verify\n",
    "spark.table(\"nessie.silver_tables.grading_scale\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036e97a-c555-43a0-a6d4-fb1e673edf0f",
   "metadata": {},
   "source": [
    "## 6. Load Bảng BENCHMARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24de0240-da7e-4d00-8870-78bfa419516a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOAD BẢNG BENCHMARK\n",
      "================================================================================\n",
      "Bảng nessie.silver_tables.benchmark đã tồn tại → dùng MERGE (upsert).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã MERGE dữ liệu mới vào bảng benchmark\n",
      "+--------------------+--------+-------+--------------+-----------------+--------------+----+-----+--------------------------+--------------------------+\n",
      "|benchmarkId         |schoolId|majorId|subjectGroupId|selectionMethodId|gradingScaleId|year|score|created_at                |updated_at                |\n",
      "+--------------------+--------+-------+--------------+-----------------+--------------+----+-----+--------------------------+--------------------------+\n",
      "|4789796714871569147 |DBG     |7620105|70            |2                |0             |2025|15.0 |2025-12-06 14:56:01.091071|2025-12-06 14:56:01.091071|\n",
      "|6236320012694458209 |DBG     |7620112|187           |1                |0             |2025|18.0 |2025-12-06 14:56:01.091071|2025-12-06 14:56:01.091071|\n",
      "|2144508859211824436 |DCD     |7510401|35            |2                |4             |2025|17.33|2025-12-06 14:56:01.091071|2025-12-06 14:56:01.091071|\n",
      "|380800951782701353  |BKA     |EV1    |22            |2                |0             |2025|22.22|2025-12-06 14:56:01.091071|2025-12-06 14:56:01.091071|\n",
      "|-7823215348815977550|VUI     |7340101|21            |2                |0             |2025|0.0  |2025-12-06 14:56:01.091071|2025-12-06 14:56:01.091071|\n",
      "+--------------------+--------+-------+--------------+-----------------+--------------+----+-----+--------------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+-----+\n",
      "|year|count|\n",
      "+----+-----+\n",
      "|2021|23215|\n",
      "|2022|27251|\n",
      "|2023|31775|\n",
      "|2024|40734|\n",
      "|2025|40424|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, trim, regexp_replace, current_timestamp,\n",
    "    avg, round, expr\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG BENCHMARK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =========================\n",
    "# 1. ĐỌC & CHUẨN HÓA DỮ LIỆU BRONZE\n",
    "# =========================\n",
    "\n",
    "df_benchmark = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"encoding\", \"UTF-8\")\n",
    "    .csv(\"s3a://bronze/structured_data/điểm chuẩn các trường (2021-2025)/Điểm_chuẩn_các_ngành_đại_học_năm(2021-2025)*.csv\")\n",
    ")\n",
    "\n",
    "# Chuẩn hóa cột PhuongThuc: bỏ phần \"năm XXXX ...\"\n",
    "df_benchmark = df_benchmark.withColumn(\n",
    "    \"PhuongThuc_cleaned\",\n",
    "    trim(regexp_replace(col(\"PhuongThuc\"), r\"\\s*năm\\s+\\d{4}.*$\", \"\"))\n",
    ")\n",
    "\n",
    "# Lookup tables từ Silver\n",
    "df_selection_lookup     = spark.table(\"nessie.silver_tables.selection_method\")\n",
    "df_subject_group_lookup = spark.table(\"nessie.silver_tables.subject_group\")\n",
    "df_grading_scale_lookup = spark.table(\"nessie.silver_tables.grading_scale\")\n",
    "\n",
    "# Join lookup + chuẩn hóa\n",
    "df_benchmark_base = (\n",
    "    df_benchmark\n",
    "    .join(\n",
    "        df_selection_lookup,\n",
    "        df_benchmark[\"PhuongThuc_cleaned\"] == df_selection_lookup[\"selectionMethodName\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    .join(\n",
    "        df_subject_group_lookup,\n",
    "        df_benchmark[\"KhoiThi\"] == df_subject_group_lookup[\"subjectGroupName\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    .join(\n",
    "        df_grading_scale_lookup,\n",
    "        trim(df_benchmark[\"PhanLoaiThangDiem\"]) == df_grading_scale_lookup[\"description\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"MaTruong\").cast(\"string\").alias(\"schoolId\"),\n",
    "        col(\"MaNganh\").cast(\"string\").alias(\"majorId\"),\n",
    "        col(\"subjectGroupId\").cast(\"int\"),\n",
    "        col(\"selectionMethodId\").cast(\"int\"),\n",
    "        col(\"gradingScaleId\").cast(\"int\"),\n",
    "        col(\"Nam\").cast(\"int\").alias(\"year\"),\n",
    "        col(\"DiemChuan\").cast(\"double\").alias(\"score\"),\n",
    "    )\n",
    "    .filter(\n",
    "        col(\"schoolId\").isNotNull() &\n",
    "        col(\"majorId\").isNotNull() &\n",
    "        col(\"gradingScaleId\").isNotNull() &\n",
    "        col(\"year\").isNotNull() &\n",
    "        col(\"score\").isNotNull() &\n",
    "        col(\"selectionMethodId\").isNotNull()\n",
    "        # col(\"subjectGroupId\").isNotNull()  # nếu muốn bắt buộc khối thi thì mở dòng này\n",
    "    )\n",
    "    .dropDuplicates([\n",
    "        \"schoolId\",\n",
    "        \"majorId\",\n",
    "        \"subjectGroupId\",\n",
    "        \"selectionMethodId\",\n",
    "        \"year\",\n",
    "        \"gradingScaleId\",\n",
    "        \"score\"\n",
    "    ])\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 2. GROUP BY & LẤY ĐIỂM TRUNG BÌNH\n",
    "# =========================\n",
    "\n",
    "df_benchmark_grouped = (\n",
    "    df_benchmark_base\n",
    "    .groupBy(\n",
    "        \"schoolId\",\n",
    "        \"majorId\",\n",
    "        \"subjectGroupId\",\n",
    "        \"selectionMethodId\",\n",
    "        \"gradingScaleId\",\n",
    "        \"year\"\n",
    "    )\n",
    "    .agg(\n",
    "        round(avg(\"score\"), 2).alias(\"score\")\n",
    "    )\n",
    ")\n",
    "\n",
    "table_name = \"nessie.silver_tables.benchmark\"\n",
    "\n",
    "# =========================\n",
    "# 3. CHECK BẢNG SILVER ĐÃ TỒN TẠI CHƯA\n",
    "# =========================\n",
    "\n",
    "try:\n",
    "    spark.table(table_name)\n",
    "    table_exists = True\n",
    "    print(f\"Bảng {table_name} đã tồn tại → dùng MERGE (upsert).\")\n",
    "except Exception:\n",
    "    table_exists = False\n",
    "    print(f\"Bảng {table_name} chưa tồn tại → tạo mới full-load.\")\n",
    "\n",
    "# =========================\n",
    "# 4. LẦN ĐẦU: TẠO BẢNG FULL (DÙNG xxhash64 LÀM benchmarkId)\n",
    "# =========================\n",
    "\n",
    "if not table_exists:\n",
    "    df_benchmark_silver = (\n",
    "        df_benchmark_grouped\n",
    "        .withColumn(\n",
    "            \"benchmarkId\",\n",
    "            expr(\n",
    "                \"\"\"\n",
    "                CAST(\n",
    "                    xxhash64(\n",
    "                        schoolId,\n",
    "                        majorId,\n",
    "                        COALESCE(subjectGroupId, -1),\n",
    "                        selectionMethodId,\n",
    "                        gradingScaleId,\n",
    "                        year\n",
    "                    ) AS BIGINT\n",
    "                )\n",
    "                \"\"\"\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"created_at\", current_timestamp())\n",
    "        .withColumn(\"updated_at\", current_timestamp())\n",
    "        .select(\n",
    "            \"benchmarkId\",\n",
    "            \"schoolId\",\n",
    "            \"majorId\",\n",
    "            \"subjectGroupId\",\n",
    "            \"selectionMethodId\",\n",
    "            \"gradingScaleId\",\n",
    "            \"year\",\n",
    "            \"score\",\n",
    "            \"created_at\",\n",
    "            \"updated_at\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_benchmark_silver.writeTo(table_name).using(\"iceberg\").createOrReplace()\n",
    "    print(f\"Đã tạo mới benchmark với {df_benchmark_silver.count()} dòng\")\n",
    "\n",
    "# =========================\n",
    "# 5. CÁC LẦN SAU: MERGE / UPSERT\n",
    "# =========================\n",
    "\n",
    "else:\n",
    "    # Staging từ bronze sau khi chuẩn hóa + group\n",
    "    df_staging = (\n",
    "        df_benchmark_grouped\n",
    "        .withColumn(\"created_at\", current_timestamp())\n",
    "        .withColumn(\"updated_at\", current_timestamp())\n",
    "    )\n",
    "\n",
    "    df_staging.createOrReplaceTempView(\"benchmark_staging\")\n",
    "\n",
    "    # MERGE:\n",
    "    # - MATCHED: update score + updated_at\n",
    "    # - NOT MATCHED: insert bản ghi mới với benchmarkId = hash(business key)\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {table_name} AS t\n",
    "        USING benchmark_staging AS s\n",
    "        ON  t.schoolId          = s.schoolId\n",
    "        AND t.majorId           = s.majorId\n",
    "        AND COALESCE(t.subjectGroupId,  -1) = COALESCE(s.subjectGroupId,  -1)\n",
    "        AND t.selectionMethodId = s.selectionMethodId\n",
    "        AND t.gradingScaleId    = s.gradingScaleId\n",
    "        AND t.year              = s.year\n",
    "\n",
    "        WHEN MATCHED THEN UPDATE SET\n",
    "            t.score      = s.score,\n",
    "            t.updated_at = current_timestamp()\n",
    "\n",
    "        WHEN NOT MATCHED THEN INSERT (\n",
    "            benchmarkId,\n",
    "            schoolId,\n",
    "            majorId,\n",
    "            subjectGroupId,\n",
    "            selectionMethodId,\n",
    "            gradingScaleId,\n",
    "            year,\n",
    "            score,\n",
    "            created_at,\n",
    "            updated_at\n",
    "        ) VALUES (\n",
    "            CAST(\n",
    "                xxhash64(\n",
    "                    s.schoolId,\n",
    "                    s.majorId,\n",
    "                    COALESCE(s.subjectGroupId, -1),\n",
    "                    s.selectionMethodId,\n",
    "                    s.gradingScaleId,\n",
    "                    s.year\n",
    "                ) AS BIGINT\n",
    "            ),\n",
    "            s.schoolId,\n",
    "            s.majorId,\n",
    "            s.subjectGroupId,\n",
    "            s.selectionMethodId,\n",
    "            s.gradingScaleId,\n",
    "            s.year,\n",
    "            s.score,\n",
    "            s.created_at,\n",
    "            s.updated_at\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Đã MERGE dữ liệu mới vào bảng benchmark\")\n",
    "\n",
    "# =========================\n",
    "# 6. VERIFY\n",
    "# =========================\n",
    "\n",
    "spark.table(table_name).show(5, truncate=False)\n",
    "spark.table(table_name).groupBy(\"year\").count().orderBy(\"year\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18094e01",
   "metadata": {},
   "source": [
    "## 7. Load Bảng REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dbdfc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOAD BẢNG REGION\n",
      "================================================================================\n",
      "Đã ghi 64 dòng vào region\n",
      "+--------+-----------------------+--------------------------+--------------------------+\n",
      "|regionId|regionName             |created_at                |updated_at                |\n",
      "+--------+-----------------------+--------------------------+--------------------------+\n",
      "|01      |Sở GDĐT Hà Nội         |2025-12-06 14:56:07.622558|2025-12-06 14:56:07.622558|\n",
      "|02      |Sở GDĐT TP. Hồ Chí Minh|2025-12-06 14:56:07.622558|2025-12-06 14:56:07.622558|\n",
      "|03      |Sở GDĐT Hải Phòng      |2025-12-06 14:56:07.622558|2025-12-06 14:56:07.622558|\n",
      "|04      |Sở GDĐT Đà Nẵng        |2025-12-06 14:56:07.622558|2025-12-06 14:56:07.622558|\n",
      "|05      |Sở GDĐT Hà Giang       |2025-12-06 14:56:07.622558|2025-12-06 14:56:07.622558|\n",
      "|06      |Sở GDĐT Cao Bằng       |2025-12-06 14:56:07.622558|2025-12-06 14:56:07.622558|\n",
      "|07      |Sở GDĐT Lai Châu       |2025-12-06 14:56:07.622558|2025-12-06 14:56:07.622558|\n",
      "|08      |Sở GDĐT Lào Cai        |2025-12-06 14:56:07.622558|2025-12-06 14:56:07.622558|\n",
      "|09      |Sở GDĐT Tuyên Quang    |2025-12-06 14:56:07.622558|2025-12-06 14:56:07.622558|\n",
      "|10      |Sở GDĐT Lạng Sơn       |2025-12-06 14:56:07.622558|2025-12-06 14:56:07.622558|\n",
      "+--------+-----------------------+--------------------------+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG REGION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_region = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"encoding\", \"UTF-8\").csv(\"s3a://bronze/structured_data/region.csv\")\n",
    "df_region_silver = df_region.select(\n",
    "    lpad(col(df_region.columns[0]).cast(\"string\"), 2, \"0\").alias(\"regionId\"),  # Format thành 2 chữ số: \"1\" -> \"01\"\n",
    "    col(df_region.columns[1]).cast(\"string\").alias(\"regionName\"),\n",
    "    current_timestamp().alias(\"created_at\"),\n",
    "    current_timestamp().alias(\"updated_at\")\n",
    ").filter(col(\"regionId\").isNotNull() & col(\"regionName\").isNotNull()).dropDuplicates([\"regionId\"])\n",
    "\n",
    "df_region_silver.writeTo(\"nessie.silver_tables.region\").using(\"iceberg\").createOrReplace()\n",
    "print(f\"Đã ghi {df_region_silver.count()} dòng vào region\")\n",
    "\n",
    "# Verify\n",
    "spark.table(\"nessie.silver_tables.region\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6217cf",
   "metadata": {},
   "source": [
    "## 8. Load Bảng STUDENT_SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d52c6fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOAD BẢNG STUDENT_SCORES - INCREMENTAL BY FILE (DELETE + APPEND)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Phát hiện 35 file mới cần xử lý.\n",
      "\n",
      "Đã load 51 môn học để mapping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staging có 5,202,085 dòng.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 6) / 7]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/local/lib/python3.10/socket.py\", line 717, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "25/12/06 14:57:56 ERROR TaskSchedulerImpl: Lost executor 0 on 172.18.0.8: Command exited with code 137\n",
      "[Stage 115:>                                                        (0 + 4) / 7]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 145\u001b[0m\n\u001b[1;32m    136\u001b[0m table_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnessie.silver_tables.student_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# =====================================================\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# 6. XOÁ studentId CŨ BẰNG CÁCH COLLECT RA PYTHON + DELETE IN (...)\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# =====================================================\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Lấy list studentId distinct trong batch mới\u001b[39;00m\n\u001b[1;32m    143\u001b[0m new_ids \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    144\u001b[0m     row\u001b[38;5;241m.\u001b[39mstudentId\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf_student_scores_stage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstudentId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistinct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m ]\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSố studentId distinct trong batch mới: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_ids)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Kiểm tra bảng silver đã tồn tại chưa\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, trim, regexp_replace, current_timestamp, lit,\n",
    "    concat, substring, udf, input_file_name, regexp_extract\n",
    ")\n",
    "from pyspark.sql.types import MapType, IntegerType, DoubleType\n",
    "from typing import Dict\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG STUDENT_SCORES - INCREMENTAL BY FILE (DELETE + APPEND)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =====================================================\n",
    "# 0. TẠO BẢNG LOG INGEST (LƯU FILE ĐÃ XỬ LÝ) NẾU CHƯA CÓ\n",
    "# =====================================================\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nessie.silver_tables.student_scores_ingest_log (\n",
    "    path STRING,\n",
    "    year INT,\n",
    "    processed_at TIMESTAMP\n",
    ") USING iceberg\n",
    "\"\"\")\n",
    "\n",
    "# =====================================================\n",
    "# 1. LẤY DANH SÁCH TẤT CẢ FILE CSV HIỆN CÓ TRONG BRONZE\n",
    "#    + TRỪ ĐI NHỮNG FILE ĐÃ INGEST (log)\n",
    "# =====================================================\n",
    "\n",
    "df_files = (\n",
    "    spark.read.format(\"binaryFile\")\n",
    "    .option(\"pathGlobFilter\", \"*.csv\")\n",
    "    .load(\"s3a://bronze/structured_data/điểm từng thí sinh/*/*.csv\")\n",
    "    .select(\"path\")\n",
    ")\n",
    "\n",
    "df_log = spark.table(\"nessie.silver_tables.student_scores_ingest_log\")\n",
    "\n",
    "df_new_files = df_files.join(df_log, on=\"path\", how=\"left_anti\")\n",
    "new_files = [r.path for r in df_new_files.collect()]\n",
    "\n",
    "if not new_files:\n",
    "    print(\" Không có file mới nào, dừng job.\")\n",
    "else:\n",
    "    print(f\" Phát hiện {len(new_files)} file mới cần xử lý.\")\n",
    "\n",
    "    # =====================================================\n",
    "    # 2. ĐỌC CHỈ CÁC FILE MỚI + THÊM CỘT YEAR\n",
    "    # =====================================================\n",
    "\n",
    "    df_scores_raw = (\n",
    "        spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"false\")\n",
    "        .option(\"encoding\", \"UTF-8\")\n",
    "        .csv(new_files)\n",
    "        .withColumn(\"path\", input_file_name())\n",
    "    )\n",
    "\n",
    "    df_scores_raw = df_scores_raw.withColumn(\n",
    "        \"Year\",\n",
    "        regexp_extract(col(\"path\"), r\"/(\\d{4})/\", 1).cast(\"int\")\n",
    "    )\n",
    "\n",
    "    # =====================================================\n",
    "    # 3. LOAD LOOKUP MÔN HỌC\n",
    "    # =====================================================\n",
    "\n",
    "    df_subject_lookup = spark.table(\"nessie.silver_tables.subject\").select(\"subjectId\", \"subjectName\")\n",
    "    subject_map = {row.subjectName: row.subjectId for row in df_subject_lookup.collect()}\n",
    "    print(f\"\\nĐã load {len(subject_map)} môn học để mapping\")\n",
    "\n",
    "    # =====================================================\n",
    "    # 4. UDF PARSE ĐIỂM → Map<subjectId, score>\n",
    "    # =====================================================\n",
    "\n",
    "    def parse_scores_with_subject_id(score_string: str) -> Dict[int, float]:\n",
    "        if not score_string or score_string.strip() == \"\":\n",
    "            return {}\n",
    "        scores_dict = {}\n",
    "        try:\n",
    "            pairs = score_string.split(\",\")\n",
    "            for pair in pairs:\n",
    "                if \":\" in pair:\n",
    "                    subject_name, score = pair.split(\":\")\n",
    "                    subject_name = subject_name.strip()\n",
    "                    # Map tên môn -> subjectId\n",
    "                    if subject_name in subject_map:\n",
    "                        subject_id = subject_map[subject_name]\n",
    "                        try:\n",
    "                            scores_dict[subject_id] = float(score.strip())\n",
    "                        except:\n",
    "                            pass\n",
    "        except:\n",
    "            pass\n",
    "        return scores_dict\n",
    "\n",
    "    parse_scores_udf = udf(parse_scores_with_subject_id, MapType(IntegerType(), DoubleType()))\n",
    "\n",
    "    # =====================================================\n",
    "    # 5. TRANSFORM → DATAFRAME STAGING (KHÔNG MERGE)\n",
    "    # =====================================================\n",
    "\n",
    "    # 1️ Biến đầy đủ để append vào silver\n",
    "    df_student_scores_stage = (\n",
    "        df_scores_raw\n",
    "        .withColumn(\"studentId\", concat(col(\"SBD\"), col(\"Year\").cast(\"string\")))\n",
    "        .withColumn(\"scores\", parse_scores_udf(col(\"DiemThi\")))   # UDF ở đây\n",
    "        .withColumn(\"regionId\", substring(col(\"SBD\"), 1, 2).cast(\"string\"))\n",
    "        .select(\n",
    "            col(\"studentId\").cast(\"string\"),\n",
    "            col(\"regionId\").cast(\"string\"),\n",
    "            col(\"Year\").cast(\"int\").alias(\"year\"),\n",
    "            col(\"scores\")\n",
    "        )\n",
    "        .filter(\n",
    "            col(\"studentId\").isNotNull() &\n",
    "            col(\"year\").isNotNull() &\n",
    "            col(\"scores\").isNotNull()\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 2️ Biến thứ hai chỉ có studentId — KHÔNG UDF → dùng để DELETE\n",
    "    df_student_ids = (\n",
    "        df_scores_raw\n",
    "        .withColumn(\"studentId\", concat(col(\"SBD\"), col(\"Year\").cast(\"string\")))\n",
    "        .select(\"studentId\")\n",
    "        .filter(col(\"studentId\").isNotNull())\n",
    "        # .dropDuplicates([\"studentId\"])\n",
    "    )\n",
    "    \n",
    "    df_student_ids.createOrReplaceTempView(\"student_scores_new_ids\")\n",
    "\n",
    "\n",
    "    staging_count = df_student_scores_stage.count()\n",
    "    print(f\"Staging có {staging_count:,} dòng.\")\n",
    "\n",
    "    table_name = \"nessie.silver_tables.student_scores\"\n",
    "\n",
    "        # =====================================================\n",
    "    # 6. XOÁ studentId CŨ BẰNG CÁCH COLLECT RA PYTHON + DELETE IN (...)\n",
    "    # =====================================================\n",
    "\n",
    "    # Lấy list studentId distinct trong batch mới\n",
    "    new_ids = [\n",
    "        row.studentId\n",
    "        for row in df_student_scores_stage.select(\"studentId\").distinct().collect()\n",
    "    ]\n",
    "\n",
    "    print(f\"Số studentId distinct trong batch mới: {len(new_ids):,}\")\n",
    "\n",
    "    # Kiểm tra bảng silver đã tồn tại chưa\n",
    "    try:\n",
    "        spark.table(table_name)\n",
    "        table_exists = True\n",
    "        print(f\"Bảng {table_name} đã tồn tại → DELETE theo list studentId + APPEND.\")\n",
    "    except Exception:\n",
    "        table_exists = False\n",
    "        print(f\"Bảng {table_name} chưa tồn tại → tạo mới từ batch, không cần xoá.\")\n",
    "\n",
    "    silver_count = spark.table(table_name).count() if table_exists else 0\n",
    "    print(f\"Số dòng trong bảng silver hiện tại: {silver_count:,}\")\n",
    "\n",
    "    if not table_exists:\n",
    "        # 1️ BẢNG CHƯA TỒN TẠI → TẠO MỚI\n",
    "        (\n",
    "            df_student_scores_stage\n",
    "            .withColumn(\"created_at\", current_timestamp())\n",
    "            .withColumn(\"updated_at\", current_timestamp())\n",
    "            .writeTo(table_name)\n",
    "            .using(\"iceberg\")\n",
    "            .createOrReplace()\n",
    "        )\n",
    "        print(f\" Đã tạo mới bảng {table_name} với {staging_count:,} dòng.\")\n",
    "    \n",
    "    elif silver_count == 0:\n",
    "        # 2️ BẢNG TỒN TẠI NHƯNG RỖNG → KHÔNG XOÁ, CHỈ APPEND\n",
    "        print(\" Bảng silver đã tồn tại nhưng rỗng → chỉ append, không xoá.\")\n",
    "    \n",
    "        (\n",
    "            df_student_scores_stage\n",
    "            .withColumn(\"created_at\", current_timestamp())\n",
    "            .withColumn(\"updated_at\", current_timestamp())\n",
    "            .writeTo(table_name)\n",
    "            .using(\"iceberg\")\n",
    "            .append()\n",
    "        )\n",
    "        print(f\" Đã append {staging_count:,} dòng mới vào {table_name}.\")\n",
    "    \n",
    "    elif new_ids:\n",
    "        # 3️ BẢNG TỒN TẠI VÀ new_ids KHÔNG RỖNG → DELETE + APPEND\n",
    "        print(\"Bảng silver có dữ liệu → DELETE + APPEND.\")\n",
    "    \n",
    "        chunk_size = 500\n",
    "        from math import ceil\n",
    "    \n",
    "        num_chunks = ceil(len(new_ids) / chunk_size)\n",
    "        print(f\"Chia studentId thành {num_chunks} chunk để xoá...\")\n",
    "    \n",
    "        for i in range(num_chunks):\n",
    "            chunk = new_ids[i * chunk_size:(i + 1) * chunk_size]\n",
    "            escaped_ids = [sid.replace(\"'\", \"''\") for sid in chunk]\n",
    "            in_list = \",\".join([f\"'{sid}'\" for sid in escaped_ids])\n",
    "    \n",
    "            sql_delete = f\"\"\"\n",
    "                DELETE FROM {table_name}\n",
    "                WHERE studentId IN ({in_list})\n",
    "            \"\"\"\n",
    "            spark.sql(sql_delete)\n",
    "    \n",
    "        print(\" Đã xoá xong các studentId cũ trong silver.\")\n",
    "    \n",
    "        (\n",
    "            df_student_scores_stage\n",
    "            .withColumn(\"created_at\", current_timestamp())\n",
    "            .withColumn(\"updated_at\", current_timestamp())\n",
    "            .writeTo(table_name)\n",
    "            .using(\"iceberg\")\n",
    "            .append()\n",
    "        )\n",
    "        print(f\" Đã append {staging_count:,} dòng mới.\")\n",
    "    \n",
    "    else:\n",
    "        # 4️ new_ids rỗng → không xoá, không append\n",
    "        print(\" Batch mới không có studentId nào hợp lệ → không làm gì cả.\")\n",
    "\n",
    "\n",
    "    # =====================================================\n",
    "    # 7. GHI LOG FILE ĐÃ XỬ LÝ\n",
    "    # =====================================================\n",
    "\n",
    "    from pyspark.sql.functions import array, explode\n",
    "\n",
    "    df_new_files_log = (\n",
    "        df_new_files\n",
    "        .withColumn(\"year\", regexp_extract(col(\"path\"), r\"/(\\d{4})/\", 1).cast(\"int\"))\n",
    "        .withColumn(\"processed_at\", current_timestamp())\n",
    "    )\n",
    "\n",
    "    (\n",
    "        df_new_files_log\n",
    "        .writeTo(\"nessie.silver_tables.student_scores_ingest_log\")\n",
    "        .using(\"iceberg\")\n",
    "        .append()\n",
    "    )\n",
    "\n",
    "    print(f\"Đã ghi log {df_new_files_log.count():,} file đã xử lý.\")\n",
    "\n",
    "    # =====================================================\n",
    "    # 8. VERIFY\n",
    "    # =====================================================\n",
    "\n",
    "    print(\"\\nMẫu dữ liệu student_scores:\")\n",
    "    spark.table(table_name).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce95322d",
   "metadata": {},
   "source": [
    "## 9. Load Bảng ARTICLE và COMMENT từ TikTok Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db0864e1-b0e6-4467-a209-b2e45e708e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "JOB 1: LOAD TIKTOK POSTS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xu ly 1 file Post moi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 115:================================>                        (4 + 3) / 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dang ghi vao Iceberg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 14:59:25 ERROR TaskSchedulerImpl: Lost executor 1 on 172.18.0.9: Command exited with code 137\n",
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:713)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:757)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:65)\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoan tat.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, when, coalesce, trim,\n",
    "    to_timestamp, current_timestamp,\n",
    "    input_file_name, regexp_replace\n",
    ")\n",
    "\n",
    "# ====================================================\n",
    "# CẤU HÌNH\n",
    "# ====================================================\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"33554432\")\n",
    "\n",
    "POSTS_PATH = \"s3a://bronze/MangXaHoi/tiktok-data/posts/*.csv\"\n",
    "TABLE_ARTICLE = \"nessie.silver_tables.article\"\n",
    "TABLE_LOG = \"nessie.silver_tables.tiktok_posts_files_log\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"JOB 1: LOAD TIKTOK POSTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Check file moi\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {TABLE_LOG} (file_path STRING, load_time TIMESTAMP) USING iceberg\")\n",
    "\n",
    "df_all = spark.read.format(\"binaryFile\").option(\"pathGlobFilter\", \"*.csv\").load(POSTS_PATH).select(\"path\")\n",
    "try:\n",
    "    df_processed = spark.table(TABLE_LOG).select(\"file_path\")\n",
    "    df_new_files = df_all.join(df_processed, df_all.path == col(\"file_path\"), \"left_anti\")\n",
    "except:\n",
    "    df_new_files = df_all\n",
    "\n",
    "new_files = [r.path for r in df_new_files.collect()]\n",
    "\n",
    "if not new_files:\n",
    "    print(\"Khong co file Post moi.\")\n",
    "else:\n",
    "    print(f\"Xu ly {len(new_files)} file Post moi.\")\n",
    "\n",
    "    # 2. Doc & Transform\n",
    "    df_raw = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"false\").csv(new_files)\n",
    "\n",
    "    df_trans = (\n",
    "        df_raw\n",
    "        .withColumn(\"timePublish\", \n",
    "            coalesce(\n",
    "                to_timestamp(col(\"TimePublish\"), \"dd-MM-yyyy\"),\n",
    "                to_timestamp(col(\"TimePublish\"), \"d-M-yyyy\"), \n",
    "                to_timestamp(regexp_replace(col(\"TimePublish\"), r\".*(\\d{1,2})\\s+Tháng\\s+(\\d{1,2}),\\s+(\\d{4}).*\", \"$1-$2-$3\"), \"d-M-yyyy\"),\n",
    "                current_timestamp()\n",
    "            ))\n",
    "        .withColumn(\"likeCount\", \n",
    "            when(col(\"Like\").contains(\"K\"), (regexp_replace(col(\"Like\"), \"K\", \"\").cast(\"float\")*1000).cast(\"int\"))\n",
    "            .when(col(\"Like\").contains(\"M\"), (regexp_replace(col(\"Like\"), \"M\", \"\").cast(\"float\")*1000000).cast(\"int\"))\n",
    "            .otherwise(coalesce(col(\"Like\").cast(\"int\"), lit(0))))\n",
    "        .withColumn(\"commentCount\", \n",
    "            when(col(\"Comment\").contains(\"K\"), (regexp_replace(col(\"Comment\"), \"K\", \"\").cast(\"float\")*1000).cast(\"int\"))\n",
    "            .when(col(\"Comment\").contains(\"M\"), (regexp_replace(col(\"Comment\"), \"M\", \"\").cast(\"float\")*1000000).cast(\"int\"))\n",
    "            .otherwise(coalesce(col(\"Comment\").cast(\"int\"), lit(0))))\n",
    "        .withColumn(\"shareCount\", \n",
    "            when(col(\"Share\").contains(\"K\"), (regexp_replace(col(\"Share\"), \"K\", \"\").cast(\"float\")*1000).cast(\"int\"))\n",
    "            .when(col(\"Share\").contains(\"M\"), (regexp_replace(col(\"Share\"), \"M\", \"\").cast(\"float\")*1000000).cast(\"int\"))\n",
    "            .otherwise(coalesce(col(\"Share\").cast(\"int\"), lit(0))))\n",
    "        .select(\n",
    "            trim(col(\"ID\")).alias(\"articleID\"),  # ID TikTok -> articleID\n",
    "            col(\"Description\").alias(\"description\"), \n",
    "            col(\"Author\").alias(\"author\"),\n",
    "            col(\"Url\").alias(\"url\"),\n",
    "            col(\"timePublish\"),\n",
    "            col(\"likeCount\"), col(\"commentCount\"), col(\"shareCount\"),\n",
    "            lit(\"TikTok\").alias(\"type\"),\n",
    "            current_timestamp().alias(\"created_at\"),\n",
    "            current_timestamp().alias(\"updated_at\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 3. Ghi du lieu (Append)\n",
    "    print(\"Dang ghi vao Iceberg...\")\n",
    "    df_trans.writeTo(TABLE_ARTICLE).using(\"iceberg\").append()\n",
    "    \n",
    "    # 4. Ghi Log\n",
    "    spark.createDataFrame([(f,) for f in new_files], [\"file_path\"]) \\\n",
    "          .withColumn(\"load_time\", current_timestamp()) \\\n",
    "          .writeTo(TABLE_LOG).using(\"iceberg\").append()\n",
    "    \n",
    "    print(\"Hoan tat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd9115d-688e-4eaa-a6ae-0c964c7e45d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, when, coalesce, trim,\n",
    "    to_timestamp, current_timestamp,\n",
    "    input_file_name, regexp_replace\n",
    ")\n",
    "\n",
    "# ====================================================\n",
    "# CẤU HÌNH CỰC NHẸ (ULTRA LOW RESOURCE)\n",
    "# ====================================================\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"16777216\") # Giam xuong 16MB/task de doc it RAM hon\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"20\")            # Tang partition len 20 de chia nho viec\n",
    "\n",
    "# Path\n",
    "POSTS_PATH = \"s3a://bronze/MangXaHoi/tiktok-data/comments/*.csv\"\n",
    "TABLE_COMMENT = \"nessie.silver_tables.comment\"\n",
    "TABLE_LOG = \"nessie.silver_tables.tiktok_comments_files_log\"\n",
    "\n",
    "# GIAM BATCH SIZE XUONG 10 (Quan trong nhat de tranh OOM)\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"JOB 2: LOAD TIKTOK COMMENTS (TINY BATCH {BATCH_SIZE} - ULTRA SAFE MODE)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Tao bang Log (Fix Py4J)\n",
    "try:\n",
    "    spark.table(TABLE_LOG)\n",
    "except:\n",
    "    print(f\"-> Bang log {TABLE_LOG} chua ton tai. Dang tao moi...\")\n",
    "    log_schema = StructType([StructField(\"file_path\", StringType(), False), StructField(\"load_time\", TimestampType(), False)])\n",
    "    spark.createDataFrame([], log_schema).writeTo(TABLE_LOG).using(\"iceberg\").create()\n",
    "\n",
    "# 2. Loc file moi (DEBUG MODE)\n",
    "print(\"-> Dang quet file nguon...\")\n",
    "df_all = spark.read.format(\"binaryFile\").option(\"pathGlobFilter\", \"*.csv\").load(POSTS_PATH).select(\"path\")\n",
    "total_files_count = df_all.count()\n",
    "print(f\"   Tong so file trong folder: {total_files_count}\")\n",
    "\n",
    "try:\n",
    "    # Doc bang log\n",
    "    df_processed = spark.table(TABLE_LOG).select(\"file_path\").distinct()\n",
    "    processed_count = df_processed.count()\n",
    "    print(f\"   So file da xu ly truoc do (Log): {processed_count}\")\n",
    "    \n",
    "    # --- FIX LOGIC LOC FILE ---\n",
    "    df_new_files = df_all.alias(\"src\").join(\n",
    "        df_processed.alias(\"log\"), \n",
    "        col(\"src.path\") == col(\"log.file_path\"), \n",
    "        \"left_anti\"\n",
    "    )\n",
    "    \n",
    "    if processed_count > 0 and df_new_files.count() == total_files_count:\n",
    "        print(\"   [DEBUG] CANH BAO: Log co du lieu nhung khong loc duoc file nao.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   [WARNING] Loi doc Log: {e}\")\n",
    "    df_new_files = df_all\n",
    "\n",
    "# Lay danh sach file can xu ly\n",
    "all_new_files = [r.path for r in df_new_files.collect()]\n",
    "# Quan trong: Cat batch nho\n",
    "files_to_process = all_new_files[:BATCH_SIZE]\n",
    "\n",
    "if not files_to_process:\n",
    "    print(\"-> KHONG CO FILE COMMENT MOI.\")\n",
    "else:\n",
    "    print(f\"-> Tim thay {len(all_new_files)} file moi.\")\n",
    "    print(f\"-> Dot nay se xu ly {len(files_to_process)} file (Batch nho de tranh sap).\")\n",
    "\n",
    "    # 3. Xu ly tung file\n",
    "    for i, file_path in enumerate(files_to_process):\n",
    "        filename = file_path.split('/')[-1]\n",
    "        print(f\"\\n--- [{i+1}/{len(files_to_process)}] Dang xu ly: {filename} ---\")\n",
    "        \n",
    "        try:\n",
    "            df_raw = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"false\").csv(file_path)\n",
    "\n",
    "            if \"ID_Post\" not in df_raw.columns:\n",
    "                print(f\"   [SKIP] File loi format (Thieu ID_Post).\")\n",
    "                spark.createDataFrame([(file_path,)], [\"file_path\"]).withColumn(\"load_time\", current_timestamp()).writeTo(TABLE_LOG).using(\"iceberg\").append()\n",
    "                continue\n",
    "\n",
    "            # Transform\n",
    "            df_trans = df_raw.select(\n",
    "                trim(col(\"ID_Post\")).alias(\"articleID\"), \n",
    "                col(\"Name\").alias(\"name\"),\n",
    "                col(\"TagName\").alias(\"tagName\"),\n",
    "                col(\"URL\").alias(\"urlUser\"),\n",
    "                col(\"Comment\").alias(\"comment\"),\n",
    "                coalesce(\n",
    "                    to_timestamp(regexp_replace(col(\"Time\"), r\"(\\d{1,2})[-/](\\d{1,2})[-/](\\d{4}).*\", \"$1-$2-$3\"), \"d-M-yyyy\"),\n",
    "                    to_timestamp(regexp_replace(col(\"Time\"), r\".*trước.*\", \"1970-01-01\"), \"yyyy-MM-dd\"),\n",
    "                    current_timestamp()\n",
    "                ).alias(\"commentTime\"),\n",
    "                coalesce(col(\"Likes\").cast(\"int\"), lit(0)).alias(\"commentLike\"),\n",
    "                when(col(\"LevelComment\") == \"Yes\", 2).otherwise(1).alias(\"levelComment\"),\n",
    "                col(\"RepliedTo\").alias(\"replyTo\"),\n",
    "                coalesce(col(\"NumberOfReplies\").cast(\"int\"), lit(0)).alias(\"numberOfReply\"),\n",
    "                current_timestamp().alias(\"created_at\"),\n",
    "                current_timestamp().alias(\"updated_at\")\n",
    "            ).filter(col(\"articleID\").isNotNull() & (col(\"articleID\") != \"\"))\n",
    "            \n",
    "            # GHI TRUC TIEP\n",
    "            if not df_trans.rdd.isEmpty():\n",
    "                # --- CHIEN THUAT GHI AN TOAN NHAT ---\n",
    "                # 1. Repartition(20): Chia nho data ra 20 task de xu ly nhe nhang hon\n",
    "                # 2. sortWithinPartitions: Giup Iceberg Writer khong phai buffer qua nhieu\n",
    "                print(\"   -> Dang chuan bi du lieu (Shuffle nhe)...\")\n",
    "                df_optimized = df_trans.repartition(20, \"articleID\").sortWithinPartitions(\"articleID\")\n",
    "                \n",
    "                print(\"   -> Dang ghi vao Iceberg...\")\n",
    "                df_optimized.writeTo(TABLE_COMMENT).using(\"iceberg\").append()\n",
    "                print(\"   -> Da ghi xong.\")\n",
    "            \n",
    "            # Ghi Log\n",
    "            spark.createDataFrame([(file_path,)], [\"file_path\"]).withColumn(\"load_time\", current_timestamp()).writeTo(TABLE_LOG).using(\"iceberg\").append()\n",
    "            \n",
    "            # Don dep RAM ngay lap tuc\n",
    "            df_trans.unpersist()\n",
    "            spark.catalog.clearCache()\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   [ERROR] {e}\")\n",
    "\n",
    "    print(f\"Hoan tat batch {len(files_to_process)} file. Hay chay lai de lam tiep batch sau.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6cac65",
   "metadata": {},
   "source": [
    "## 10. Load Bảng ARTICLE và COMMENT từ Facebook Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d43ae7-672d-4299-a206-dce3c7664c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "JOB 3: LOAD FACEBOOK POSTS (MERGE/UPSERT)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xu ly 1 file moi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 15:01:38 ERROR TaskSchedulerImpl: Lost executor 3 on 172.18.0.8: worker lost: Not receiving heartbeat for 60 seconds\n",
      "25/12/06 15:01:38 ERROR TaskSchedulerImpl: Lost executor 2 on 172.18.0.7: Command exited with code 137\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoan tat.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import (\n",
    "    col, trim, to_timestamp, lit, current_timestamp, \n",
    "    input_file_name, coalesce, udf\n",
    ")\n",
    "\n",
    "# ====================================================\n",
    "# PARSE TIME UDF\n",
    "# ====================================================\n",
    "MONTH_MAP = {\"Tháng 1\": \"01\", \"Tháng 2\": \"02\", \"Tháng 3\": \"03\", \"Tháng 4\": \"04\", \"Tháng 5\": \"05\", \"Tháng 6\": \"06\", \"Tháng 7\": \"07\", \"Tháng 8\": \"08\", \"Tháng 9\": \"09\", \"Tháng 10\": \"10\", \"Tháng 11\": \"11\", \"Tháng 12\": \"12\"}\n",
    "\n",
    "def parse_vietnam_datetime(dt_str):\n",
    "    if not dt_str: return None\n",
    "    try:\n",
    "        if \",\" in dt_str: dt_str = dt_str.split(\",\", 1)[1].strip()\n",
    "        match = re.search(r\"(\\d+)\\s+(Tháng\\s+\\d+)\", dt_str)\n",
    "        if not match: return None\n",
    "        day, month_text = match.group(1), match.group(2)\n",
    "        month = MONTH_MAP.get(month_text, \"01\")\n",
    "        year = re.search(r\",\\s*(\\d{4})\", dt_str).group(1) if re.search(r\",\\s*(\\d{4})\", dt_str) else \"2025\"\n",
    "        time_str = re.search(r\"lúc\\s+(\\d{1,2}:\\d{2})\", dt_str).group(1) if re.search(r\"lúc\\s+(\\d{1,2}:\\d{2})\", dt_str) else \"00:00\"\n",
    "        return f\"{year}-{month}-{int(day):02d} {time_str}:00\"\n",
    "    except: return None\n",
    "\n",
    "parse_vn_time_udf = udf(parse_vietnam_datetime, StringType())\n",
    "\n",
    "# ====================================================\n",
    "# CẤU HÌNH\n",
    "# ====================================================\n",
    "POSTS_GLOB = \"s3a://bronze/MangXaHoi/Face-data/posts/*.csv\"\n",
    "TABLE_ARTICLE = \"nessie.silver_tables.article\"\n",
    "TABLE_LOG = \"nessie.silver_tables.fb_posts_files_log\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"JOB 3: LOAD FACEBOOK POSTS (MERGE/UPSERT)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {TABLE_LOG} (file_path STRING, load_time TIMESTAMP) USING iceberg\")\n",
    "\n",
    "# 1. Loc file moi\n",
    "df_all = spark.read.option(\"header\", \"true\").csv(POSTS_GLOB).withColumn(\"file_path\", input_file_name())\n",
    "try:\n",
    "    df_proc = spark.table(TABLE_LOG).select(\"file_path\").distinct()\n",
    "    new_files = [r.file_path for r in df_all.join(df_proc, \"file_path\", \"left_anti\").select(\"file_path\").distinct().collect()]\n",
    "except:\n",
    "    new_files = [r.file_path for r in df_all.select(\"file_path\").distinct().collect()]\n",
    "\n",
    "if new_files:\n",
    "    print(f\"Xu ly {len(new_files)} file moi.\")\n",
    "    \n",
    "    # 2. Transform\n",
    "    df_raw = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(new_files)\n",
    "    df_trans = df_raw.select(\n",
    "        trim(col(\"ID\")).alias(\"articleID\"),         # ID FB -> articleID\n",
    "        trim(col(\"Description\")).alias(\"description\"),\n",
    "        trim(col(\"Author\")).alias(\"author\"),\n",
    "        trim(col(\"Url\")).alias(\"url\"),\n",
    "        coalesce(to_timestamp(parse_vn_time_udf(col(\"TimePublish\"))), to_timestamp(col(\"TimePublish\")), current_timestamp()).alias(\"timePublish\"),\n",
    "        coalesce(col(\"Like\").cast(\"int\"), lit(0)).alias(\"likeCount\"),\n",
    "        coalesce(col(\"Share\").cast(\"int\"), lit(0)).alias(\"shareCount\"),\n",
    "        coalesce(col(\"Comment\").cast(\"int\"), lit(0)).alias(\"commentCount\")\n",
    "    )\n",
    "\n",
    "    # 3. MERGE (Upsert)\n",
    "    df_trans.createOrReplaceTempView(\"fb_source\")\n",
    "    \n",
    "    # Update metrics\n",
    "    spark.sql(f\"\"\"\n",
    "    MERGE INTO {TABLE_ARTICLE} t USING fb_source s\n",
    "    ON t.url = s.url AND t.type = 'facebook'\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        t.description = s.description, t.timePublish = s.timePublish,\n",
    "        t.likeCount = s.likeCount, t.shareCount = s.shareCount, t.commentCount = s.commentCount,\n",
    "        t.updated_at = current_timestamp()\n",
    "    \"\"\")\n",
    "\n",
    "    # Insert new\n",
    "    spark.sql(f\"\"\"\n",
    "    INSERT INTO {TABLE_ARTICLE}\n",
    "    SELECT s.articleID, s.description, s.author, s.url, s.timePublish,\n",
    "           s.likeCount, s.commentCount, s.shareCount, 'facebook', current_timestamp(), current_timestamp()\n",
    "    FROM fb_source s\n",
    "    WHERE NOT EXISTS (SELECT 1 FROM {TABLE_ARTICLE} t WHERE t.url = s.url AND t.type = 'facebook')\n",
    "    \"\"\")\n",
    "    \n",
    "    # 4. Log\n",
    "    spark.createDataFrame([(f,) for f in new_files], [\"file_path\"]).withColumn(\"load_time\", current_timestamp()).writeTo(TABLE_LOG).using(\"iceberg\").append()\n",
    "    print(\"Hoan tat.\")\n",
    "else:\n",
    "    print(\"Khong co file moi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9611c5c2-66d6-44cb-a24e-eac348c1d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, when, coalesce, trim, broadcast,\n",
    "    to_timestamp, current_timestamp,\n",
    "    input_file_name, regexp_replace\n",
    ")\n",
    "\n",
    "# ====================================================\n",
    "# CẤU HÌNH TỐI ƯU\n",
    "# ====================================================\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"33554432\")\n",
    "# Giam shuffle partition de tiet kiem RAM\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "\n",
    "COMMENTS_GLOB = \"s3a://bronze/MangXaHoi/Face-data/comments/*.csv\"\n",
    "TABLE_COMMENT = \"nessie.silver_tables.comment\"\n",
    "TABLE_LOG = \"nessie.silver_tables.fb_comments_files_log\"\n",
    "\n",
    "# Batch size: Xu ly 50 file/lan chay\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"JOB 4: LOAD FACEBOOK COMMENTS (FAST APPEND - NO VALIDATION - BATCH {BATCH_SIZE})\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Loc file moi\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {TABLE_LOG} (file_path STRING, load_time TIMESTAMP) USING iceberg\")\n",
    "\n",
    "df_all = spark.read.format(\"binaryFile\").option(\"pathGlobFilter\", \"*.csv\").load(COMMENTS_GLOB).select(\"path\")\n",
    "try:\n",
    "    df_proc = spark.table(TABLE_LOG).select(\"file_path\").distinct()\n",
    "    df_new = df_all.join(df_proc, df_all.path == col(\"file_path\"), \"left_anti\")\n",
    "except:\n",
    "    df_new = df_all\n",
    "\n",
    "# Lay danh sach file can xu ly (Batching)\n",
    "all_new_files = [r.path for r in df_new.collect()]\n",
    "files_to_process = all_new_files[:BATCH_SIZE]\n",
    "\n",
    "if not files_to_process:\n",
    "    print(\"Khong co file moi.\")\n",
    "else:\n",
    "    print(f\"Tim thay {len(all_new_files)} file moi.\")\n",
    "    print(f\"-> Dot nay xu ly {len(files_to_process)} file.\")\n",
    "\n",
    "    # 2. Xu ly tung file\n",
    "    for i, file_path in enumerate(files_to_process):\n",
    "        filename = file_path.split('/')[-1]\n",
    "        print(f\"\\n--- [{i+1}/{len(files_to_process)}] Dang xu ly: {filename} ---\")\n",
    "        \n",
    "        try:\n",
    "            df_raw = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(file_path)\n",
    "            \n",
    "            if \"Id_post\" not in df_raw.columns:\n",
    "                print(f\"   [SKIP] Thieu cot Id_post.\")\n",
    "                spark.createDataFrame([(file_path,)], [\"file_path\"]).withColumn(\"load_time\", current_timestamp()).writeTo(TABLE_LOG).using(\"iceberg\").append()\n",
    "                continue\n",
    "\n",
    "            # Transform (Set NULL cho cot thieu)\n",
    "            df_trans = df_raw.select(\n",
    "                trim(col(\"Id_post\")).alias(\"articleID\"), \n",
    "                lit(None).cast(\"string\").alias(\"name\"),\n",
    "                lit(None).cast(\"string\").alias(\"tagName\"),\n",
    "                lit(None).cast(\"string\").alias(\"urlUser\"),\n",
    "                (col(\"Comment\") if \"Comment\" in df_raw.columns else lit(\"\")).alias(\"comment\"),\n",
    "                lit(None).cast(\"timestamp\").alias(\"commentTime\"),\n",
    "                lit(None).cast(\"int\").alias(\"commentLike\"),\n",
    "                lit(None).cast(\"int\").alias(\"levelComment\"),\n",
    "                lit(None).cast(\"string\").alias(\"replyTo\"),\n",
    "                lit(None).cast(\"int\").alias(\"numberOfReply\"),\n",
    "                current_timestamp().alias(\"created_at\"),\n",
    "                current_timestamp().alias(\"updated_at\")\n",
    "            ).filter(col(\"articleID\").isNotNull() & (col(\"articleID\") != \"\"))\n",
    "\n",
    "            # --- GHI TRUC TIEP (APPEND ONLY) ---\n",
    "            # Khong kiem tra Article ton tai\n",
    "            # Khong kiem tra Duplicate\n",
    "            \n",
    "            if not df_trans.rdd.isEmpty():\n",
    "                print(f\"   -> Dang ghi comment vao Iceberg...\")\n",
    "                # Repartition(1) de gom thanh 1 file parquet gon gang tren o cung\n",
    "                df_trans.repartition(1).writeTo(TABLE_COMMENT).using(\"iceberg\").append()\n",
    "                print(\"   -> Xong.\")\n",
    "            else:\n",
    "                print(\"   -> File rong hoac khong co ID bai viet.\")\n",
    "\n",
    "            # Log\n",
    "            spark.createDataFrame([(file_path,)], [\"file_path\"]).withColumn(\"load_time\", current_timestamp()).writeTo(TABLE_LOG).using(\"iceberg\").append()\n",
    "            \n",
    "            # Giai phong bo nho\n",
    "            spark.catalog.clearCache()\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   [ERROR] {e}\")\n",
    "\n",
    "    print(f\"Hoan tat batch {len(files_to_process)} file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63a01ba3-579e-4e90-b89d-827dddc5197c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "KIỂM TRA SỐ LƯỢNG BÀI VIẾT (FACEBOOK & TIKTOK)\n",
      "================================================================================\n",
      "\n",
      "--- Thống kê chi tiết theo nguồn ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    type|count|\n",
      "+--------+-----+\n",
      "|  TikTok| 1346|\n",
      "|facebook| 1949|\n",
      "+--------+-----+\n",
      "\n",
      "-> TỔNG CỘNG: 3295 bài viết.\n",
      "\n",
      "--- 5 bài viết mới nhất ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===================================================>   (216 + 6) / 232]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+--------------------------------------------------+--------------------------+\n",
      "|       articleID|    type|                                       description|                created_at|\n",
      "+----------------+--------+--------------------------------------------------+--------------------------+\n",
      "|1925786774945096|facebook|                    Topic: Đại học Y Dược Cần Thơ.|2025-12-06 15:00:21.496511|\n",
      "|1918505135673260|facebook|                    Topic: Ngành Kỹ thuật Hóa học.|2025-12-06 15:00:21.496511|\n",
      "|1917479792442461|facebook|Chuyển Trường Đại học Phenikaa thành Đại học Ph...|2025-12-06 15:00:21.496511|\n",
      "|1918095542380886|facebook|Mọi người ơi, tình hình là giờ em mới tìm được ...|2025-12-06 15:00:21.496511|\n",
      "|1915006809356426|facebook|Tốt nghiệp xong, tân cử nhân Trường Đại học Thă...|2025-12-06 15:00:21.496511|\n",
      "+----------------+--------+--------------------------------------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Hoàn tất kiểm tra.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, lit\n",
    "\n",
    "# ====================================================\n",
    "# CẤU HÌNH\n",
    "# ====================================================\n",
    "TABLE_ARTICLE = \"nessie.silver_tables.article\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"KIỂM TRA SỐ LƯỢNG BÀI VIẾT (FACEBOOK & TIKTOK)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Đọc dữ liệu từ bảng Article\n",
    "try:\n",
    "    df_article = spark.table(TABLE_ARTICLE)\n",
    "    \n",
    "    # 2. Thống kê theo loại (Type)\n",
    "    print(\"\\n--- Thống kê chi tiết theo nguồn ---\")\n",
    "    df_stats = df_article.groupBy(\"type\").count().orderBy(\"type\")\n",
    "    df_stats.show()\n",
    "    \n",
    "    # 3. Tính tổng số lượng\n",
    "    total_count = df_article.count()\n",
    "    print(f\"-> TỔNG CỘNG: {total_count} bài viết.\")\n",
    "    \n",
    "    # 4. Kiểm tra mẫu dữ liệu (Optional)\n",
    "    print(\"\\n--- 5 bài viết mới nhất ---\")\n",
    "    df_article.select(\"articleID\", \"type\", \"description\", \"created_at\") \\\n",
    "              .orderBy(col(\"created_at\").desc()) \\\n",
    "              .show(5, truncate=50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi đọc bảng {TABLE_ARTICLE}: {e}\")\n",
    "    print(\"Có thể bảng chưa được tạo hoặc chưa có dữ liệu.\")\n",
    "\n",
    "print(\"\\nHoàn tất kiểm tra.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4c11d8e-7930-4437-bcc7-d868553d19dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Spark Session đã được dừng!\n"
     ]
    }
   ],
   "source": [
    "# Dừng Spark Session để giải phóng resources\n",
    "spark.stop()\n",
    "print(\" Spark Session đã được dừng!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
