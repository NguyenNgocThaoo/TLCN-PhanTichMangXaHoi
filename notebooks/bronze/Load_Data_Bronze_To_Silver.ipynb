{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c24213a7",
   "metadata": {},
   "source": [
    "# Load Data từ Bronze Layer sang Silver Layer\n",
    "\n",
    "Notebook này sẽ đọc dữ liệu từ Bronze layer (MinIO) và xử lý để load vào các bảng Iceberg trong Silver layer với Nessie catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063eaad0",
   "metadata": {},
   "source": [
    "## 1. Import Libraries và Khởi tạo Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c718b51-ae4c-4fcf-b0b4-fcf8b57dfad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import csv, io, os, re\n",
    "from datetime import datetime\n",
    "from typing import Dict\n",
    "\n",
    "# Cấu hình AWS/MinIO credentials\n",
    "os.environ.update({\n",
    "    'AWS_REGION': 'us-east-1',\n",
    "    'AWS_ACCESS_KEY_ID': 'admin',\n",
    "    'AWS_SECRET_ACCESS_KEY': 'admin123'\n",
    "})\n",
    "\n",
    "# Khởi tạo Spark Session với Nessie Catalog\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Load_Bronze_To_Silver\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.memory\", \"1536m\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    # Nessie Catalog\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v2\")\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://silver/\")\n",
    "    .config(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    # S3/MinIO Config\n",
    "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.access-key-id\", \"admin\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.secret-access-key\", \"admin123\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.region\", \"us-east-1\")\n",
    "    # Hadoop S3A Config\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.region\", \"us-east-1\")\n",
    "    # Executor Environment\n",
    "    .config(\"spark.executorEnv.AWS_REGION\", \"us-east-1\")\n",
    "    .config(\"spark.executorEnv.AWS_ACCESS_KEY_ID\", \"admin\")\n",
    "    .config(\"spark.executorEnv.AWS_SECRET_ACCESS_KEY\", \"admin123\")\n",
    "    # Local JAR files\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/hadoop-aws-3.3.4.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS nessie.silver_tables\")\n",
    "spark.sql(\"USE nessie.silver_tables\")\n",
    "print(f\" Spark Session initialized | Master: {spark.sparkContext.master} | App ID: {spark.sparkContext.applicationId}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43664703",
   "metadata": {},
   "source": [
    "## 2. Load Bảng SCHOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88066c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG SCHOOL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Đọc và merge tất cả các năm\n",
    "years = [2021, 2022, 2023, 2024, 2025]\n",
    "base_path = \"s3a://bronze/structured_data/danh sách các trường Đại Học (2021-2025)/Danh_sách_các_trường_Đại_Học_\"\n",
    "df_school = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv([f\"{base_path}{year}.csv\" for year in years]).select(\"TenTruong\", \"MaTruong\", \"TinhThanh\").dropDuplicates()\n",
    "\n",
    "# Transform\n",
    "df_school_silver = df_school.select(\n",
    "    col(\"MaTruong\").cast(\"string\").alias(\"schoolId\"),\n",
    "    col(\"TenTruong\").cast(\"string\").alias(\"schoolName\"),\n",
    "    col(\"TinhThanh\").cast(\"string\").alias(\"province\"),\n",
    "    current_timestamp().alias(\"created_at\"),\n",
    "    current_timestamp().alias(\"updated_at\")\n",
    ").filter(col(\"schoolId\").isNotNull() & col(\"schoolName\").isNotNull())\n",
    "\n",
    "# Ghi vào Silver\n",
    "df_school_silver.writeTo(\"nessie.silver_tables.school\").using(\"iceberg\").createOrReplace()\n",
    "print(f\"Đã ghi {df_school_silver.count()} dòng vào school\")\n",
    "\n",
    "# Verify\n",
    "spark.table(\"nessie.silver_tables.school\").show(5, truncate=False)\n",
    "\n",
    "# Giải phóng bộ nhớ\n",
    "df_school.unpersist()\n",
    "df_school_silver.unpersist()\n",
    "del df_school, df_school_silver\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"✓ Đã giải phóng bộ nhớ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0235c2c",
   "metadata": {},
   "source": [
    "## 3. Load Bảng MAJOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf9d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, trim, regexp_replace, current_timestamp\n",
    "\n",
    "df_major = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .csv(\"s3a://bronze/structured_data/danh sách các ngành đại học/Danh_sách_các_ngành.csv\")\n",
    "\n",
    "df_major_clean = df_major.select(\n",
    "    regexp_replace(trim(col(df_major.columns[0])).cast(\"string\"), r\"\\.0$\", \"\").alias(\"majorId\"),\n",
    "    trim(col(df_major.columns[1])).cast(\"string\").alias(\"majorName\")\n",
    ").filter(\n",
    "    (col(\"majorId\").isNotNull()) &\n",
    "    (col(\"majorName\").isNotNull()) &\n",
    "    (col(\"majorId\") != \"\") &\n",
    "    (col(\"majorName\") != \"\") &\n",
    "    (lower(col(\"majorId\")) != \"nan\")\n",
    ")\n",
    "\n",
    "# Chuẩn hoá để dedupe theo lowercase\n",
    "df_major_silver = df_major_clean \\\n",
    "    .withColumn(\"majorId_lower\", lower(col(\"majorId\"))) \\\n",
    "    .dropDuplicates([\"majorId_lower\"]) \\\n",
    "    .select(\n",
    "        col(\"majorId\"),\n",
    "        col(\"majorName\"),\n",
    "        current_timestamp().alias(\"created_at\"),\n",
    "        current_timestamp().alias(\"updated_at\")\n",
    "    )\n",
    "\n",
    "df_major_silver.writeTo(\"nessie.silver_tables.major\").using(\"iceberg\").createOrReplace()\n",
    "\n",
    "print(f\"Đã ghi {df_major_silver.count()} dòng vào major\")\n",
    "spark.table(\"nessie.silver_tables.major\").show(5, truncate=False)\n",
    "\n",
    "# Giải phóng bộ nhớ\n",
    "df_major.unpersist()\n",
    "df_major_clean.unpersist()\n",
    "df_major_silver.unpersist()\n",
    "del df_major, df_major_clean, df_major_silver\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"✓ Đã giải phóng bộ nhớ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d64a8",
   "metadata": {},
   "source": [
    "## 4. Load Bảng SUBJECT_GROUP và SUBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c053cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG SUBJECT_GROUP và SUBJECT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Đọc file tohop_mon_fixed.csv\n",
    "df_tohop = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"encoding\", \"UTF-8\").csv(\"s3a://bronze/structured_data/tohop_mon_fixed.csv\")\n",
    "\n",
    "# --- SUBJECT_GROUP ---\n",
    "df_subject_group_silver = df_tohop.select(\n",
    "    col(df_tohop.columns[0]).cast(\"int\").alias(\"subjectGroupId\"),\n",
    "    col(df_tohop.columns[1]).cast(\"string\").alias(\"subjectGroupName\"),\n",
    "    col(df_tohop.columns[2]).cast(\"string\").alias(\"subjectCombination\"),\n",
    "    current_timestamp().alias(\"created_at\"),\n",
    "    current_timestamp().alias(\"updated_at\")\n",
    ").filter(col(\"subjectGroupId\").isNotNull() & col(\"subjectGroupName\").isNotNull() & col(\"subjectCombination\").isNotNull()).dropDuplicates([\"subjectGroupName\", \"subjectCombination\"])\n",
    "df_subject_group_silver.writeTo(\"nessie.silver_tables.subject_group\").using(\"iceberg\").createOrReplace()\n",
    "print(f\"Đã ghi {df_subject_group_silver.count()} dòng vào subject_group\")\n",
    "\n",
    "# --- SUBJECT ---\n",
    "df_subject = (\n",
    "    df_tohop.select(explode(split(col(df_tohop.columns[2]), \"-\")).alias(\"subjectName\"))\n",
    "            .withColumn(\"subjectName\", trim(col(\"subjectName\")))\n",
    "            .filter(col(\"subjectName\").isNotNull() & (col(\"subjectName\") != \"\"))\n",
    "            .withColumn(\"subjectName_lower\", lower(col(\"subjectName\")))\n",
    "            # loại bỏ trùng theo chữ thường\n",
    "            .dropDuplicates([\"subjectName_lower\"])\n",
    ")\n",
    "\n",
    "window_spec = Window.orderBy(\"subjectName_lower\")\n",
    "df_subject_silver = df_subject.withColumn(\"subjectId\", row_number().over(window_spec)).select(\n",
    "    col(\"subjectId\").cast(\"int\"),\n",
    "    col(\"subjectName\").cast(\"string\"),\n",
    "    current_timestamp().alias(\"created_at\"),\n",
    "    current_timestamp().alias(\"updated_at\")\n",
    ")\n",
    "df_subject_silver.writeTo(\"nessie.silver_tables.subject\").using(\"iceberg\").createOrReplace()\n",
    "print(f\"Đã ghi {df_subject_silver.count()} dòng vào subject\")\n",
    "\n",
    "# Verify\n",
    "spark.table(\"nessie.silver_tables.subject_group\").orderBy(\"subjectGroupId\").show(5, truncate=False)\n",
    "spark.table(\"nessie.silver_tables.subject\").show(5, truncate=False)\n",
    "\n",
    "# Giải phóng bộ nhớ\n",
    "df_tohop.unpersist()\n",
    "df_subject_group_silver.unpersist()\n",
    "df_subject.unpersist()\n",
    "df_subject_silver.unpersist()\n",
    "del df_tohop, df_subject_group_silver, df_subject, df_subject_silver\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"✓ Đã giải phóng bộ nhớ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e6236f",
   "metadata": {},
   "source": [
    "## 5. Load Bảng SELECTION_METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG SELECTION_METHOD\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Đọc từ file benchmark để lấy các phương thức xét tuyển\n",
    "df_benchmark = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"encoding\", \"UTF-8\").csv(\"s3a://bronze/structured_data/điểm chuẩn các trường (2021-2025)/Điểm_chuẩn_các_ngành_đại_học_năm(2021-2025)*.csv\")\n",
    "\n",
    "# Lấy PhuongThuc và loại bỏ \"năm ...\"\n",
    "df_selection = df_benchmark.select(trim(regexp_replace(col(\"PhuongThuc\"), r\"\\s*năm\\s+\\d{4}.*$\", \"\")).alias(\"selectionMethodName\")).filter(col(\"selectionMethodName\").isNotNull() & (col(\"selectionMethodName\") != \"\")).distinct()\n",
    "\n",
    "window_spec = Window.orderBy(\"selectionMethodName\")\n",
    "df_selection_method_silver = df_selection.withColumn(\"selectionMethodId\", row_number().over(window_spec)).select(\n",
    "    col(\"selectionMethodId\").cast(\"int\"),\n",
    "    col(\"selectionMethodName\").cast(\"string\"),\n",
    "    current_timestamp().alias(\"created_at\"),\n",
    "    current_timestamp().alias(\"updated_at\")\n",
    ")\n",
    "df_selection_method_silver.writeTo(\"nessie.silver_tables.selection_method\").using(\"iceberg\").createOrReplace()\n",
    "print(f\"Đã ghi {df_selection_method_silver.count()} dòng vào selection_method\")\n",
    "\n",
    "# Verify\n",
    "spark.table(\"nessie.silver_tables.selection_method\").show(10, truncate=False)\n",
    "\n",
    "# Giải phóng bộ nhớ\n",
    "df_benchmark.unpersist()\n",
    "df_selection.unpersist()\n",
    "df_selection_method_silver.unpersist()\n",
    "del df_benchmark, df_selection, df_selection_method_silver\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"✓ Đã giải phóng bộ nhớ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911622a",
   "metadata": {},
   "source": [
    "## 6. Load Bảng GradingScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ca8916-830b-4f32-bbbb-d0ac527e6568",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG GRADING_SCALE TỪ PHANLOAITHANGDIEM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Đọc dữ liệu gốc từ file CSV (giống benchmark)\n",
    "df_raw = (\n",
    "    spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .option(\"encoding\", \"UTF-8\")\n",
    "        .csv(\"s3a://bronze/structured_data/điểm chuẩn các trường (2021-2025)/Điểm_chuẩn_các_ngành_đại_học_năm(2021-2025)*.csv\")\n",
    ")\n",
    "\n",
    "# 2. Lấy unique PhanLoaiThangDiem\n",
    "df_grading_raw = (\n",
    "    df_raw\n",
    "        .select(trim(col(\"PhanLoaiThangDiem\")).alias(\"description\"))\n",
    "        .filter(col(\"description\").isNotNull() & (col(\"description\") != \"\"))\n",
    "        .dropDuplicates([\"description\"])\n",
    ")\n",
    "\n",
    "# 3. Tách giá trị số trong description làm \"value\" (nếu có, vd: \"thang 40\" -> 40)\n",
    "df_grading = (\n",
    "    df_grading_raw\n",
    "        .withColumn(\n",
    "            \"value\",\n",
    "            regexp_extract(col(\"description\"), r\"(\\d+(?:\\.\\d+)?)\", 1).cast(\"float\")\n",
    "        )\n",
    "        .withColumn(\"gradingScaleId\", monotonically_increasing_id().cast(\"int\"))\n",
    "        .withColumn(\"created_at\", current_timestamp())\n",
    "        .withColumn(\"updated_at\", current_timestamp())\n",
    "        .select(\n",
    "            \"gradingScaleId\",\n",
    "            \"value\",\n",
    "            \"description\",\n",
    "            \"created_at\",\n",
    "            \"updated_at\"\n",
    "        )\n",
    ")\n",
    "\n",
    "# 4. Ghi vào bảng Iceberg grading_scale đã tạo trước đó\n",
    "df_grading.writeTo(\"nessie.silver_tables.grading_scale\") \\\n",
    "          .using(\"iceberg\") \\\n",
    "          .createOrReplace()\n",
    "\n",
    "print(f\"Đã ghi {df_grading.count()} dòng vào grading_scale\")\n",
    "\n",
    "# 5. Verify\n",
    "spark.table(\"nessie.silver_tables.grading_scale\").show(truncate=False)\n",
    "\n",
    "# Giải phóng bộ nhớ\n",
    "df_raw.unpersist()\n",
    "df_grading_raw.unpersist()\n",
    "df_grading.unpersist()\n",
    "del df_raw, df_grading_raw, df_grading\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"✓ Đã giải phóng bộ nhớ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036e97a-c555-43a0-a6d4-fb1e673edf0f",
   "metadata": {},
   "source": [
    "## 6. Load Bảng BENCHMARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de0240-da7e-4d00-8870-78bfa419516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, trim, regexp_replace, current_timestamp,\n",
    "    avg, round, expr\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG BENCHMARK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =========================\n",
    "# 1. ĐỌC & CHUẨN HÓA DỮ LIỆU BRONZE\n",
    "# =========================\n",
    "\n",
    "df_benchmark = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"encoding\", \"UTF-8\")\n",
    "    .csv(\"s3a://bronze/structured_data/điểm chuẩn các trường (2021-2025)/Điểm_chuẩn_các_ngành_đại_học_năm(2021-2025)*.csv\")\n",
    ")\n",
    "\n",
    "# Chuẩn hóa cột PhuongThuc: bỏ phần \"năm XXXX ...\"\n",
    "df_benchmark = df_benchmark.withColumn(\n",
    "    \"PhuongThuc_cleaned\",\n",
    "    trim(regexp_replace(col(\"PhuongThuc\"), r\"\\s*năm\\s+\\d{4}.*$\", \"\"))\n",
    ")\n",
    "\n",
    "# Lookup tables từ Silver\n",
    "df_selection_lookup     = spark.table(\"nessie.silver_tables.selection_method\")\n",
    "df_subject_group_lookup = spark.table(\"nessie.silver_tables.subject_group\")\n",
    "df_grading_scale_lookup = spark.table(\"nessie.silver_tables.grading_scale\")\n",
    "\n",
    "# Join lookup + chuẩn hóa\n",
    "df_benchmark_base = (\n",
    "    df_benchmark\n",
    "    .join(\n",
    "        df_selection_lookup,\n",
    "        df_benchmark[\"PhuongThuc_cleaned\"] == df_selection_lookup[\"selectionMethodName\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    .join(\n",
    "        df_subject_group_lookup,\n",
    "        df_benchmark[\"KhoiThi\"] == df_subject_group_lookup[\"subjectGroupName\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    .join(\n",
    "        df_grading_scale_lookup,\n",
    "        trim(df_benchmark[\"PhanLoaiThangDiem\"]) == df_grading_scale_lookup[\"description\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"MaTruong\").cast(\"string\").alias(\"schoolId\"),\n",
    "        col(\"MaNganh\").cast(\"string\").alias(\"majorId\"),\n",
    "        col(\"subjectGroupId\").cast(\"int\"),\n",
    "        col(\"selectionMethodId\").cast(\"int\"),\n",
    "        col(\"gradingScaleId\").cast(\"int\"),\n",
    "        col(\"Nam\").cast(\"int\").alias(\"year\"),\n",
    "        col(\"DiemChuan\").cast(\"double\").alias(\"score\"),\n",
    "    )\n",
    "    .filter(\n",
    "        col(\"schoolId\").isNotNull() &\n",
    "        col(\"majorId\").isNotNull() &\n",
    "        col(\"gradingScaleId\").isNotNull() &\n",
    "        col(\"year\").isNotNull() &\n",
    "        col(\"score\").isNotNull() &\n",
    "        col(\"selectionMethodId\").isNotNull()\n",
    "        # col(\"subjectGroupId\").isNotNull()  # nếu muốn bắt buộc khối thi thì mở dòng này\n",
    "    )\n",
    "    .dropDuplicates([\n",
    "        \"schoolId\",\n",
    "        \"majorId\",\n",
    "        \"subjectGroupId\",\n",
    "        \"selectionMethodId\",\n",
    "        \"year\",\n",
    "        \"gradingScaleId\",\n",
    "        \"score\"\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Giải phóng lookup tables\n",
    "df_selection_lookup.unpersist()\n",
    "df_subject_group_lookup.unpersist()\n",
    "df_grading_scale_lookup.unpersist()\n",
    "del df_selection_lookup, df_subject_group_lookup, df_grading_scale_lookup\n",
    "\n",
    "# =========================\n",
    "# 2. GROUP BY & LẤY ĐIỂM TRUNG BÌNH\n",
    "# =========================\n",
    "\n",
    "df_benchmark_grouped = (\n",
    "    df_benchmark_base\n",
    "    .groupBy(\n",
    "        \"schoolId\",\n",
    "        \"majorId\",\n",
    "        \"subjectGroupId\",\n",
    "        \"selectionMethodId\",\n",
    "        \"gradingScaleId\",\n",
    "        \"year\"\n",
    "    )\n",
    "    .agg(\n",
    "        round(avg(\"score\"), 2).alias(\"score\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Giải phóng df_benchmark_base\n",
    "df_benchmark_base.unpersist()\n",
    "del df_benchmark_base\n",
    "\n",
    "table_name = \"nessie.silver_tables.benchmark\"\n",
    "\n",
    "# =========================\n",
    "# 3. CHECK BẢNG SILVER ĐÃ TỒN TẠI CHƯA\n",
    "# =========================\n",
    "\n",
    "try:\n",
    "    spark.table(table_name)\n",
    "    table_exists = True\n",
    "    print(f\"Bảng {table_name} đã tồn tại → dùng MERGE (upsert).\")\n",
    "except Exception:\n",
    "    table_exists = False\n",
    "    print(f\"Bảng {table_name} chưa tồn tại → tạo mới full-load.\")\n",
    "\n",
    "# =========================\n",
    "# 4. LẦN ĐẦU: TẠO BẢNG FULL (DÙNG xxhash64 LÀM benchmarkId)\n",
    "# =========================\n",
    "\n",
    "if not table_exists:\n",
    "    df_benchmark_silver = (\n",
    "        df_benchmark_grouped\n",
    "        .withColumn(\n",
    "            \"benchmarkId\",\n",
    "            expr(\n",
    "                \"\"\"\n",
    "                CAST(\n",
    "                    xxhash64(\n",
    "                        schoolId,\n",
    "                        majorId,\n",
    "                        COALESCE(subjectGroupId, -1),\n",
    "                        selectionMethodId,\n",
    "                        gradingScaleId,\n",
    "                        year\n",
    "                    ) AS BIGINT\n",
    "                )\n",
    "                \"\"\"\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"created_at\", current_timestamp())\n",
    "        .withColumn(\"updated_at\", current_timestamp())\n",
    "        .select(\n",
    "            \"benchmarkId\",\n",
    "            \"schoolId\",\n",
    "            \"majorId\",\n",
    "            \"subjectGroupId\",\n",
    "            \"selectionMethodId\",\n",
    "            \"gradingScaleId\",\n",
    "            \"year\",\n",
    "            \"score\",\n",
    "            \"created_at\",\n",
    "            \"updated_at\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_benchmark_silver.writeTo(table_name).using(\"iceberg\").createOrReplace()\n",
    "    print(f\"Đã tạo mới benchmark với {df_benchmark_silver.count()} dòng\")\n",
    "    \n",
    "    # Giải phóng\n",
    "    df_benchmark_silver.unpersist()\n",
    "    del df_benchmark_silver\n",
    "\n",
    "# =========================\n",
    "# 5. CÁC LẦN SAU: MERGE / UPSERT\n",
    "# =========================\n",
    "\n",
    "else:\n",
    "    # Staging từ bronze sau khi chuẩn hóa + group\n",
    "    df_staging = (\n",
    "        df_benchmark_grouped\n",
    "        .withColumn(\"created_at\", current_timestamp())\n",
    "        .withColumn(\"updated_at\", current_timestamp())\n",
    "    )\n",
    "\n",
    "    df_staging.createOrReplaceTempView(\"benchmark_staging\")\n",
    "\n",
    "    # MERGE:\n",
    "    # - MATCHED: update score + updated_at\n",
    "    # - NOT MATCHED: insert bản ghi mới với benchmarkId = hash(business key)\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {table_name} AS t\n",
    "        USING benchmark_staging AS s\n",
    "        ON  t.schoolId          = s.schoolId\n",
    "        AND t.majorId           = s.majorId\n",
    "        AND COALESCE(t.subjectGroupId,  -1) = COALESCE(s.subjectGroupId,  -1)\n",
    "        AND t.selectionMethodId = s.selectionMethodId\n",
    "        AND t.gradingScaleId    = s.gradingScaleId\n",
    "        AND t.year              = s.year\n",
    "\n",
    "        WHEN MATCHED THEN UPDATE SET\n",
    "            t.score      = s.score,\n",
    "            t.updated_at = current_timestamp()\n",
    "\n",
    "        WHEN NOT MATCHED THEN INSERT (\n",
    "            benchmarkId,\n",
    "            schoolId,\n",
    "            majorId,\n",
    "            subjectGroupId,\n",
    "            selectionMethodId,\n",
    "            gradingScaleId,\n",
    "            year,\n",
    "            score,\n",
    "            created_at,\n",
    "            updated_at\n",
    "        ) VALUES (\n",
    "            CAST(\n",
    "                xxhash64(\n",
    "                    s.schoolId,\n",
    "                    s.majorId,\n",
    "                    COALESCE(s.subjectGroupId, -1),\n",
    "                    s.selectionMethodId,\n",
    "                    s.gradingScaleId,\n",
    "                    s.year\n",
    "                ) AS BIGINT\n",
    "            ),\n",
    "            s.schoolId,\n",
    "            s.majorId,\n",
    "            s.subjectGroupId,\n",
    "            s.selectionMethodId,\n",
    "            s.gradingScaleId,\n",
    "            s.year,\n",
    "            s.score,\n",
    "            s.created_at,\n",
    "            s.updated_at\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Đã MERGE dữ liệu mới vào bảng benchmark\")\n",
    "    \n",
    "    # Giải phóng\n",
    "    df_staging.unpersist()\n",
    "    del df_staging\n",
    "\n",
    "# =========================\n",
    "# 6. VERIFY\n",
    "# =========================\n",
    "\n",
    "spark.table(table_name).show(5, truncate=False)\n",
    "spark.table(table_name).groupBy(\"year\").count().orderBy(\"year\").show()\n",
    "\n",
    "# Giải phóng bộ nhớ cuối cùng\n",
    "df_benchmark.unpersist()\n",
    "df_benchmark_grouped.unpersist()\n",
    "del df_benchmark, df_benchmark_grouped\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"✓ Đã giải phóng bộ nhớ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18094e01",
   "metadata": {},
   "source": [
    "## 7. Load Bảng REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbdfc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG REGION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_region = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"encoding\", \"UTF-8\").csv(\"s3a://bronze/structured_data/region.csv\")\n",
    "df_region_silver = df_region.select(\n",
    "    lpad(col(df_region.columns[0]).cast(\"string\"), 2, \"0\").alias(\"regionId\"),  # Format thành 2 chữ số: \"1\" -> \"01\"\n",
    "    col(df_region.columns[1]).cast(\"string\").alias(\"regionName\"),\n",
    "    current_timestamp().alias(\"created_at\"),\n",
    "    current_timestamp().alias(\"updated_at\")\n",
    ").filter(col(\"regionId\").isNotNull() & col(\"regionName\").isNotNull()).dropDuplicates([\"regionId\"])\n",
    "\n",
    "df_region_silver.writeTo(\"nessie.silver_tables.region\").using(\"iceberg\").createOrReplace()\n",
    "print(f\"Đã ghi {df_region_silver.count()} dòng vào region\")\n",
    "\n",
    "# Verify\n",
    "spark.table(\"nessie.silver_tables.region\").show(10, truncate=False)\n",
    "\n",
    "# Giải phóng bộ nhớ\n",
    "df_region.unpersist()\n",
    "df_region_silver.unpersist()\n",
    "del df_region, df_region_silver\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"✓ Đã giải phóng bộ nhớ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6217cf",
   "metadata": {},
   "source": [
    "## 8. Load Bảng STUDENT_SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c6fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, trim, regexp_replace, current_timestamp, lit,\n",
    "    concat, substring, udf, input_file_name, regexp_extract\n",
    ")\n",
    "from pyspark.sql.types import MapType, IntegerType, DoubleType\n",
    "from typing import Dict\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOAD BẢNG STUDENT_SCORES - INCREMENTAL BY FILE (DELETE + APPEND)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =====================================================\n",
    "# 0. TẠO BẢNG LOG INGEST (LƯU FILE ĐÃ XỬ LÝ) NẾU CHƯA CÓ\n",
    "# =====================================================\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nessie.silver_tables.student_scores_ingest_log (\n",
    "    path STRING,\n",
    "    year INT,\n",
    "    processed_at TIMESTAMP\n",
    ") USING iceberg\n",
    "\"\"\")\n",
    "\n",
    "# =====================================================\n",
    "# 1. LẤY DANH SÁCH TẤT CẢ FILE CSV HIỆN CÓ TRONG BRONZE\n",
    "#    + TRỪ ĐI NHỮNG FILE ĐÃ INGEST (log)\n",
    "# =====================================================\n",
    "\n",
    "df_files = (\n",
    "    spark.read.format(\"binaryFile\")\n",
    "    .option(\"pathGlobFilter\", \"*.csv\")\n",
    "    .load(\"s3a://bronze/structured_data/điểm từng thí sinh/*/*.csv\")\n",
    "    .select(\"path\")\n",
    ")\n",
    "\n",
    "df_log = spark.table(\"nessie.silver_tables.student_scores_ingest_log\")\n",
    "\n",
    "df_new_files = df_files.join(df_log, on=\"path\", how=\"left_anti\")\n",
    "new_files = [r.path for r in df_new_files.collect()]\n",
    "\n",
    "if not new_files:\n",
    "    print(\" Không có file mới nào, dừng job.\")\n",
    "else:\n",
    "    print(f\" Phát hiện {len(new_files)} file mới cần xử lý.\")\n",
    "\n",
    "    # =====================================================\n",
    "    # 2. ĐỌC CHỈ CÁC FILE MỚI + THÊM CỘT YEAR\n",
    "    # =====================================================\n",
    "\n",
    "    df_scores_raw = (\n",
    "        spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"false\")\n",
    "        .option(\"encoding\", \"UTF-8\")\n",
    "        .csv(new_files)\n",
    "        .withColumn(\"path\", input_file_name())\n",
    "    )\n",
    "\n",
    "    df_scores_raw = df_scores_raw.withColumn(\n",
    "        \"Year\",\n",
    "        regexp_extract(col(\"path\"), r\"/(\\d{4})/\", 1).cast(\"int\")\n",
    "    )\n",
    "\n",
    "    # =====================================================\n",
    "    # 3. LOAD LOOKUP MÔN HỌC\n",
    "    # =====================================================\n",
    "\n",
    "    df_subject_lookup = spark.table(\"nessie.silver_tables.subject\").select(\"subjectId\", \"subjectName\")\n",
    "    subject_map = {row.subjectName: row.subjectId for row in df_subject_lookup.collect()}\n",
    "    print(f\"\\nĐã load {len(subject_map)} môn học để mapping\")\n",
    "\n",
    "    # =====================================================\n",
    "    # 4. UDF PARSE ĐIỂM → Map<subjectId, score>\n",
    "    # =====================================================\n",
    "\n",
    "    def parse_scores_with_subject_id(score_string: str) -> Dict[int, float]:\n",
    "        if not score_string or score_string.strip() == \"\":\n",
    "            return {}\n",
    "        scores_dict = {}\n",
    "        try:\n",
    "            pairs = score_string.split(\",\")\n",
    "            for pair in pairs:\n",
    "                if \":\" in pair:\n",
    "                    subject_name, score = pair.split(\":\")\n",
    "                    subject_name = subject_name.strip()\n",
    "                    # Map tên môn -> subjectId\n",
    "                    if subject_name in subject_map:\n",
    "                        subject_id = subject_map[subject_name]\n",
    "                        try:\n",
    "                            scores_dict[subject_id] = float(score.strip())\n",
    "                        except:\n",
    "                            pass\n",
    "        except:\n",
    "            pass\n",
    "        return scores_dict\n",
    "\n",
    "    parse_scores_udf = udf(parse_scores_with_subject_id, MapType(IntegerType(), DoubleType()))\n",
    "\n",
    "    # =====================================================\n",
    "    # 5. TRANSFORM → DATAFRAME STAGING (KHÔNG MERGE)\n",
    "    # =====================================================\n",
    "\n",
    "    # 1️ Biến đầy đủ để append vào silver\n",
    "    df_student_scores_stage = (\n",
    "        df_scores_raw\n",
    "        .withColumn(\"studentId\", concat(col(\"SBD\"), col(\"Year\").cast(\"string\")))\n",
    "        .withColumn(\"scores\", parse_scores_udf(col(\"DiemThi\")))   # UDF ở đây\n",
    "        .withColumn(\"regionId\", substring(col(\"SBD\"), 1, 2).cast(\"string\"))\n",
    "        .select(\n",
    "            col(\"studentId\").cast(\"string\"),\n",
    "            col(\"regionId\").cast(\"string\"),\n",
    "            col(\"Year\").cast(\"int\").alias(\"year\"),\n",
    "            col(\"scores\")\n",
    "        )\n",
    "        .filter(\n",
    "            col(\"studentId\").isNotNull() &\n",
    "            col(\"year\").isNotNull() &\n",
    "            col(\"scores\").isNotNull()\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 2️ Biến thứ hai chỉ có studentId — KHÔNG UDF → dùng để DELETE\n",
    "    df_student_ids = (\n",
    "        df_scores_raw\n",
    "        .withColumn(\"studentId\", concat(col(\"SBD\"), col(\"Year\").cast(\"string\")))\n",
    "        .select(\"studentId\")\n",
    "        .filter(col(\"studentId\").isNotNull())\n",
    "        # .dropDuplicates([\"studentId\"])\n",
    "    )\n",
    "    \n",
    "    df_student_ids.createOrReplaceTempView(\"student_scores_new_ids\")\n",
    "\n",
    "\n",
    "    staging_count = df_student_scores_stage.count()\n",
    "    print(f\"Staging có {staging_count:,} dòng.\")\n",
    "\n",
    "    table_name = \"nessie.silver_tables.student_scores\"\n",
    "\n",
    "        # =====================================================\n",
    "    # 6. XOÁ studentId CŨ BẰNG CÁCH COLLECT RA PYTHON + DELETE IN (...)\n",
    "    # =====================================================\n",
    "\n",
    "    # Lấy list studentId distinct trong batch mới\n",
    "    new_ids = [\n",
    "        row.studentId\n",
    "        for row in df_student_scores_stage.select(\"studentId\").distinct().collect()\n",
    "    ]\n",
    "\n",
    "    print(f\"Số studentId distinct trong batch mới: {len(new_ids):,}\")\n",
    "\n",
    "    # Kiểm tra bảng silver đã tồn tại chưa\n",
    "    try:\n",
    "        spark.table(table_name)\n",
    "        table_exists = True\n",
    "        print(f\"Bảng {table_name} đã tồn tại → DELETE theo list studentId + APPEND.\")\n",
    "    except Exception:\n",
    "        table_exists = False\n",
    "        print(f\"Bảng {table_name} chưa tồn tại → tạo mới từ batch, không cần xoá.\")\n",
    "\n",
    "    silver_count = spark.table(table_name).count() if table_exists else 0\n",
    "    print(f\"Số dòng trong bảng silver hiện tại: {silver_count:,}\")\n",
    "\n",
    "    if not table_exists:\n",
    "        # 1️ BẢNG CHƯA TỒN TẠI → TẠO MỚI\n",
    "        (\n",
    "            df_student_scores_stage\n",
    "            .withColumn(\"created_at\", current_timestamp())\n",
    "            .withColumn(\"updated_at\", current_timestamp())\n",
    "            .writeTo(table_name)\n",
    "            .using(\"iceberg\")\n",
    "            .createOrReplace()\n",
    "        )\n",
    "        print(f\" Đã tạo mới bảng {table_name} với {staging_count:,} dòng.\")\n",
    "    \n",
    "    elif silver_count == 0:\n",
    "        # 2️ BẢNG TỒN TẠI NHƯNG RỖNG → KHÔNG XOÁ, CHỈ APPEND\n",
    "        print(\" Bảng silver đã tồn tại nhưng rỗng → chỉ append, không xoá.\")\n",
    "    \n",
    "        (\n",
    "            df_student_scores_stage\n",
    "            .withColumn(\"created_at\", current_timestamp())\n",
    "            .withColumn(\"updated_at\", current_timestamp())\n",
    "            .writeTo(table_name)\n",
    "            .using(\"iceberg\")\n",
    "            .append()\n",
    "        )\n",
    "        print(f\" Đã append {staging_count:,} dòng mới vào {table_name}.\")\n",
    "    \n",
    "    elif new_ids:\n",
    "        # 3️ BẢNG TỒN TẠI VÀ new_ids KHÔNG RỖNG → DELETE + APPEND\n",
    "        print(\"Bảng silver có dữ liệu → DELETE + APPEND.\")\n",
    "    \n",
    "        chunk_size = 500\n",
    "        from math import ceil\n",
    "    \n",
    "        num_chunks = ceil(len(new_ids) / chunk_size)\n",
    "        print(f\"Chia studentId thành {num_chunks} chunk để xoá...\")\n",
    "    \n",
    "        for i in range(num_chunks):\n",
    "            chunk = new_ids[i * chunk_size:(i + 1) * chunk_size]\n",
    "            escaped_ids = [sid.replace(\"'\", \"''\") for sid in chunk]\n",
    "            in_list = \",\".join([f\"'{sid}'\" for sid in escaped_ids])\n",
    "    \n",
    "            sql_delete = f\"\"\"\n",
    "                DELETE FROM {table_name}\n",
    "                WHERE studentId IN ({in_list})\n",
    "            \"\"\"\n",
    "            spark.sql(sql_delete)\n",
    "    \n",
    "        print(\" Đã xoá xong các studentId cũ trong silver.\")\n",
    "    \n",
    "        (\n",
    "            df_student_scores_stage\n",
    "            .withColumn(\"created_at\", current_timestamp())\n",
    "            .withColumn(\"updated_at\", current_timestamp())\n",
    "            .writeTo(table_name)\n",
    "            .using(\"iceberg\")\n",
    "            .append()\n",
    "        )\n",
    "        print(f\" Đã append {staging_count:,} dòng mới.\")\n",
    "    \n",
    "    else:\n",
    "        # 4️ new_ids rỗng → không xoá, không append\n",
    "        print(\" Batch mới không có studentId nào hợp lệ → không làm gì cả.\")\n",
    "\n",
    "\n",
    "    # =====================================================\n",
    "    # 7. GHI LOG FILE ĐÃ XỬ LÝ\n",
    "    # =====================================================\n",
    "\n",
    "    from pyspark.sql.functions import array, explode\n",
    "\n",
    "    df_new_files_log = (\n",
    "        df_new_files\n",
    "        .withColumn(\"year\", regexp_extract(col(\"path\"), r\"/(\\d{4})/\", 1).cast(\"int\"))\n",
    "        .withColumn(\"processed_at\", current_timestamp())\n",
    "    )\n",
    "\n",
    "    (\n",
    "        df_new_files_log\n",
    "        .writeTo(\"nessie.silver_tables.student_scores_ingest_log\")\n",
    "        .using(\"iceberg\")\n",
    "        .append()\n",
    "    )\n",
    "\n",
    "    print(f\"Đã ghi log {df_new_files_log.count():,} file đã xử lý.\")\n",
    "\n",
    "    # =====================================================\n",
    "    # 8. VERIFY\n",
    "    # =====================================================\n",
    "\n",
    "    print(\"\\nMẫu dữ liệu student_scores:\")\n",
    "    spark.table(table_name).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce95322d",
   "metadata": {},
   "source": [
    "## 9. Load Bảng ARTICLE và COMMENT từ TikTok Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0864e1-b0e6-4467-a209-b2e45e708e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, when, coalesce, trim,\n",
    "    to_timestamp, current_timestamp,\n",
    "    input_file_name, regexp_replace\n",
    ")\n",
    "\n",
    "# ====================================================\n",
    "# CẤU HÌNH\n",
    "# ====================================================\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"33554432\")\n",
    "\n",
    "POSTS_PATH = \"s3a://bronze/MangXaHoi/tiktok-data/posts/*.csv\"\n",
    "TABLE_ARTICLE = \"nessie.silver_tables.article\"\n",
    "TABLE_LOG = \"nessie.silver_tables.tiktok_posts_files_log\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"JOB 1: LOAD TIKTOK POSTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Check file moi\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {TABLE_LOG} (file_path STRING, load_time TIMESTAMP) USING iceberg\")\n",
    "\n",
    "df_all = spark.read.format(\"binaryFile\").option(\"pathGlobFilter\", \"*.csv\").load(POSTS_PATH).select(\"path\")\n",
    "try:\n",
    "    df_processed = spark.table(TABLE_LOG).select(\"file_path\")\n",
    "    df_new_files = df_all.join(df_processed, df_all.path == col(\"file_path\"), \"left_anti\")\n",
    "except:\n",
    "    df_new_files = df_all\n",
    "\n",
    "new_files = [r.path for r in df_new_files.collect()]\n",
    "\n",
    "if not new_files:\n",
    "    print(\"Khong co file Post moi.\")\n",
    "else:\n",
    "    print(f\"Xu ly {len(new_files)} file Post moi.\")\n",
    "\n",
    "    # 2. Doc & Transform\n",
    "    df_raw = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"false\").csv(new_files)\n",
    "\n",
    "    df_trans = (\n",
    "        df_raw\n",
    "        .withColumn(\"timePublish\", \n",
    "            coalesce(\n",
    "                to_timestamp(col(\"TimePublish\"), \"dd-MM-yyyy\"),\n",
    "                to_timestamp(col(\"TimePublish\"), \"d-M-yyyy\"), \n",
    "                to_timestamp(regexp_replace(col(\"TimePublish\"), r\".*(\\d{1,2})\\s+Tháng\\s+(\\d{1,2}),\\s+(\\d{4}).*\", \"$1-$2-$3\"), \"d-M-yyyy\"),\n",
    "                current_timestamp()\n",
    "            ))\n",
    "        .withColumn(\"likeCount\", \n",
    "            when(col(\"Like\").contains(\"K\"), (regexp_replace(col(\"Like\"), \"K\", \"\").cast(\"float\")*1000).cast(\"int\"))\n",
    "            .when(col(\"Like\").contains(\"M\"), (regexp_replace(col(\"Like\"), \"M\", \"\").cast(\"float\")*1000000).cast(\"int\"))\n",
    "            .otherwise(coalesce(col(\"Like\").cast(\"int\"), lit(0))))\n",
    "        .withColumn(\"commentCount\", \n",
    "            when(col(\"Comment\").contains(\"K\"), (regexp_replace(col(\"Comment\"), \"K\", \"\").cast(\"float\")*1000).cast(\"int\"))\n",
    "            .when(col(\"Comment\").contains(\"M\"), (regexp_replace(col(\"Comment\"), \"M\", \"\").cast(\"float\")*1000000).cast(\"int\"))\n",
    "            .otherwise(coalesce(col(\"Comment\").cast(\"int\"), lit(0))))\n",
    "        .withColumn(\"shareCount\", \n",
    "            when(col(\"Share\").contains(\"K\"), (regexp_replace(col(\"Share\"), \"K\", \"\").cast(\"float\")*1000).cast(\"int\"))\n",
    "            .when(col(\"Share\").contains(\"M\"), (regexp_replace(col(\"Share\"), \"M\", \"\").cast(\"float\")*1000000).cast(\"int\"))\n",
    "            .otherwise(coalesce(col(\"Share\").cast(\"int\"), lit(0))))\n",
    "        .select(\n",
    "            trim(col(\"ID\")).alias(\"articleID\"),  # ID TikTok -> articleID\n",
    "            col(\"Description\").alias(\"description\"), \n",
    "            col(\"Author\").alias(\"author\"),\n",
    "            col(\"Url\").alias(\"url\"),\n",
    "            col(\"timePublish\"),\n",
    "            col(\"likeCount\"), col(\"commentCount\"), col(\"shareCount\"),\n",
    "            lit(\"TikTok\").alias(\"type\"),\n",
    "            current_timestamp().alias(\"created_at\"),\n",
    "            current_timestamp().alias(\"updated_at\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 3. Ghi du lieu (Append)\n",
    "    print(\"Dang ghi vao Iceberg...\")\n",
    "    df_trans.writeTo(TABLE_ARTICLE).using(\"iceberg\").append()\n",
    "    \n",
    "    # 4. Ghi Log\n",
    "    spark.createDataFrame([(f,) for f in new_files], [\"file_path\"]) \\\n",
    "          .withColumn(\"load_time\", current_timestamp()) \\\n",
    "          .writeTo(TABLE_LOG).using(\"iceberg\").append()\n",
    "    \n",
    "    print(\"Hoan tat.\")\n",
    "    \n",
    "    # Giải phóng bộ nhớ\n",
    "    df_all.unpersist()\n",
    "    df_new_files.unpersist()\n",
    "    df_raw.unpersist()\n",
    "    df_trans.unpersist()\n",
    "    del df_all, df_new_files, df_raw, df_trans\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"✓ Đã giải phóng bộ nhớ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd9115d-688e-4eaa-a6ae-0c964c7e45d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, LongType\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, when, coalesce, trim,\n",
    "    to_timestamp, current_timestamp,\n",
    "    input_file_name, regexp_replace,\n",
    "    row_number, monotonically_increasing_id\n",
    ")\n",
    "\n",
    "# ====================================================\n",
    "# CẤU HÌNH LOW RESOURCE\n",
    "# ====================================================\n",
    "# spark = SparkSession.builder... (Giả sử session đã có)\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"16777216\") # 16MB\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"20\")\n",
    "\n",
    "# Path & Table\n",
    "POSTS_PATH = \"s3a://bronze/MangXaHoi/tiktok-data/comments/*.csv\"\n",
    "TABLE_COMMENT = \"nessie.silver_tables.comment\"\n",
    "TABLE_LOG = \"nessie.silver_tables.tiktok_comments_files_log\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"JOB 2: LOAD TIKTOK COMMENTS (FULL RUN - NO BATCH LIMIT)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ====================================================\n",
    "# 1. CHUẨN BỊ & LỌC FILE\n",
    "# ====================================================\n",
    "\n",
    "# Tạo bảng Log nếu chưa có\n",
    "try:\n",
    "    spark.table(TABLE_LOG)\n",
    "except:\n",
    "    print(f\"-> Bang log {TABLE_LOG} chua ton tai. Dang tao moi...\")\n",
    "    log_schema = StructType([StructField(\"file_path\", StringType(), False), StructField(\"load_time\", TimestampType(), False)])\n",
    "    spark.createDataFrame([], log_schema).writeTo(TABLE_LOG).using(\"iceberg\").create()\n",
    "\n",
    "# Quét file nguồn\n",
    "print(\"-> Dang quet file nguon...\")\n",
    "df_all = spark.read.format(\"binaryFile\").option(\"pathGlobFilter\", \"*.csv\").load(POSTS_PATH).select(\"path\")\n",
    "total_files_count = df_all.count()\n",
    "print(f\"   Tong so file trong folder: {total_files_count}\")\n",
    "\n",
    "# Lọc file mới (Anti Join với Log)\n",
    "try:\n",
    "    df_processed = spark.table(TABLE_LOG).select(\"file_path\").distinct()\n",
    "    processed_count = df_processed.count()\n",
    "    print(f\"   So file da xu ly truoc do: {processed_count}\")\n",
    "    \n",
    "    df_new_files = df_all.alias(\"src\").join(\n",
    "        df_processed.alias(\"log\"), \n",
    "        col(\"src.path\") == col(\"log.file_path\"), \n",
    "        \"left_anti\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"   [WARNING] Loi doc Log: {e}\")\n",
    "    df_new_files = df_all\n",
    "\n",
    "# Lấy toàn bộ danh sách file cần xử lý\n",
    "# [THAY ĐỔI]: Lấy hết, không cắt batch nữa\n",
    "files_to_process = [r.path for r in df_new_files.collect()]\n",
    "\n",
    "if not files_to_process:\n",
    "    print(\"-> KHONG CO FILE COMMENT MOI.\")\n",
    "else:\n",
    "    print(f\"-> Tim thay {len(files_to_process)} file moi. Se xu ly TOAN BO.\")\n",
    "\n",
    "    # ====================================================\n",
    "    # 2. XỬ LÝ TUẦN TỰ (LOOP)\n",
    "    # ====================================================\n",
    "    \n",
    "    # Lấy max commentID hiện tại\n",
    "    try:\n",
    "        max_id_row = spark.sql(f\"SELECT COALESCE(MAX(commentID), 0) as max_id FROM {TABLE_COMMENT}\").collect()\n",
    "        max_id = max_id_row[0][\"max_id\"] if max_id_row else 0\n",
    "        print(f\"-> Max commentID hien tai: {max_id:,}\")\n",
    "    except:\n",
    "        max_id = 0\n",
    "        print(\"-> Bang comment chua co du lieu, bat dau tu ID = 0\")\n",
    "    \n",
    "    # Duyệt qua từng file\n",
    "    for i, file_path in enumerate(files_to_process):\n",
    "        filename = file_path.split('/')[-1]\n",
    "        print(f\"\\n--- [{i+1}/{len(files_to_process)}] Dang xu ly: {filename} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Đọc CSV\n",
    "            df_raw = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"false\").csv(file_path)\n",
    "\n",
    "            if \"ID_Post\" not in df_raw.columns:\n",
    "                print(f\"   [SKIP] File loi format (Thieu ID_Post).\")\n",
    "                # Ghi log bỏ qua để lần sau không đọc lại\n",
    "                spark.createDataFrame([(file_path,)], [\"file_path\"]).withColumn(\"load_time\", current_timestamp()).writeTo(TABLE_LOG).using(\"iceberg\").append()\n",
    "                continue\n",
    "\n",
    "            # Transform dữ liệu\n",
    "            df_trans = df_raw.select(\n",
    "                trim(col(\"ID_Post\")).alias(\"articleID\"), \n",
    "                col(\"Name\").alias(\"name\"),\n",
    "                col(\"TagName\").alias(\"tagName\"),\n",
    "                col(\"URL\").alias(\"urlUser\"),\n",
    "                col(\"Comment\").alias(\"comment\"),\n",
    "                coalesce(\n",
    "                    to_timestamp(regexp_replace(col(\"Time\"), r\"(\\d{1,2})[-/](\\d{1,2})[-/](\\d{4}).*\", \"$1-$2-$3\"), \"d-M-yyyy\"),\n",
    "                    to_timestamp(regexp_replace(col(\"Time\"), r\".*trước.*\", \"1970-01-01\"), \"yyyy-MM-dd\"),\n",
    "                    current_timestamp()\n",
    "                ).alias(\"commentTime\"),\n",
    "                coalesce(col(\"Likes\").cast(\"int\"), lit(0)).alias(\"commentLike\"),\n",
    "                when(col(\"LevelComment\") == \"Yes\", 2).otherwise(1).alias(\"levelComment\"),\n",
    "                col(\"RepliedTo\").alias(\"replyTo\"),\n",
    "                coalesce(col(\"NumberOfReplies\").cast(\"int\"), lit(0)).alias(\"numberOfReply\"),\n",
    "                current_timestamp().alias(\"created_at\"),\n",
    "                current_timestamp().alias(\"updated_at\")\n",
    "            ).filter(col(\"articleID\").isNotNull() & (col(\"articleID\") != \"\"))\n",
    "            \n",
    "            # Tạo commentID nối tiếp\n",
    "            window_spec = Window.orderBy(lit(1))\n",
    "            df_trans = df_trans.withColumn(\"commentID\", (row_number().over(window_spec) + max_id).cast(\"bigint\"))\n",
    "            \n",
    "            # Cập nhật max_id cho vòng lặp sau\n",
    "            current_count = df_trans.count() # Action này trigger tính toán\n",
    "            max_id += current_count\n",
    "            print(f\"   -> File nay co {current_count:,} comment. Max ID ke tiep: {max_id:,}\")\n",
    "            \n",
    "            if current_count > 0:\n",
    "                # Ghi vào Iceberg\n",
    "                print(\"   -> Dang ghi vao Iceberg...\")\n",
    "                # Repartition để tránh tạo quá nhiều file nhỏ trong Iceberg\n",
    "                df_trans.repartition(20).writeTo(TABLE_COMMENT).using(\"iceberg\").append()\n",
    "                print(\"   -> Ghi xong.\")\n",
    "            \n",
    "            # Ghi Log đã xử lý xong file này\n",
    "            spark.createDataFrame([(file_path,)], [\"file_path\"]).withColumn(\"load_time\", current_timestamp()).writeTo(TABLE_LOG).using(\"iceberg\").append()\n",
    "            \n",
    "            # Dọn dẹp RAM\n",
    "            df_trans.unpersist()\n",
    "            del df_trans\n",
    "            del df_raw\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   [ERROR] Loi xu ly file {filename}: {e}\")\n",
    "            # Nếu lỗi file này thì bỏ qua, chạy file kế tiếp\n",
    "            continue\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DA HOAN TAT TAT CA FILE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6cac65",
   "metadata": {},
   "source": [
    "## 10. Load Bảng ARTICLE và COMMENT từ Facebook Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dce657-6e8e-423f-ae1b-26a1dca0041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import (\n",
    "    col, trim, to_timestamp, lit, current_timestamp, \n",
    "    input_file_name, coalesce, udf\n",
    ")\n",
    "\n",
    "# ====================================================\n",
    "# PARSE TIME UDF\n",
    "# ====================================================\n",
    "MONTH_MAP = {\"Tháng 1\": \"01\", \"Tháng 2\": \"02\", \"Tháng 3\": \"03\", \"Tháng 4\": \"04\", \"Tháng 5\": \"05\", \"Tháng 6\": \"06\", \"Tháng 7\": \"07\", \"Tháng 8\": \"08\", \"Tháng 9\": \"09\", \"Tháng 10\": \"10\", \"Tháng 11\": \"11\", \"Tháng 12\": \"12\"}\n",
    "\n",
    "def parse_vietnam_datetime(dt_str):\n",
    "    if not dt_str: return None\n",
    "    try:\n",
    "        if \",\" in dt_str: dt_str = dt_str.split(\",\", 1)[1].strip()\n",
    "        match = re.search(r\"(\\d+)\\s+(Tháng\\s+\\d+)\", dt_str)\n",
    "        if not match: return None\n",
    "        day, month_text = match.group(1), match.group(2)\n",
    "        month = MONTH_MAP.get(month_text, \"01\")\n",
    "        year = re.search(r\",\\s*(\\d{4})\", dt_str).group(1) if re.search(r\",\\s*(\\d{4})\", dt_str) else \"2025\"\n",
    "        time_str = re.search(r\"lúc\\s+(\\d{1,2}:\\d{2})\", dt_str).group(1) if re.search(r\"lúc\\s+(\\d{1,2}:\\d{2})\", dt_str) else \"00:00\"\n",
    "        return f\"{year}-{month}-{int(day):02d} {time_str}:00\"\n",
    "    except: return None\n",
    "\n",
    "parse_vn_time_udf = udf(parse_vietnam_datetime, StringType())\n",
    "\n",
    "# ====================================================\n",
    "# CẤU HÌNH\n",
    "# ====================================================\n",
    "POSTS_GLOB = \"s3a://bronze/MangXaHoi/Face-data/posts/*.csv\"\n",
    "TABLE_ARTICLE = \"nessie.silver_tables.article\"\n",
    "TABLE_LOG = \"nessie.silver_tables.fb_posts_files_log\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"JOB 3: LOAD FACEBOOK POSTS (MERGE/UPSERT)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {TABLE_LOG} (file_path STRING, load_time TIMESTAMP) USING iceberg\")\n",
    "\n",
    "# 1. Loc file moi\n",
    "df_all = spark.read.option(\"header\", \"true\").csv(POSTS_GLOB).withColumn(\"file_path\", input_file_name())\n",
    "try:\n",
    "    df_proc = spark.table(TABLE_LOG).select(\"file_path\").distinct()\n",
    "    new_files = [r.file_path for r in df_all.join(df_proc, \"file_path\", \"left_anti\").select(\"file_path\").distinct().collect()]\n",
    "except:\n",
    "    new_files = [r.file_path for r in df_all.select(\"file_path\").distinct().collect()]\n",
    "\n",
    "if new_files:\n",
    "    print(f\"Xu ly {len(new_files)} file moi.\")\n",
    "    \n",
    "    # 2. Transform\n",
    "    df_raw = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(new_files)\n",
    "    df_trans = df_raw.select(\n",
    "        trim(col(\"ID\")).alias(\"articleID\"),         # ID FB -> articleID\n",
    "        trim(col(\"Description\")).alias(\"description\"),\n",
    "        trim(col(\"Author\")).alias(\"author\"),\n",
    "        trim(col(\"Url\")).alias(\"url\"),\n",
    "        coalesce(to_timestamp(parse_vn_time_udf(col(\"TimePublish\"))), to_timestamp(col(\"TimePublish\")), current_timestamp()).alias(\"timePublish\"),\n",
    "        coalesce(col(\"Like\").cast(\"int\"), lit(0)).alias(\"likeCount\"),\n",
    "        coalesce(col(\"Share\").cast(\"int\"), lit(0)).alias(\"shareCount\"),\n",
    "        coalesce(col(\"Comment\").cast(\"int\"), lit(0)).alias(\"commentCount\")\n",
    "    )\n",
    "\n",
    "    # 3. MERGE (Upsert)\n",
    "    df_trans.createOrReplaceTempView(\"fb_source\")\n",
    "    \n",
    "    # Update metrics\n",
    "    spark.sql(f\"\"\"\n",
    "    MERGE INTO {TABLE_ARTICLE} t USING fb_source s\n",
    "    ON t.url = s.url AND t.type = 'facebook'\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        t.description = s.description, t.timePublish = s.timePublish,\n",
    "        t.likeCount = s.likeCount, t.shareCount = s.shareCount, t.commentCount = s.commentCount,\n",
    "        t.updated_at = current_timestamp()\n",
    "    \"\"\")\n",
    "\n",
    "    # Insert new\n",
    "    spark.sql(f\"\"\"\n",
    "    INSERT INTO {TABLE_ARTICLE}\n",
    "    SELECT s.articleID, s.description, s.author, s.url, s.timePublish,\n",
    "           s.likeCount, s.commentCount, s.shareCount, 'facebook', current_timestamp(), current_timestamp()\n",
    "    FROM fb_source s\n",
    "    WHERE NOT EXISTS (SELECT 1 FROM {TABLE_ARTICLE} t WHERE t.url = s.url AND t.type = 'facebook')\n",
    "    \"\"\")\n",
    "    \n",
    "    # 4. Log\n",
    "    spark.createDataFrame([(f,) for f in new_files], [\"file_path\"]).withColumn(\"load_time\", current_timestamp()).writeTo(TABLE_LOG).using(\"iceberg\").append()\n",
    "    print(\"Hoan tat.\")\n",
    "    \n",
    "    # Giải phóng bộ nhớ\n",
    "    df_all.unpersist()\n",
    "    df_raw.unpersist()\n",
    "    df_trans.unpersist()\n",
    "    del df_all, df_raw, df_trans\n",
    "    if 'df_proc' in locals():\n",
    "        df_proc.unpersist()\n",
    "        del df_proc\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"✓ Đã giải phóng bộ nhớ\")\n",
    "else:\n",
    "    print(\"Khong co file moi.\")\n",
    "    # Giải phóng df_all nếu không có file mới\n",
    "    df_all.unpersist()\n",
    "    del df_all\n",
    "    import gc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9611c5c2-66d6-44cb-a24e-eac348c1d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, LongType\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, when, coalesce, trim, broadcast,\n",
    "    to_timestamp, current_timestamp,\n",
    "    input_file_name, regexp_replace, row_number\n",
    ")\n",
    "\n",
    "# ====================================================\n",
    "# CẤU HÌNH LOW RESOURCE\n",
    "# ====================================================\n",
    "# spark = SparkSession.builder... (Giả sử session đã có)\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"33554432\") # 32MB\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "\n",
    "# Path & Table\n",
    "COMMENTS_GLOB = \"s3a://bronze/MangXaHoi/Face-data/comments/*.csv\"\n",
    "TABLE_COMMENT = \"nessie.silver_tables.comment\"\n",
    "TABLE_LOG = \"nessie.silver_tables.fb_comments_files_log\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"JOB 4: LOAD FACEBOOK COMMENTS (FULL RUN - NO BATCH LIMIT)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ====================================================\n",
    "# 1. CHUẨN BỊ & LỌC FILE\n",
    "# ====================================================\n",
    "\n",
    "# Tạo bảng Log\n",
    "try:\n",
    "    spark.table(TABLE_LOG)\n",
    "except:\n",
    "    print(f\"-> Bang log {TABLE_LOG} chua ton tai. Dang tao moi...\")\n",
    "    log_schema = StructType([StructField(\"file_path\", StringType(), False), StructField(\"load_time\", TimestampType(), False)])\n",
    "    spark.createDataFrame([], log_schema).writeTo(TABLE_LOG).using(\"iceberg\").create()\n",
    "\n",
    "# Quét file nguồn\n",
    "print(\"-> Dang quet file nguon...\")\n",
    "df_all = spark.read.format(\"binaryFile\").option(\"pathGlobFilter\", \"*.csv\").load(COMMENTS_GLOB).select(\"path\")\n",
    "total_files_count = df_all.count()\n",
    "print(f\"   Tong so file trong folder: {total_files_count}\")\n",
    "\n",
    "# Lọc file mới (Anti Join)\n",
    "try:\n",
    "    df_processed = spark.table(TABLE_LOG).select(\"file_path\").distinct()\n",
    "    processed_count = df_processed.count()\n",
    "    print(f\"   So file da xu ly truoc do: {processed_count}\")\n",
    "    \n",
    "    df_new_files = df_all.alias(\"src\").join(\n",
    "        df_processed.alias(\"log\"), \n",
    "        col(\"src.path\") == col(\"log.file_path\"), \n",
    "        \"left_anti\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"   [WARNING] Loi doc Log: {e}\")\n",
    "    df_new_files = df_all\n",
    "\n",
    "# Lấy toàn bộ danh sách file\n",
    "files_to_process = [r.path for r in df_new_files.collect()]\n",
    "\n",
    "if not files_to_process:\n",
    "    print(\"-> KHONG CO FILE MOI.\")\n",
    "else:\n",
    "    print(f\"-> Tim thay {len(files_to_process)} file moi. Se xu ly TOAN BO.\")\n",
    "\n",
    "    # ====================================================\n",
    "    # 2. XỬ LÝ TUẦN TỰ (LOOP)\n",
    "    # ====================================================\n",
    "\n",
    "    # Lấy max commentID hiện tại\n",
    "    try:\n",
    "        max_id_row = spark.sql(f\"SELECT COALESCE(MAX(commentID), 0) as max_id FROM {TABLE_COMMENT}\").collect()\n",
    "        max_id = max_id_row[0][\"max_id\"] if max_id_row else 0\n",
    "        print(f\"-> Max commentID hien tai: {max_id:,}\")\n",
    "    except:\n",
    "        max_id = 0\n",
    "        print(\"-> Bang comment chua co du lieu, bat dau tu ID = 0\")\n",
    "\n",
    "    # Duyệt từng file\n",
    "    for i, file_path in enumerate(files_to_process):\n",
    "        filename = file_path.split('/')[-1]\n",
    "        print(f\"\\n--- [{i+1}/{len(files_to_process)}] Dang xu ly: {filename} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Đọc CSV\n",
    "            df_raw = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(file_path)\n",
    "            \n",
    "            # Kiểm tra cột bắt buộc\n",
    "            if \"Id_post\" not in df_raw.columns:\n",
    "                print(f\"   [SKIP] File thieu cot 'Id_post'.\")\n",
    "                spark.createDataFrame([(file_path,)], [\"file_path\"]).withColumn(\"load_time\", current_timestamp()).writeTo(TABLE_LOG).using(\"iceberg\").append()\n",
    "                continue\n",
    "\n",
    "            # Transform (Mapping cho Facebook - Set NULL nhiều cột thiếu)\n",
    "            df_trans = df_raw.select(\n",
    "                trim(col(\"Id_post\")).alias(\"articleID\"), \n",
    "                lit(None).cast(\"string\").alias(\"name\"),\n",
    "                lit(None).cast(\"string\").alias(\"tagName\"),\n",
    "                lit(None).cast(\"string\").alias(\"urlUser\"),\n",
    "                (col(\"Comment\") if \"Comment\" in df_raw.columns else lit(\"\")).alias(\"comment\"),\n",
    "                lit(None).cast(\"timestamp\").alias(\"commentTime\"),\n",
    "                lit(0).alias(\"commentLike\"), # Default 0 thay vì Null để tránh lỗi tính toán sau này\n",
    "                lit(1).alias(\"levelComment\"), # Default level 1\n",
    "                lit(None).cast(\"string\").alias(\"replyTo\"),\n",
    "                lit(0).alias(\"numberOfReply\"),\n",
    "                current_timestamp().alias(\"created_at\"),\n",
    "                current_timestamp().alias(\"updated_at\")\n",
    "            ).filter(col(\"articleID\").isNotNull() & (col(\"articleID\") != \"\"))\n",
    "            \n",
    "            # Tạo commentID nối tiếp\n",
    "            window_spec = Window.orderBy(lit(1))\n",
    "            df_trans = df_trans.withColumn(\"commentID\", (row_number().over(window_spec) + max_id).cast(\"bigint\"))\n",
    "            \n",
    "            # Cập nhật max_id\n",
    "            current_count = df_trans.count()\n",
    "            max_id += current_count\n",
    "            print(f\"   -> File co {current_count:,} dong. Max ID ke tiep: {max_id:,}\")\n",
    "\n",
    "            if current_count > 0:\n",
    "                print(f\"   -> Dang ghi vao Iceberg...\")\n",
    "                # Repartition(1) vì file FB thường nhỏ và rời rạc, gom lại để tránh file nhỏ trên HDFS/S3\n",
    "                df_trans.select(\n",
    "                    \"commentID\", \"articleID\", \"name\", \"tagName\", \"urlUser\", \"comment\",\n",
    "                    \"commentTime\", \"commentLike\", \"levelComment\", \"replyTo\", \"numberOfReply\",\n",
    "                    \"created_at\", \"updated_at\"\n",
    "                ).repartition(1).writeTo(TABLE_COMMENT).using(\"iceberg\").append()\n",
    "                print(\"   -> Ghi xong.\")\n",
    "            else:\n",
    "                print(\"   -> File rong hoac khong co ID hop le.\")\n",
    "\n",
    "            # Ghi Log\n",
    "            spark.createDataFrame([(file_path,)], [\"file_path\"]).withColumn(\"load_time\", current_timestamp()).writeTo(TABLE_LOG).using(\"iceberg\").append()\n",
    "            \n",
    "            # Clean up\n",
    "            df_trans.unpersist()\n",
    "            del df_trans\n",
    "            del df_raw\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   [ERROR] Loi xu ly file {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DA HOAN TAT TAT CA FILE FACEBOOK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a01ba3-579e-4e90-b89d-827dddc5197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, lit\n",
    "\n",
    "# ====================================================\n",
    "# CẤU HÌNH\n",
    "# ====================================================\n",
    "TABLE_ARTICLE = \"nessie.silver_tables.article\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"KIỂM TRA SỐ LƯỢNG BÀI VIẾT (FACEBOOK & TIKTOK)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Đọc dữ liệu từ bảng Article\n",
    "try:\n",
    "    df_article = spark.table(TABLE_ARTICLE)\n",
    "    \n",
    "    # 2. Thống kê theo loại (Type)\n",
    "    print(\"\\n--- Thống kê chi tiết theo nguồn ---\")\n",
    "    df_stats = df_article.groupBy(\"type\").count().orderBy(\"type\")\n",
    "    df_stats.show()\n",
    "    \n",
    "    # 3. Tính tổng số lượng\n",
    "    total_count = df_article.count()\n",
    "    print(f\"-> TỔNG CỘNG: {total_count} bài viết.\")\n",
    "    \n",
    "    # 4. Kiểm tra mẫu dữ liệu (Optional)\n",
    "    print(\"\\n--- 5 bài viết mới nhất ---\")\n",
    "    df_article.select(\"articleID\", \"type\", \"description\", \"created_at\") \\\n",
    "              .orderBy(col(\"created_at\").desc()) \\\n",
    "              .show(5, truncate=50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi đọc bảng {TABLE_ARTICLE}: {e}\")\n",
    "    print(\"Có thể bảng chưa được tạo hoặc chưa có dữ liệu.\")\n",
    "\n",
    "print(\"\\nHoàn tất kiểm tra.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c11d8e-7930-4437-bcc7-d868553d19dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dừng Spark Session để giải phóng resources\n",
    "spark.stop()\n",
    "print(\" Spark Session đã được dừng!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
