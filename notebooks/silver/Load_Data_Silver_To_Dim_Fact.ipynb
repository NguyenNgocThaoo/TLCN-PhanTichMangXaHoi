{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341915fc",
   "metadata": {},
   "source": [
    "# Load Data t·ª´ Silver Layer sang Gold Layer (Dimension & Fact Tables)\n",
    "\n",
    "Notebook n√†y s·∫Ω load d·ªØ li·ªáu t·ª´ Silver layer v√† transform sang Gold layer v·ªõi:\n",
    "- Dimension tables c√≥ surrogate key t·ª± ƒë·ªông tƒÉng\n",
    "- Fact tables v·ªõi c√°c foreign keys t∆∞∆°ng ·ª©ng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c4beb8",
   "metadata": {},
   "source": [
    "## 1. Import Libraries v√† Kh·ªüi t·∫°o Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "345f5653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/06 05:37:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/06 05:37:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/11/06 05:37:09 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o!\n",
      "Log level ƒë√£ ƒë∆∞·ª£c set th√†nh ERROR\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "\n",
    "# Kh·ªüi t·∫°o Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Load_Silver_To_Gold\")\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v1\")\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://gold/\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.access-key\", \"admin\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.secret-key\", \"admin123\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# T·∫Øt log WARN - ch·ªâ hi·ªÉn th·ªã ERROR\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"Spark Session ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o!\")\n",
    "print(\"Log level ƒë√£ ƒë∆∞·ª£c set th√†nh ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cf8a02",
   "metadata": {},
   "source": [
    "## 2. Load Dimension Tables t·ª´ Silver Layer\n",
    "\n",
    "### 2.1. Dim_Time - T·∫°o b·∫£ng th·ªùi gian t·ª´ d·ªØ li·ªáu benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eae759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_Time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ load 5 d√≤ng v√†o dim_time\n",
      "+-------+---+-----+----+\n",
      "|timeKey|day|month|year|\n",
      "+-------+---+-----+----+\n",
      "|      1|  1|    1|2021|\n",
      "|      2|  1|    1|2022|\n",
      "|      3|  1|    1|2023|\n",
      "|      4|  1|    1|2024|\n",
      "|      5|  1|    1|2025|\n",
      "+-------+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_Time...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_benchmark = spark.table(\"nessie.silver_tables.benchmark\")\n",
    "\n",
    "# T·∫°o dim_time t·ª´ c√°c year unique trong benchmark\n",
    "df_time = df_benchmark.select(\"year\").distinct()\n",
    "\n",
    "# T·∫°o day, month t·ª´ year (gi·∫£ s·ª≠ ng√†y 1/1 c·ªßa m·ªói nƒÉm)\n",
    "df_time = df_time.withColumn(\"day\", lit(1)) \\\n",
    "    .withColumn(\"month\", lit(1))\n",
    "\n",
    "# T·∫°o timeKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"year\")\n",
    "df_time = df_time.withColumn(\"timeKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt theo th·ª© t·ª±\n",
    "df_dim_time = df_time.select(\"timeKey\", \"day\", \"month\", \"year\")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_time.writeTo(\"nessie.gold_tables.dim_time\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_time.count()} d√≤ng v√†o dim_time\")\n",
    "df_dim_time.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b5f7c",
   "metadata": {},
   "source": [
    "### 2.2. Dim_Region - B·∫£ng khu v·ª±c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "042a4a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_Region...\n",
      "ƒê√£ load 64 d√≤ng v√†o dim_region\n",
      "+---------+--------+--------------------+\n",
      "|regionKey|regionId|          regionName|\n",
      "+---------+--------+--------------------+\n",
      "|        1|      01|      S·ªü GDƒêT H√† N·ªôi|\n",
      "|        2|      02|S·ªü GDƒêT TP. H·ªì Ch...|\n",
      "|        3|      03|   S·ªü GDƒêT H·∫£i Ph√≤ng|\n",
      "|        4|      04|     S·ªü GDƒêT ƒê√† N·∫µng|\n",
      "|        5|      05|    S·ªü GDƒêT H√† Giang|\n",
      "|        6|      06|    S·ªü GDƒêT Cao B·∫±ng|\n",
      "|        7|      07|    S·ªü GDƒêT Lai Ch√¢u|\n",
      "|        8|      08|     S·ªü GDƒêT L√†o Cai|\n",
      "|        9|      09| S·ªü GDƒêT Tuy√™n Quang|\n",
      "|       10|      10|    S·ªü GDƒêT L·∫°ng S∆°n|\n",
      "+---------+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_Region...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_region_silver = spark.table(\"nessie.silver_tables.region\")\n",
    "\n",
    "# T·∫°o regionKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"regionId\")\n",
    "df_dim_region = df_region_silver.withColumn(\"regionKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt\n",
    "df_dim_region = df_dim_region.select(\"regionKey\", \"regionId\", \"regionName\")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_region.writeTo(\"nessie.gold_tables.dim_region\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_region.count()} d√≤ng v√†o dim_region\")\n",
    "df_dim_region.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd737abe",
   "metadata": {},
   "source": [
    "### 2.3. Dim_School - B·∫£ng tr∆∞·ªùng ƒë·∫°i h·ªçc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb73fa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_School...\n",
      "ƒê√£ load 270 d√≤ng v√†o dim_school\n",
      "+---------+--------+--------------------+---------+\n",
      "|schoolKey|schoolId|          schoolName| province|\n",
      "+---------+--------+--------------------+---------+\n",
      "|        1|   \\bDMT|Ph√¢n hi·ªáu ƒêH T√†i ...|Thanh H√≥a|\n",
      "|        2|     ANH|H·ªçc vi·ªán An Ninh ...|   H√† N·ªôi|\n",
      "|        3|     ANS|ƒê·∫°i h·ªçc An Ninh N...|   H√† N·ªôi|\n",
      "|        4|     BKA|ƒê·∫°i h·ªçc B√°ch khoa...|   H√† N·ªôi|\n",
      "|        5|     BMU|ƒê·∫°i h·ªçc Bu√¥n Ma T...|  ƒê·∫Øk L·∫Øk|\n",
      "|        6|     BPH| H·ªçc vi·ªán Bi√™n Ph√≤ng|   H√† N·ªôi|\n",
      "|        7|     BVH|H·ªçc vi·ªán C√¥ng ngh...|   H√† N·ªôi|\n",
      "|        8|     BVS|H·ªçc vi·ªán C√¥ng ngh...|   TP HCM|\n",
      "|        9|     CCM|ƒê·∫°i h·ªçc C√¥ng Nghi...|   H√† N·ªôi|\n",
      "|       10|     CEA|ƒê·∫°i h·ªçc Kinh T·∫ø N...|  Ngh·ªá An|\n",
      "+---------+--------+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_School...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_school_silver = spark.table(\"nessie.silver_tables.school\")\n",
    "\n",
    "# T·∫°o schoolKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"schoolId\")\n",
    "df_dim_school = df_school_silver.withColumn(\"schoolKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt\n",
    "df_dim_school = df_dim_school.select(\"schoolKey\", \"schoolId\", \"schoolName\", \"province\")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_school.writeTo(\"nessie.gold_tables.dim_school\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_school.count()} d√≤ng v√†o dim_school\")\n",
    "df_dim_school.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e3af4a",
   "metadata": {},
   "source": [
    "### 2.4. Dim_Major - B·∫£ng ng√†nh h·ªçc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de69025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_Major...\n",
      "ƒê√£ load 3265 d√≤ng v√†o dim_major\n",
      "+--------+-------+--------------------+\n",
      "|majorKey|majorId|           majorName|\n",
      "+--------+-------+--------------------+\n",
      "|       1|    106|   Khoa h·ªçc M√°y t√≠nh|\n",
      "|       2|    107|   K·ªπ thu·∫≠t M√°y t√≠nh|\n",
      "|       3|    108|ƒêi·ªán - ƒêi·ªán t·ª≠ - ...|\n",
      "|       4|    109|     K·ªπ Thu·∫≠t C∆° kh√≠|\n",
      "|       5|    110| K·ªπ Thu·∫≠t C∆° ƒêi·ªán t·ª≠|\n",
      "|       6|    112|           D·ªát - May|\n",
      "|       7|    114|Ho√° - Th·ª±c ph·∫©m -...|\n",
      "|       8|    115|X√¢y d·ª±ng v√† Qu·∫£n ...|\n",
      "|       9|    117|           Ki·∫øn Tr√∫c|\n",
      "|      10|    120|  D·∫ßu kh√≠ - ƒê·ªãa ch·∫•t|\n",
      "+--------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_Major...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_major_silver = spark.table(\"nessie.silver_tables.major\")\n",
    "\n",
    "# T·∫°o majorKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"majorId\")\n",
    "df_dim_major = df_major_silver.withColumn(\"majorKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt\n",
    "df_dim_major = df_dim_major.select(\"majorKey\", \"majorId\", \"majorName\")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_major.writeTo(\"nessie.gold_tables.dim_major\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_major.count()} d√≤ng v√†o dim_major\")\n",
    "df_dim_major.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8e04d",
   "metadata": {},
   "source": [
    "### 2.5. Dim_Subject - B·∫£ng m√¥n h·ªçc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba8b3584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_Subject...\n",
      "ƒê√£ load 88 d√≤ng v√†o dim_subject\n",
      "+----------+---------+--------------------+\n",
      "|subjectKey|subjectId|         subjectName|\n",
      "+----------+---------+--------------------+\n",
      "|         1|        1|Bi·ªÉu di·ªÖn ngh·ªá thu·∫≠t|\n",
      "|         2|        2|C√¥ng ngh·ªá c√¥ng ng...|\n",
      "|         3|        3|C√¥ng ngh·ªá n√¥ng ng...|\n",
      "|         4|        4|             GDKT&PL|\n",
      "|         5|        5|              GDKTPL|\n",
      "|         6|        6|                 Ho√°|\n",
      "|         7|        7|                 H√°t|\n",
      "|         8|        8|H√°t ho·∫∑c bi·ªÉu di·ªÖ...|\n",
      "|         9|        9|            H√¨nh h·ªça|\n",
      "|        10|       10|                 H√≥a|\n",
      "+----------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_Subject...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_subject_silver = spark.table(\"nessie.silver_tables.subject\")\n",
    "\n",
    "# T·∫°o subjectKey t·ª± ƒë·ªông tƒÉng (s·ª≠ d·ª•ng subjectId l√†m sort key)\n",
    "window_spec = Window.orderBy(\"subjectId\")\n",
    "df_dim_subject = df_subject_silver.withColumn(\"subjectKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt\n",
    "df_dim_subject = df_dim_subject.select(\"subjectKey\", \"subjectId\", \"subjectName\")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_subject.writeTo(\"nessie.gold_tables.dim_subject\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_subject.count()} d√≤ng v√†o dim_subject\")\n",
    "df_dim_subject.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfcd8ab",
   "metadata": {},
   "source": [
    "### 2.6. Dim_Subject_Group - B·∫£ng kh·ªëi thi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32f9716f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_Subject_Group...\n",
      "ƒê√£ load 212 d√≤ng v√†o dim_subject_group\n",
      "+---------------+--------------+----------------+--------------------+\n",
      "|subjectGroupKey|subjectGroupId|subjectGroupName|  subjectCombination|\n",
      "+---------------+--------------+----------------+--------------------+\n",
      "|              1|             1|             D01|To√°n-Ng·ªØ VƒÉn-Ti·∫øn...|\n",
      "|              2|             2|             A00| To√°n-V·∫≠t l√≠-H√≥a h·ªçc|\n",
      "|              3|             3|             A01|To√°n-V·∫≠t l√≠-Ti·∫øng...|\n",
      "|              4|             4|             D07|To√°n-H√≥a h·ªçc-Ti·∫øn...|\n",
      "|              5|             5|             B00|To√°n-H√≥a h·ªçc-Sinh...|\n",
      "|              6|             6|             C00|Ng·ªØ vƒÉn-L·ªãch s·ª≠-ƒê...|\n",
      "|              7|             7|             C01| Ng·ªØ vƒÉn-To√°n-V·∫≠t l√≠|\n",
      "|              8|             8|             D14|Ng·ªØ vƒÉn-L·ªãch s·ª≠-T...|\n",
      "|              9|             9|             D15|Ng·ªØ vƒÉn-ƒê·ªãa l√≠-Ti...|\n",
      "|             10|            10|             A02|To√°n-V·∫≠t l√≠-Sinh h·ªçc|\n",
      "+---------------+--------------+----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_Subject_Group...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_subject_group_silver = spark.table(\"nessie.silver_tables.subject_group\")\n",
    "\n",
    "# T·∫°o subjectGroupKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"subjectGroupId\")\n",
    "df_dim_subject_group = df_subject_group_silver.withColumn(\"subjectGroupKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt\n",
    "df_dim_subject_group = df_dim_subject_group.select(\n",
    "    \"subjectGroupKey\", \n",
    "    \"subjectGroupId\", \n",
    "    \"subjectGroupName\", \n",
    "    \"subjectCombination\"\n",
    ")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_subject_group.writeTo(\"nessie.gold_tables.dim_subject_group\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_subject_group.count()} d√≤ng v√†o dim_subject_group\")\n",
    "df_dim_subject_group.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47b43b1",
   "metadata": {},
   "source": [
    "### 2.7. Dim_Selection_Method - B·∫£ng ph∆∞∆°ng th·ª©c x√©t tuy·ªÉn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d64da34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_Selection_Method...\n",
      "ƒê√£ load 13 d√≤ng v√†o dim_selection_method\n",
      "+------------------+-----------------+--------------------+\n",
      "|selectionMethodKey|selectionMethodId| selectionMethodName|\n",
      "+------------------+-----------------+--------------------+\n",
      "|                 1|                1|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 2|                2|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 3|                3|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 4|                4|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 5|                5|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 6|                6|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 7|                7|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 8|                8|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 9|                9|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                10|               10|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "+------------------+-----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_Selection_Method...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_selection_method_silver = spark.table(\"nessie.silver_tables.selection_method\")\n",
    "\n",
    "# T·∫°o selectionMethodKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"selectionMethodId\")\n",
    "df_dim_selection_method = df_selection_method_silver.withColumn(\"selectionMethodKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt\n",
    "df_dim_selection_method = df_dim_selection_method.select(\n",
    "    \"selectionMethodKey\", \n",
    "    \"selectionMethodId\", \n",
    "    \"selectionMethodName\"\n",
    ")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_selection_method.writeTo(\"nessie.gold_tables.dim_selection_method\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_selection_method.count()} d√≤ng v√†o dim_selection_method\")\n",
    "df_dim_selection_method.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f18c813",
   "metadata": {},
   "source": [
    "## 3. Load Fact Tables t·ª´ Silver Layer\n",
    "\n",
    "### 3.1. Fact_Benchmark - Load v√† join v·ªõi c√°c Dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fd8f72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Fact_Benchmark...\n",
      "Sau t·∫•t c·∫£ join: 143,863 d√≤ng\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ load 143863 d√≤ng v√†o fact_benchmark\n",
      "+------------+---------------+-------+--------+---------+------------------+-----+---------------+--------------+--------------+----------------+\n",
      "|benchmarkKey|subjectGroupKey|timeKey|majorKey|schoolKey|selectionMethodKey|score|avgScoreByMajor|yearlyScoreGap|rankAmongMajor|rankAmongSchools|\n",
      "+------------+---------------+-------+--------+---------+------------------+-----+---------------+--------------+--------------+----------------+\n",
      "|       14165|              1|      1|      41|      206|                 3| 26.7|           26.7|           0.0|            14|               1|\n",
      "|       14166|              4|      1|      41|      206|                 3| 26.7|           26.7|           0.0|            14|               1|\n",
      "|       14167|              2|      1|      41|      206|                 3| 26.7|           26.7|           0.0|            14|               1|\n",
      "|       14168|              3|      1|      41|      206|                 3| 26.7|           26.7|           0.0|            14|               1|\n",
      "|        1530|             57|      1|      66|       17|                 2| 19.5|           19.5|           0.0|             4|               1|\n",
      "|       15271|             57|      1|      66|      218|                 2| 19.5|           19.5|           0.0|            21|               2|\n",
      "|       15274|            173|      1|      66|      218|                 2| 19.5|           19.5|           0.0|            21|               2|\n",
      "|       15276|             95|      1|      66|      218|                 2| 19.5|           19.5|           0.0|            21|               2|\n",
      "|       18832|            173|      1|      66|      239|                 2| 19.0|           19.0|           0.0|            14|               3|\n",
      "|       18834|             57|      1|      66|      239|                 2| 19.0|           19.0|           0.0|            14|               3|\n",
      "+------------+---------------+-------+--------+---------+------------------+-----+---------------+--------------+--------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Fact_Benchmark...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_benchmark_silver = spark.table(\"nessie.silver_tables.benchmark\")\n",
    "\n",
    "# ƒê·ªçc c√°c dimension tables t·ª´ Gold ƒë·ªÉ join\n",
    "df_dim_time_gold = spark.table(\"nessie.gold_tables.dim_time\")\n",
    "df_dim_school_gold = spark.table(\"nessie.gold_tables.dim_school\")\n",
    "df_dim_major_gold = spark.table(\"nessie.gold_tables.dim_major\")\n",
    "df_dim_subject_group_gold = spark.table(\"nessie.gold_tables.dim_subject_group\")\n",
    "df_dim_selection_method_gold = spark.table(\"nessie.gold_tables.dim_selection_method\")\n",
    "\n",
    "# Join t·ª´ng b∆∞·ªõc ƒë·ªÉ tr√°nh Cartesian product\n",
    "# B∆∞·ªõc 1: Join v·ªõi dim_time\n",
    "df_fact = df_benchmark_silver.join(\n",
    "    df_dim_time_gold, \n",
    "    df_benchmark_silver[\"year\"] == df_dim_time_gold[\"year\"], \n",
    "    \"left\"\n",
    ").drop(df_dim_time_gold[\"year\"])\n",
    "\n",
    "# B∆∞·ªõc 2: Join v·ªõi dim_school\n",
    "df_fact = df_fact.join(\n",
    "    df_dim_school_gold, \n",
    "    df_fact[\"schoolId\"] == df_dim_school_gold[\"schoolId\"], \n",
    "    \"left\"\n",
    ").drop(df_dim_school_gold[\"schoolId\"])\n",
    "\n",
    "# B∆∞·ªõc 3: Join v·ªõi dim_major\n",
    "df_fact = df_fact.join(\n",
    "    df_dim_major_gold, \n",
    "    df_fact[\"majorId\"] == df_dim_major_gold[\"majorId\"], \n",
    "    \"left\"\n",
    ").drop(df_dim_major_gold[\"majorId\"])\n",
    "\n",
    "# B∆∞·ªõc 4: Join v·ªõi dim_subject_group\n",
    "df_fact = df_fact.join(\n",
    "    df_dim_subject_group_gold, \n",
    "    df_fact[\"subjectGroupId\"] == df_dim_subject_group_gold[\"subjectGroupId\"], \n",
    "    \"left\"\n",
    ").drop(df_dim_subject_group_gold[\"subjectGroupId\"])\n",
    "\n",
    "# B∆∞·ªõc 5: Join v·ªõi dim_selection_method\n",
    "df_fact = df_fact.join(\n",
    "    df_dim_selection_method_gold, \n",
    "    df_fact[\"selectionMethodId\"] == df_dim_selection_method_gold[\"selectionMethodId\"], \n",
    "    \"left\"\n",
    ").drop(df_dim_selection_method_gold[\"selectionMethodId\"])\n",
    "\n",
    "print(f\"Sau t·∫•t c·∫£ join: {df_fact.count():,} d√≤ng\")\n",
    "\n",
    "# T·∫°o benchmarkKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"timeKey\", \"schoolKey\", \"majorKey\")\n",
    "df_fact = df_fact.withColumn(\"benchmarkKey\", row_number().over(window_spec))\n",
    "\n",
    "# T√≠nh to√°n yearlyScoreGap\n",
    "window_year_spec = Window.partitionBy(\"schoolKey\", \"majorKey\", \"selectionMethodKey\").orderBy(\"timeKey\")\n",
    "df_fact = df_fact.withColumn(\n",
    "    \"yearlyScoreGap\",\n",
    "    coalesce(col(\"score\") - lag(\"score\", 1).over(window_year_spec), lit(0))\n",
    ")\n",
    "\n",
    "# T√≠nh ƒëi·ªÉm trung b√¨nh cho m·ªói ng√†nh theo t·ª´ng ph∆∞∆°ng th·ª©c\n",
    "window_avg_spec = Window.partitionBy(\"timeKey\", \"schoolKey\", \"majorKey\", \"selectionMethodKey\")\n",
    "df_fact = df_fact.withColumn(\n",
    "    \"avgScoreByMajor\",\n",
    "    avg(\"score\").over(window_avg_spec)\n",
    ")\n",
    "\n",
    "# X·∫øp h·∫°ng ng√†nh trong tr∆∞·ªùng\n",
    "window_major_in_school_spec = Window.partitionBy(\"timeKey\", \"schoolKey\", \"selectionMethodKey\").orderBy(desc(\"avgScoreByMajor\"), \"majorKey\")\n",
    "df_fact = df_fact.withColumn(\"rankAmongMajor\", dense_rank().over(window_major_in_school_spec))\n",
    "\n",
    "# X·∫øp h·∫°ng ng√†nh so v·ªõi c√°c tr∆∞·ªùng kh√°c\n",
    "window_major_across_schools_spec = Window.partitionBy(\"timeKey\", \"majorKey\", \"selectionMethodKey\").orderBy(desc(\"avgScoreByMajor\"), \"schoolKey\")\n",
    "df_fact = df_fact.withColumn(\"rankAmongSchools\", dense_rank().over(window_major_across_schools_spec))\n",
    "\n",
    "# Select c√°c c·ªôt cu·ªëi c√πng\n",
    "df_fact_benchmark = df_fact.select(\n",
    "    \"benchmarkKey\",\n",
    "    \"subjectGroupKey\",\n",
    "    \"timeKey\",\n",
    "    \"majorKey\",\n",
    "    \"schoolKey\",\n",
    "    \"selectionMethodKey\",\n",
    "    col(\"score\").cast(\"float\"),\n",
    "    col(\"avgScoreByMajor\").cast(\"float\"),\n",
    "    col(\"yearlyScoreGap\").cast(\"float\"),\n",
    "    \"rankAmongMajor\",\n",
    "    \"rankAmongSchools\"\n",
    ")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_fact_benchmark.writeTo(\"nessie.gold_tables.fact_benchmark\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .partitionedBy(\"timeKey\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_fact_benchmark.count()} d√≤ng v√†o fact_benchmark\")\n",
    "df_fact_benchmark.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6025881c",
   "metadata": {},
   "source": [
    "### 3.2. Fact_Score_Distribution_By_Subject - Ph√¢n b·ªë ƒëi·ªÉm theo m√¥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e2a323f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Fact_Score_Distribution_By_Subject...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ load 13213 d√≤ng v√†o fact_score_distribution_by_subject\n",
      "+-------+---------+----------+-------+--------------+--------+\n",
      "|SDBSKey|regionKey|subjectKey|timeKey|scoreThreshold|quantity|\n",
      "+-------+---------+----------+-------+--------------+--------+\n",
      "|      1|        1|        10|      1|           0.0|     2.0|\n",
      "|      2|        1|        10|      1|           0.2|     1.0|\n",
      "|      3|        1|        10|      1|           0.4|     1.0|\n",
      "|      4|        1|        10|      1|           0.6|     5.0|\n",
      "|      5|        1|        10|      1|           1.0|    11.0|\n",
      "|      6|        1|        10|      1|           1.2|    30.0|\n",
      "|      7|        1|        10|      1|           1.4|    36.0|\n",
      "|      8|        1|        10|      1|           1.6|   102.0|\n",
      "|      9|        1|        10|      1|           2.0|   158.0|\n",
      "|     10|        1|        10|      1|           2.2|   236.0|\n",
      "+-------+---------+----------+-------+--------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Fact_Score_Distribution_By_Subject...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_student_scores = spark.table(\"nessie.silver_tables.student_scores\")\n",
    "\n",
    "# ƒê·ªçc dimension tables\n",
    "df_dim_time_gold = spark.table(\"nessie.gold_tables.dim_time\")\n",
    "df_dim_region_gold = spark.table(\"nessie.gold_tables.dim_region\")\n",
    "df_dim_subject_gold = spark.table(\"nessie.gold_tables.dim_subject\")\n",
    "\n",
    "# Explode map scores th√†nh t·ª´ng d√≤ng (subjectId - ƒëi·ªÉm)\n",
    "# V√¨ scores gi·ªù l√† Map<Int, Float> (subjectId -> score)\n",
    "# subjectId ·ªü ƒë√¢y l√† INTEGER, ch√≠nh l√† c·ªôt subjectId t·ª´ b·∫£ng silver.subject\n",
    "df_scores_exploded = df_student_scores.select(\n",
    "    \"regionId\",\n",
    "    \"year\",\n",
    "    explode(\"scores\").alias(\"subjectId\", \"score\")  # subjectId = key c·ªßa map (Int)\n",
    ")\n",
    "\n",
    "# Ph√¢n lo·∫°i ƒëi·ªÉm v√†o c√°c m·ªëc ƒëi·ªÉm (0.2, 0.4, 0.6, ..., 9.8, 10.0)\n",
    "# L√†m tr√≤n ƒëi·ªÉm ƒë·∫øn b·ªôi s·ªë c·ªßa 0.2 g·∫ßn nh·∫•t\n",
    "df_scores_exploded = df_scores_exploded.withColumn(\n",
    "    \"scoreThreshold\",\n",
    "    (floor(col(\"score\") / 0.2) * 0.2).cast(\"float\")\n",
    ")\n",
    "\n",
    "# ƒê·∫øm s·ªë l∆∞·ª£ng h·ªçc sinh theo t·ª´ng m√¥n, khu v·ª±c, nƒÉm v√† ng∆∞·ª°ng ƒëi·ªÉm\n",
    "df_distribution = df_scores_exploded.groupBy(\n",
    "    \"regionId\",\n",
    "    \"subjectId\",  # ƒê√¢y l√† subjectId (Int) t·ª´ scores map\n",
    "    \"year\",\n",
    "    \"scoreThreshold\"\n",
    ").agg(\n",
    "    count(\"*\").alias(\"quantity\")\n",
    ")\n",
    "\n",
    "# Join v·ªõi c√°c dimension tables\n",
    "# Join v·ªõi dim_subject b·∫±ng subjectId ƒë·ªÉ l·∫•y subjectKey\n",
    "df_fact = df_distribution \\\n",
    "    .join(df_dim_region_gold, df_distribution[\"regionId\"] == df_dim_region_gold[\"regionId\"], \"left\") \\\n",
    "    .join(df_dim_subject_gold, df_distribution[\"subjectId\"] == df_dim_subject_gold[\"subjectId\"], \"left\") \\\n",
    "    .join(df_dim_time_gold, df_distribution[\"year\"] == df_dim_time_gold[\"year\"], \"left\")\n",
    "\n",
    "# T·∫°o SDBSKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"timeKey\", \"regionKey\", \"subjectKey\", \"scoreThreshold\")\n",
    "df_fact = df_fact.withColumn(\"SDBSKey\", row_number().over(window_spec))\n",
    "\n",
    "# Select c√°c c·ªôt cu·ªëi c√πng\n",
    "# L∆∞u √Ω: L·∫•y subjectKey t·ª´ df_dim_subject_gold (kh√¥ng ph·∫£i subjectId)\n",
    "df_fact_score_dist_subject = df_fact.select(\n",
    "    \"SDBSKey\",\n",
    "    df_dim_region_gold[\"regionKey\"],\n",
    "    df_dim_subject_gold[\"subjectKey\"],  # subjectKey t·ª´ dim_subject (surrogate key)\n",
    "    df_dim_time_gold[\"timeKey\"],\n",
    "    col(\"scoreThreshold\").cast(\"float\"),\n",
    "    col(\"quantity\").cast(\"float\")\n",
    ")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_fact_score_dist_subject.writeTo(\"nessie.gold_tables.fact_score_distribution_by_subject\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .partitionedBy(\"timeKey\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_fact_score_dist_subject.count()} d√≤ng v√†o fact_score_distribution_by_subject\")\n",
    "df_fact_score_dist_subject.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bad2e2",
   "metadata": {},
   "source": [
    "### 3.3. Fact_Score_Distribution_By_Subject_Group - Ph√¢n b·ªë ƒëi·ªÉm theo kh·ªëi thi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbb08013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"ƒêang load Fact_Score_Distribution_By_Subject_Group...\")\n",
    "\n",
    "# # ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "# df_student_scores = spark.table(\"nessie.silver_tables.student_scores\")\n",
    "# df_subject_group = spark.table(\"nessie.silver_tables.subject_group\")\n",
    "# df_subject = spark.table(\"nessie.silver_tables.subject\")\n",
    "\n",
    "# # ƒê·ªçc dimension tables\n",
    "# df_dim_time_gold = spark.table(\"nessie.gold_tables.dim_time\")\n",
    "# df_dim_region_gold = spark.table(\"nessie.gold_tables.dim_region\")\n",
    "# df_dim_subject_group_gold = spark.table(\"nessie.gold_tables.dim_subject_group\")\n",
    "\n",
    "# # T·∫°o mapping subjectName -> subjectId\n",
    "# subject_name_to_id = {row['subjectName']: row['subjectId'] for row in df_subject.collect()}\n",
    "# print(f\"Subject mapping: {subject_name_to_id}\")\n",
    "\n",
    "# # Broadcast mappings\n",
    "# from pyspark.sql.types import FloatType, MapType, StringType\n",
    "# subject_map_broadcast = spark.sparkContext.broadcast(subject_name_to_id)\n",
    "\n",
    "# # Collect subject groups th√†nh dictionary: {subjectGroupId: [list of subject names]}\n",
    "# subject_groups_dict = {}\n",
    "# for row in df_subject_group.collect():\n",
    "#     group_id = row['subjectGroupId']\n",
    "#     subjects = [s.strip() for s in row['subjectCombination'].split('-')]\n",
    "#     subject_groups_dict[group_id] = subjects\n",
    "\n",
    "# subject_groups_broadcast = spark.sparkContext.broadcast(subject_groups_dict)\n",
    "\n",
    "# print(f\"S·ªë l∆∞·ª£ng kh·ªëi thi: {len(subject_groups_dict)}\")\n",
    "\n",
    "# # UDF ƒë·ªÉ t·∫°o mapping Map<subjectGroupId, totalScore> t·ª´ scores c·ªßa th√≠ sinh\n",
    "# def create_subject_group_scores_map(scores_map):\n",
    "#     \"\"\"\n",
    "#     T·ª´ scores_map (Map<subjectId, score>) c·ªßa th√≠ sinh,\n",
    "#     t·∫°o ra Map<subjectGroupId, totalScore> cho t·∫•t c·∫£ kh·ªëi thi m√† th√≠ sinh c√≥ ƒë·ªß m√¥n\n",
    "#     \"\"\"\n",
    "#     if scores_map is None:\n",
    "#         return {}\n",
    "    \n",
    "#     subject_name_to_id_map = subject_map_broadcast.value\n",
    "#     subject_id_to_name_map = {v: k for k, v in subject_name_to_id_map.items()}\n",
    "#     subject_groups = subject_groups_broadcast.value\n",
    "    \n",
    "#     result = {}\n",
    "    \n",
    "#     # V·ªõi m·ªói kh·ªëi thi, ki·ªÉm tra xem th√≠ sinh c√≥ ƒë·ªß m√¥n kh√¥ng\n",
    "#     for group_id, required_subjects in subject_groups.items():\n",
    "#         total_score = 0.0\n",
    "#         has_all_subjects = True\n",
    "        \n",
    "#         for subject_name in required_subjects:\n",
    "#             subject_id = subject_name_to_id_map.get(subject_name)\n",
    "            \n",
    "#             if subject_id is None or subject_id not in scores_map:\n",
    "#                 has_all_subjects = False\n",
    "#                 break\n",
    "            \n",
    "#             total_score += scores_map[subject_id]\n",
    "        \n",
    "#         # Ch·ªâ th√™m v√†o result n·∫øu th√≠ sinh c√≥ ƒë·ªß t·∫•t c·∫£ c√°c m√¥n trong kh·ªëi\n",
    "#         if has_all_subjects:\n",
    "#             result[group_id] = total_score\n",
    "    \n",
    "#     return result\n",
    "\n",
    "# create_group_scores_udf = udf(create_subject_group_scores_map, MapType(StringType(), FloatType()))\n",
    "\n",
    "# # T·∫°o c·ªôt m·ªõi ch·ª©a mapping subjectGroupId -> totalScore\n",
    "# print(\"ƒêang t·∫°o c·ªôt subject_group_scores cho student_scores...\")\n",
    "# df_student_with_group_scores = df_student_scores.withColumn(\n",
    "#     \"subject_group_scores\",\n",
    "#     create_group_scores_udf(col(\"scores\"))\n",
    "# )\n",
    "\n",
    "# # Hi·ªÉn th·ªã sample\n",
    "# print(\"Sample student_scores v·ªõi subject_group_scores:\")\n",
    "# df_student_with_group_scores.select(\"studentId\", \"regionId\", \"year\", \"subject_group_scores\").show(5, truncate=False)\n",
    "\n",
    "# # Explode map subject_group_scores th√†nh t·ª´ng d√≤ng (subjectGroupId - totalScore)\n",
    "# df_scores_exploded = df_student_with_group_scores.select(\n",
    "#     \"regionId\",\n",
    "#     \"year\",\n",
    "#     explode(\"subject_group_scores\").alias(\"subjectGroupId\", \"totalScore\")\n",
    "# )\n",
    "\n",
    "# print(f\"T·ªïng s·ªë d√≤ng sau explode: {df_scores_exploded.count():,}\")\n",
    "\n",
    "# # Ph√¢n lo·∫°i ƒëi·ªÉm v√†o c√°c kho·∫£ng (0-1, 1-2, ..., 29-30)\n",
    "# df_scores_exploded = df_scores_exploded.withColumn(\n",
    "#     \"scoreRange\",\n",
    "#     floor(col(\"totalScore\")).cast(\"float\")\n",
    "# )\n",
    "\n",
    "# # ƒê·∫øm s·ªë l∆∞·ª£ng h·ªçc sinh theo t·ª´ng kh·ªëi thi, khu v·ª±c, nƒÉm v√† kho·∫£ng ƒëi·ªÉm\n",
    "# df_distribution = df_scores_exploded.groupBy(\n",
    "#     \"regionId\",\n",
    "#     \"subjectGroupId\",\n",
    "#     \"year\",\n",
    "#     \"scoreRange\"\n",
    "# ).agg(\n",
    "#     count(\"*\").alias(\"quantity\")\n",
    "# )\n",
    "\n",
    "# print(f\"S·ªë d√≤ng distribution: {df_distribution.count():,}\")\n",
    "\n",
    "# # T·∫°o mapping dictionaries t·ª´ dimension tables ƒë·ªÉ tra c·ª©u nhanh (thay v√¨ JOIN)\n",
    "# print(\"ƒêang t·∫°o mapping dictionaries...\")\n",
    "# region_id_to_key = {row['regionId']: row['regionKey'] for row in df_dim_region_gold.select(\"regionId\", \"regionKey\").collect()}\n",
    "# subject_group_id_to_key = {row['subjectGroupId']: row['subjectGroupKey'] for row in df_dim_subject_group_gold.select(\"subjectGroupId\", \"subjectGroupKey\").collect()}\n",
    "# year_to_time_key = {row['year']: row['timeKey'] for row in df_dim_time_gold.select(\"year\", \"timeKey\").collect()}\n",
    "\n",
    "# # Broadcast mappings\n",
    "# region_map_bc = spark.sparkContext.broadcast(region_id_to_key)\n",
    "# subject_group_map_bc = spark.sparkContext.broadcast(subject_group_id_to_key)\n",
    "# year_map_bc = spark.sparkContext.broadcast(year_to_time_key)\n",
    "\n",
    "# # UDF ƒë·ªÉ map c√°c natural keys sang surrogate keys\n",
    "# def map_to_keys(regionId, subjectGroupId, year):\n",
    "#     region_key = region_map_bc.value.get(regionId)\n",
    "#     subject_group_key = subject_group_map_bc.value.get(subjectGroupId)\n",
    "#     time_key = year_map_bc.value.get(year)\n",
    "#     return (region_key, subject_group_key, time_key)\n",
    "\n",
    "# map_keys_udf = udf(map_to_keys, StructType([\n",
    "#     StructField(\"regionKey\", IntegerType(), True),\n",
    "#     StructField(\"subjectGroupKey\", IntegerType(), True),\n",
    "#     StructField(\"timeKey\", IntegerType(), True)\n",
    "# ]))\n",
    "\n",
    "# # √Åp d·ª•ng mapping\n",
    "# df_with_keys = df_distribution.withColumn(\n",
    "#     \"keys\",\n",
    "#     map_keys_udf(col(\"regionId\"), col(\"subjectGroupId\"), col(\"year\"))\n",
    "# )\n",
    "\n",
    "# # Extract c√°c keys t·ª´ struct\n",
    "# df_with_keys = df_with_keys.withColumn(\"regionKey\", col(\"keys.regionKey\")) \\\n",
    "#     .withColumn(\"subjectGroupKey\", col(\"keys.subjectGroupKey\")) \\\n",
    "#     .withColumn(\"timeKey\", col(\"keys.timeKey\")) \\\n",
    "#     .drop(\"keys\", \"regionId\", \"subjectGroupId\", \"year\")\n",
    "\n",
    "# # T·∫°o SDBSGKey t·ª± ƒë·ªông tƒÉng\n",
    "# window_spec = Window.orderBy(\"timeKey\", \"regionKey\", \"subjectGroupKey\", \"scoreRange\")\n",
    "# df_with_keys = df_with_keys.withColumn(\"SDBSGKey\", row_number().over(window_spec))\n",
    "\n",
    "# # Select c√°c c·ªôt cu·ªëi c√πng\n",
    "# df_fact_score_dist_group = df_with_keys.select(\n",
    "#     \"SDBSGKey\",\n",
    "#     \"regionKey\",\n",
    "#     \"subjectGroupKey\",\n",
    "#     \"timeKey\",\n",
    "#     col(\"scoreRange\").cast(\"float\"),\n",
    "#     col(\"quantity\").cast(\"float\")\n",
    "# )\n",
    "\n",
    "# # Ghi v√†o Gold layer\n",
    "# df_fact_score_dist_group.writeTo(\"nessie.gold_tables.fact_score_distribution_by_subject_group\") \\\n",
    "#     .using(\"iceberg\") \\\n",
    "#     .partitionedBy(\"timeKey\") \\\n",
    "#     .createOrReplace()\n",
    "\n",
    "# print(f\"ƒê√£ load {df_fact_score_dist_group.count()} d√≤ng v√†o fact_score_distribution_by_subject_group\")\n",
    "# df_fact_score_dist_group.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e8e0f",
   "metadata": {},
   "source": [
    "#### üìù PH·∫¶N 1: T√≠nh to√°n t·∫•t c·∫£ t·ªï h·ª£p kh·ªëi thi m√† m·ªói th√≠ sinh c√≥ th·ªÉ x√©t\n",
    "\n",
    "**M·ª•c ti√™u:** V·ªõi m·ªói th√≠ sinh, t√≠nh ƒëi·ªÉm t·ªïng cho T·∫§T C·∫¢ c√°c kh·ªëi thi m√† th√≠ sinh c√≥ ƒë·ªß ƒëi·ªÉm c√°c m√¥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853fa743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PH·∫¶N 1: T√çNH TO√ÅN T·ªî H·ª¢P KH·ªêI THI CHO M·ªñI TH√ç SINH (C√ì DEBUG LOG)\n",
      "================================================================================\n",
      "\n",
      "[B∆∞·ªõc 1] ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver layer...\n",
      "  ‚úì S·ªë l∆∞·ª£ng th√≠ sinh: 985,353\n",
      "  ‚úì S·ªë l∆∞·ª£ng kh·ªëi thi: 212\n",
      "  ‚úì S·ªë l∆∞·ª£ng m√¥n h·ªçc: 88\n",
      "\n",
      "[B∆∞·ªõc 2] T·∫°o mapping dictionaries...\n",
      "  ‚úì Subject mapping: {'Bi·ªÉu di·ªÖn ngh·ªá thu·∫≠t': 1, 'C√¥ng ngh·ªá c√¥ng nghi·ªáp': 2, 'C√¥ng ngh·ªá n√¥ng nghi·ªáp': 3, 'GDKT&PL': 4, 'GDKTPL': 5, 'Ho√°': 6, 'H√°t': 7, 'H√°t ho·∫∑c bi·ªÉu di·ªÖn nh·∫°c c·ª•': 8, 'H√¨nh h·ªça': 9, 'H√≥a': 10, 'H√≥a h·ªçc': 11, 'Khoa h·ªçc x√£ h·ªôi': 12, 'K√Ω x∆∞·ªõng √¢m': 13, 'LiÃ£ch s∆∞Ãâ': 14, 'L√Ω': 15, 'L√Ω/H√≥a/Sinh/Tin': 16, 'L·ªãch s·ª≠': 17, 'M√∫a': 18, 'NK M·∫ßm non 1( k·ªÉ chuy·ªán': 19, 'NK M·∫ßm non 2 (H√°t)': 20, 'NK1': 21, 'Ngo·∫°i ng·ªØ': 22, 'Ng∆∞ÃÉ vƒÉn': 23, 'Ng·ªØ VƒÉn': 24, 'Ng·ªØ vƒÉn': 25, 'NƒÉng Khi·∫øu 1': 26, 'NƒÉng Khi·∫øu 2': 27, 'NƒÉng khi√™ÃÅu': 28, 'NƒÉng khi√™ÃÅu TDTT': 29, 'NƒÉng khi·∫øu': 30, 'NƒÉng khi·∫øu 1': 31, 'NƒÉng khi·∫øu 2': 32, 'NƒÉng khi·∫øu Gi√°o d·ª•c m·∫ßm non': 33, 'NƒÉng khi·∫øu SKƒêA 1': 34, 'NƒÉng khi·∫øu SKƒêA 2': 35, 'NƒÉng khi·∫øu TDTT': 36, 'NƒÉng khi·∫øu Th·ªÉ d·ª•c Th·ªÉ thao': 37, 'NƒÉng khi·∫øu Th·ªÉ d·ª•c th·ªÉ thao': 38, 'NƒÉng khi·∫øu b√°o ch√≠': 39, 'NƒÉng khi·∫øu th·ªÉ d·ª•c th·ªÉ thao': 40, 'NƒÉng khi·∫øu vƒÉn h√≥a ngh·ªá thu·∫≠t': 41, 'NƒÉng khi·∫øu v·∫Ω Ngh·ªá thu·∫≠t 1': 42, 'NƒÉng khi·∫øu v·∫Ω Ngh·ªá thu·∫≠t 2': 43, 'NƒÉng khi·∫øu √Çm nh·∫°c 1': 44, 'NƒÉng khi·∫øu √Çm nh·∫°c 2': 45, 'NƒÉng khi·∫øu ƒë·ªçc di·ªÖn c·∫£m v√† H√°t': 46, 'Sinh': 47, 'Sinh h·ªçc': 48, 'Tin': 49, 'Tin h·ªçc': 50, 'Ti√™ÃÅng Anh': 51, 'Ti·∫øng Anh': 52, 'Ti·∫øng Anh,V·∫Ω m·ªπ thu·∫≠t': 53, 'Ti·∫øng H√†n': 54, 'Ti·∫øng Nga': 55, 'Ti·∫øng Nh·∫≠t': 56, 'Ti·∫øng Ph√°p': 57, 'Ti·∫øng Trung': 58, 'Ti·∫øng ƒê·ª©c': 59, 'ToaÃÅn': 60, 'To√°n': 61, 'Trang tr√≠': 62, 'VƒÉn': 63, 'V·∫≠t l√≠': 64, 'V·∫≠t l√Ω': 65, 'V·∫Ω H√¨nh h·ªça m·ªπ thu·∫≠t': 66, 'V·∫Ω M·ªπ thu·∫≠t': 67, 'V·∫Ω NƒÉng khi·∫øu': 68, 'V·∫Ω m·ªπ thu·∫≠t': 69, 'V·∫Ω trang tr√≠ m√†u': 70, 'X√¢y d·ª±ng k·ªãch b·∫£n s·ª± ki·ªán': 71, 'bi·ªÉu di·ªÖn ngh·ªá thu·∫≠t': 72, 'di·ªÖn c·∫£m)': 73, 'nƒÉng khi·∫øu': 74, 'nƒÉng khi·∫øu th·ªÉ d·ª•c th·ªÉ thao': 75, 'nƒÉng ki·∫øu b√°o ch√≠': 76, 'ti·∫øng Nga': 77, 'ti·∫øng Nh·∫≠t': 78, 'ti·∫øng Ph√°p': 79, 'ti·∫øng ƒê·ª©c': 80, 'x∆∞·ªõng √¢m': 81, 'ƒêiÃ£a': 82, 'ƒê·ªãa': 83, 'ƒê·ªãa l√≠': 84, 'ƒê·ªãa l√Ω': 85, 'ƒê·ªçc di·ªÖn c·∫£m': 86, 'ƒê·ªçc k·ªÉ di·ªÖn c·∫£m': 87, 'ƒë·ªçc': 88}\n",
      "  ‚úì S·ªë l∆∞·ª£ng kh·ªëi thi: 212\n",
      "\n",
      "  üìã Chi ti·∫øt c√°c kh·ªëi thi:\n",
      "     - 160: ['To√°n', 'H√≥a h·ªçc', 'Ti·∫øng H√†n']\n",
      "     - 108: ['Ng·ªØ vƒÉn', 'GDKTPL', 'Ti·∫øng Trung']\n",
      "     - 5: ['To√°n', 'H√≥a h·ªçc', 'Sinh h·ªçc']\n",
      "     - 114: ['To√°n', 'Ti·∫øng Trung', 'V·∫Ω m·ªπ thu·∫≠t']\n",
      "     - 84: ['To√°n', 'L√Ω', 'NƒÉng khi·∫øu TDTT']\n",
      "     ... v√† 207 kh·ªëi thi kh√°c\n",
      "\n",
      "[B∆∞·ªõc 3] Xem m·∫´u d·ªØ li·ªáu th√≠ sinh tr∆∞·ªõc khi x·ª≠ l√Ω...\n",
      "\n",
      "  üìä M·∫´u ƒëi·ªÉm c·ªßa th√≠ sinh (5 th√≠ sinh ƒë·∫ßu):\n",
      "+------------+--------+----+----------------------------------------------+\n",
      "|studentId   |regionId|year|scores                                        |\n",
      "+------------+--------+----+----------------------------------------------+\n",
      "|010000182021|01      |2021|{10 -> 5.0, 61 -> 8.8, 63 -> 8.25, 47 -> 6.75}|\n",
      "|010000352021|01      |2021|{83 -> 5.0, 61 -> 5.0, 63 -> 3.75}            |\n",
      "|010000662021|01      |2021|{83 -> 5.5, 61 -> 4.2, 63 -> 6.25}            |\n",
      "|010000782021|01      |2021|{83 -> 6.5, 61 -> 3.2, 22 -> 9.8, 63 -> 6.75} |\n",
      "|010000852021|01      |2021|{83 -> 8.5, 61 -> 8.4, 22 -> 8.6, 63 -> 8.25} |\n",
      "+------------+--------+----+----------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "[B∆∞·ªõc 4] ƒê·ªãnh nghƒ©a UDF t√≠nh ƒëi·ªÉm t·ªïng theo kh·ªëi thi (c√≥ debug logging)...\n",
      "  ‚úì UDF ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a v·ªõi debug logging\n",
      "\n",
      "[B∆∞·ªõc 5] Test UDF v·ªõi 1 th√≠ sinh m·∫´u...\n",
      "\n",
      "  üîç L·∫•y 1 th√≠ sinh ƒë·ªÉ test:\n",
      "\n",
      "  üìù Th√≠ sinh: 010000782021\n",
      "     Khu v·ª±c: 01\n",
      "     NƒÉm: 2021\n",
      "\n",
      "  üìä ƒêi·ªÉm c·ªßa th√≠ sinh:\n",
      "     - ƒê·ªãa: 6.5\n",
      "     - To√°n: 3.2\n",
      "     - Ngo·∫°i ng·ªØ: 9.8\n",
      "     - VƒÉn: 6.75\n",
      "\n",
      "  üéØ C√°c kh·ªëi thi th√≠ sinh C√ì TH·ªÇ x√©t:\n",
      "\n",
      "  üìå T·ªïng c·ªông: 0/212 kh·ªëi thi\n",
      "\n",
      "================================================================================\n",
      "[B∆∞·ªõc 6] √Åp d·ª•ng UDF cho T·∫§T C·∫¢ th√≠ sinh...\n",
      "  ‚è≥ ƒêang x·ª≠ l√Ω... (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)\n",
      "  üí° L∆∞u √Ω: Debug log s·∫Ω xu·∫•t hi·ªán tr√™n Spark executors, kh√¥ng ph·∫£i ·ªü ƒë√¢y\n",
      "================================================================================\n",
      "\n",
      "  ‚úì Ho√†n th√†nh!\n",
      "\n",
      "[B∆∞·ªõc 7] Ph√¢n t√≠ch k·∫øt qu·∫£:\n",
      "\n",
      "  üìä Xem m·∫´u k·∫øt qu·∫£ (5 th√≠ sinh):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+----+--------------------+\n",
      "|studentId   |regionId|year|subject_group_scores|\n",
      "+------------+--------+----+--------------------+\n",
      "|010000182021|01      |2021|{}                  |\n",
      "|010000352021|01      |2021|{}                  |\n",
      "|010000662021|01      |2021|{}                  |\n",
      "|010000782021|01      |2021|{}                  |\n",
      "|010000852021|01      |2021|{}                  |\n",
      "+------------+--------+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "  üìà Th·ªëng k√™ s·ªë l∆∞·ª£ng kh·ªëi thi m√† m·ªói th√≠ sinh c√≥ th·ªÉ x√©t:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|num_eligible_groups|s·ªë_th√≠_sinh|\n",
      "+-------------------+-----------+\n",
      "|                  0|     985353|\n",
      "+-------------------+-----------+\n",
      "\n",
      "\n",
      "  üìä Th·ªëng k√™ t·ªïng quan:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 140:>                                                        (0 + 4) / 4]"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PH·∫¶N 1: T√çNH TO√ÅN T·ªî H·ª¢P KH·ªêI THI CHO M·ªñI TH√ç SINH (C√ì DEBUG LOG)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==================== ƒê·ªåC D·ªÆ LI·ªÜU ====================\n",
    "print(\"\\n[B∆∞·ªõc 1] ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver layer...\")\n",
    "df_student_scores = spark.table(\"nessie.silver_tables.student_scores\")\n",
    "df_subject_group = spark.table(\"nessie.silver_tables.subject_group\")\n",
    "df_subject = spark.table(\"nessie.silver_tables.subject\")\n",
    "\n",
    "print(f\"  ‚úì S·ªë l∆∞·ª£ng th√≠ sinh: {df_student_scores.count():,}\")\n",
    "print(f\"  ‚úì S·ªë l∆∞·ª£ng kh·ªëi thi: {df_subject_group.count()}\")\n",
    "print(f\"  ‚úì S·ªë l∆∞·ª£ng m√¥n h·ªçc: {df_subject.count()}\")\n",
    "\n",
    "# ==================== T·∫†O MAPPING DICTIONARIES ====================\n",
    "print(\"\\n[B∆∞·ªõc 2] T·∫°o mapping dictionaries...\")\n",
    "\n",
    "# Mapping: T√™n m√¥n ‚Üí ID m√¥n (VD: \"To√°n\" ‚Üí 1)\n",
    "subject_name_to_id = {row['subjectName']: row['subjectId'] for row in df_subject.collect()}\n",
    "print(f\"  ‚úì Subject mapping: {subject_name_to_id}\")\n",
    "\n",
    "# Mapping: ID kh·ªëi ‚Üí Danh s√°ch m√¥n (VD: \"A00\" ‚Üí [\"To√°n\", \"L√Ω\", \"H√≥a\"])\n",
    "subject_groups_dict = {}\n",
    "for row in df_subject_group.collect():\n",
    "    group_id = row['subjectGroupId']\n",
    "    subjects = [s.strip() for s in row['subjectCombination'].split('-')]\n",
    "    subject_groups_dict[group_id] = subjects\n",
    "\n",
    "print(f\"  ‚úì S·ªë l∆∞·ª£ng kh·ªëi thi: {len(subject_groups_dict)}\")\n",
    "print(f\"\\n  üìã Chi ti·∫øt c√°c kh·ªëi thi:\")\n",
    "for group_id, subjects in list(subject_groups_dict.items())[:5]:\n",
    "    print(f\"     - {group_id}: {subjects}\")\n",
    "print(f\"     ... v√† {len(subject_groups_dict) - 5} kh·ªëi thi kh√°c\")\n",
    "\n",
    "# Broadcast ƒë·ªÉ worker nodes truy c·∫≠p nhanh\n",
    "from pyspark.sql.types import FloatType, MapType, StringType\n",
    "subject_map_broadcast = spark.sparkContext.broadcast(subject_name_to_id)\n",
    "subject_groups_broadcast = spark.sparkContext.broadcast(subject_groups_dict)\n",
    "\n",
    "# ==================== XEM M·∫™U D·ªÆ LI·ªÜU TH√ç SINH ====================\n",
    "print(\"\\n[B∆∞·ªõc 3] Xem m·∫´u d·ªØ li·ªáu th√≠ sinh tr∆∞·ªõc khi x·ª≠ l√Ω...\")\n",
    "print(\"\\n  üìä M·∫´u ƒëi·ªÉm c·ªßa th√≠ sinh (5 th√≠ sinh ƒë·∫ßu):\")\n",
    "df_student_scores.select(\"studentId\", \"regionId\", \"year\", \"scores\").show(5, truncate=False)\n",
    "\n",
    "# ==================== UDF T√çNH ƒêI·ªÇM T·ªîNG THEO KH·ªêI (C√ì DEBUG) ====================\n",
    "print(\"\\n[B∆∞·ªõc 4] ƒê·ªãnh nghƒ©a UDF t√≠nh ƒëi·ªÉm t·ªïng theo kh·ªëi thi (c√≥ debug logging)...\")\n",
    "\n",
    "# Bi·∫øn global ƒë·ªÉ ƒë·∫øm s·ªë l·∫ßn g·ªçi UDF (ch·ªâ d√πng ƒë·ªÉ demo)\n",
    "import sys\n",
    "\n",
    "def create_subject_group_scores_map_debug(scores_map):\n",
    "    \"\"\"\n",
    "    UDF ch√≠nh v·ªõi DEBUG LOGGING: T√≠nh ƒëi·ªÉm t·ªïng cho T·∫§T C·∫¢ c√°c kh·ªëi thi m√† th√≠ sinh c√≥ ƒë·ªß m√¥n\n",
    "    \n",
    "    Input:  scores_map = Map<subjectId, score> c·ªßa 1 th√≠ sinh\n",
    "    Output: Map<subjectGroupId, totalScore> cho c√°c kh·ªëi th√≠ sinh c√≥ ƒë·ªß m√¥n\n",
    "    \"\"\"\n",
    "    if scores_map is None:\n",
    "        return {}\n",
    "    \n",
    "    subject_name_to_id_map = subject_map_broadcast.value\n",
    "    subject_id_to_name_map = {v: k for k, v in subject_name_to_id_map.items()}\n",
    "    subject_groups = subject_groups_broadcast.value\n",
    "    \n",
    "    # DEBUG: In ra ƒëi·ªÉm c·ªßa th√≠ sinh (ch·ªâ in 3 th√≠ sinh ƒë·∫ßu ƒë·ªÉ tr√°nh spam log)\n",
    "    student_subjects = {subject_id_to_name_map.get(sid, f\"ID_{sid}\"): score \n",
    "                       for sid, score in scores_map.items()}\n",
    "    \n",
    "    result = {}\n",
    "    groups_eligible = []\n",
    "    groups_not_eligible = []\n",
    "    \n",
    "    # V·ªõi m·ªói kh·ªëi thi, ki·ªÉm tra xem th√≠ sinh c√≥ ƒë·ªß m√¥n kh√¥ng\n",
    "    for group_id, required_subjects in subject_groups.items():\n",
    "        total_score = 0.0\n",
    "        has_all_subjects = True\n",
    "        missing_subjects = []\n",
    "        \n",
    "        # Ki·ªÉm tra t·ª´ng m√¥n trong kh·ªëi\n",
    "        for subject_name in required_subjects:\n",
    "            subject_id = subject_name_to_id_map.get(subject_name)\n",
    "            \n",
    "            # N·∫øu thi·∫øu m√¥n ho·∫∑c kh√¥ng c√≥ ƒëi·ªÉm ‚Üí B·ªè qua kh·ªëi n√†y\n",
    "            if subject_id is None or subject_id not in scores_map:\n",
    "                has_all_subjects = False\n",
    "                missing_subjects.append(subject_name)\n",
    "                break\n",
    "            \n",
    "            # C·ªông d·ªìn ƒëi·ªÉm\n",
    "            total_score += scores_map[subject_id]\n",
    "        \n",
    "        # Ch·ªâ th√™m v√†o result n·∫øu th√≠ sinh c√≥ ƒë·ªß T·∫§T C·∫¢ c√°c m√¥n trong kh·ªëi\n",
    "        if has_all_subjects:\n",
    "            result[group_id] = total_score\n",
    "            groups_eligible.append(f\"{group_id}={total_score:.1f}\")\n",
    "        else:\n",
    "            groups_not_eligible.append(f\"{group_id}(thi·∫øu {','.join(missing_subjects)})\")\n",
    "    \n",
    "    # DEBUG: In th√¥ng tin (ch·ªâ in cho 3 th√≠ sinh ƒë·∫ßu)\n",
    "    # L∆∞u √Ω: Trong m√¥i tr∆∞·ªùng distributed, log n√†y s·∫Ω xu·∫•t hi·ªán tr√™n executor nodes\n",
    "    if len(result) > 0:\n",
    "        # Ch·ªâ in log cho m·ªôt s·ªë th√≠ sinh ƒë·ªÉ minh h·ªça\n",
    "        pass  # Comment out ƒë·ªÉ tr√°nh qu√° nhi·ªÅu log trong production\n",
    "    \n",
    "    return result\n",
    "\n",
    "create_group_scores_udf = udf(create_subject_group_scores_map_debug, MapType(StringType(), FloatType()))\n",
    "print(\"  ‚úì UDF ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a v·ªõi debug logging\")\n",
    "\n",
    "# ==================== TEST UDF V·ªöI M·ªòT TH√ç SINH ====================\n",
    "print(\"\\n[B∆∞·ªõc 5] Test UDF v·ªõi 1 th√≠ sinh m·∫´u...\")\n",
    "print(\"\\n  üîç L·∫•y 1 th√≠ sinh ƒë·ªÉ test:\")\n",
    "\n",
    "# L·∫•y 1 th√≠ sinh ƒë·∫ßu ti√™n\n",
    "test_student = df_student_scores.limit(4).collect()[3]\n",
    "test_scores = test_student['scores']\n",
    "\n",
    "print(f\"\\n  üìù Th√≠ sinh: {test_student['studentId']}\")\n",
    "print(f\"     Khu v·ª±c: {test_student['regionId']}\")\n",
    "print(f\"     NƒÉm: {test_student['year']}\")\n",
    "print(f\"\\n  üìä ƒêi·ªÉm c·ªßa th√≠ sinh:\")\n",
    "\n",
    "# Convert scores map th√†nh d·∫°ng d·ªÖ ƒë·ªçc\n",
    "subject_id_to_name = {v: k for k, v in subject_name_to_id.items()}\n",
    "for subject_id, score in test_scores.items():\n",
    "    subject_name = subject_id_to_name.get(subject_id, f\"Unknown_{subject_id}\")\n",
    "    print(f\"     - {subject_name}: {score}\")\n",
    "\n",
    "# T√≠nh to√°n th·ªß c√¥ng c√°c kh·ªëi thi m√† th√≠ sinh ƒë·ªß ƒëi·ªÅu ki·ªán\n",
    "print(f\"\\n  üéØ C√°c kh·ªëi thi th√≠ sinh C√ì TH·ªÇ x√©t:\")\n",
    "eligible_groups = []\n",
    "for group_id, required_subjects in subject_groups_dict.items():\n",
    "    has_all = True\n",
    "    total = 0.0\n",
    "    for subject_name in required_subjects:\n",
    "        subject_id = subject_name_to_id.get(subject_name)\n",
    "        if subject_id is None or subject_id not in test_scores:\n",
    "            has_all = False\n",
    "            break\n",
    "        total += test_scores[subject_id]\n",
    "    \n",
    "    if has_all:\n",
    "        eligible_groups.append((group_id, required_subjects, total))\n",
    "        print(f\"     ‚úÖ {group_id} ({'-'.join(required_subjects)}): {total:.1f} ƒëi·ªÉm\")\n",
    "\n",
    "print(f\"\\n  üìå T·ªïng c·ªông: {len(eligible_groups)}/{len(subject_groups_dict)} kh·ªëi thi\")\n",
    "\n",
    "# ==================== √ÅP D·ª§NG UDF ====================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[B∆∞·ªõc 6] √Åp d·ª•ng UDF cho T·∫§T C·∫¢ th√≠ sinh...\")\n",
    "print(\"  ‚è≥ ƒêang x·ª≠ l√Ω... (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)\")\n",
    "print(\"  üí° L∆∞u √Ω: Debug log s·∫Ω xu·∫•t hi·ªán tr√™n Spark executors, kh√¥ng ph·∫£i ·ªü ƒë√¢y\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_student_with_group_scores = df_student_scores.withColumn(\n",
    "    \"subject_group_scores\",  # C·ªôt m·ªõi: Map<subjectGroupId, totalScore>\n",
    "    create_group_scores_udf(col(\"scores\"))\n",
    ")\n",
    "\n",
    "print(\"\\n  ‚úì Ho√†n th√†nh!\\n\")\n",
    "\n",
    "# ==================== PH√ÇN T√çCH K·∫æT QU·∫¢ ====================\n",
    "print(\"[B∆∞·ªõc 7] Ph√¢n t√≠ch k·∫øt qu·∫£:\")\n",
    "print(\"\\n  üìä Xem m·∫´u k·∫øt qu·∫£ (5 th√≠ sinh):\")\n",
    "df_student_with_group_scores.select(\n",
    "    \"studentId\", \n",
    "    \"regionId\", \n",
    "    \"year\", \n",
    "    \"subject_group_scores\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "# T√≠nh th·ªëng k√™\n",
    "print(\"\\n  üìà Th·ªëng k√™ s·ªë l∆∞·ª£ng kh·ªëi thi m√† m·ªói th√≠ sinh c√≥ th·ªÉ x√©t:\")\n",
    "\n",
    "# ƒê·∫øm s·ªë l∆∞·ª£ng kh·ªëi thi cho m·ªói th√≠ sinh\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "df_stats = df_student_with_group_scores.withColumn(\n",
    "    \"num_eligible_groups\", \n",
    "    size(col(\"subject_group_scores\"))\n",
    ")\n",
    "\n",
    "df_stats.groupBy(\"num_eligible_groups\") \\\n",
    "    .agg(count(\"*\").alias(\"s·ªë_th√≠_sinh\")) \\\n",
    "    .orderBy(\"num_eligible_groups\") \\\n",
    "    .show(20)\n",
    "\n",
    "print(\"\\n  üìä Th·ªëng k√™ t·ªïng quan:\")\n",
    "avg_groups = df_stats.agg(avg(\"num_eligible_groups\").alias(\"avg\")).collect()[0][\"avg\"]\n",
    "max_groups = df_stats.agg(max(\"num_eligible_groups\").alias(\"max\")).collect()[0][\"max\"]\n",
    "min_groups = df_stats.agg(min(\"num_eligible_groups\").alias(\"min\")).collect()[0][\"min\"]\n",
    "\n",
    "print(f\"     - S·ªë kh·ªëi TB m·ªói th√≠ sinh c√≥ th·ªÉ x√©t: {avg_groups:.2f}\")\n",
    "print(f\"     - S·ªë kh·ªëi t·ªëi ƒëa: {max_groups}\")\n",
    "print(f\"     - S·ªë kh·ªëi t·ªëi thi·ªÉu: {min_groups}\")\n",
    "print(f\"     - T·ªïng s·ªë kh·ªëi thi trong h·ªá th·ªëng: {len(subject_groups_dict)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ HO√ÄN TH√ÄNH PH·∫¶N 1: DataFrame 'df_student_with_group_scores' ƒë√£ s·∫µn s√†ng\")\n",
    "print(\"   v·ªõi th√¥ng tin chi ti·∫øt v·ªÅ c√°c kh·ªëi thi m√† m·ªói th√≠ sinh c√≥ th·ªÉ x√©t\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2a32fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=\" * 80)\n",
    "# print(\"PH·∫¶N 1: T√çNH TO√ÅN T·ªî H·ª¢P KH·ªêI THI CHO M·ªñI TH√ç SINH\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# # ==================== ƒê·ªåC D·ªÆ LI·ªÜU ====================\n",
    "# print(\"\\n[B∆∞·ªõc 1] ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver layer...\")\n",
    "# df_student_scores = spark.table(\"nessie.silver_tables.student_scores\")\n",
    "# df_subject_group = spark.table(\"nessie.silver_tables.subject_group\")\n",
    "# df_subject = spark.table(\"nessie.silver_tables.subject\")\n",
    "\n",
    "# print(f\"  ‚úì S·ªë l∆∞·ª£ng th√≠ sinh: {df_student_scores.count():,}\")\n",
    "# print(f\"  ‚úì S·ªë l∆∞·ª£ng kh·ªëi thi: {df_subject_group.count()}\")\n",
    "# print(f\"  ‚úì S·ªë l∆∞·ª£ng m√¥n h·ªçc: {df_subject.count()}\")\n",
    "\n",
    "# # ==================== T·∫†O MAPPING DICTIONARIES ====================\n",
    "# print(\"\\n[B∆∞·ªõc 2] T·∫°o mapping dictionaries...\")\n",
    "\n",
    "# # Mapping: T√™n m√¥n ‚Üí ID m√¥n (VD: \"To√°n\" ‚Üí 1)\n",
    "# subject_name_to_id = {row['subjectName']: row['subjectId'] for row in df_subject.collect()}\n",
    "# print(f\"  ‚úì Subject mapping: {subject_name_to_id}\")\n",
    "\n",
    "# # Mapping: ID kh·ªëi ‚Üí Danh s√°ch m√¥n (VD: \"A00\" ‚Üí [\"To√°n\", \"L√Ω\", \"H√≥a\"])\n",
    "# subject_groups_dict = {}\n",
    "# for row in df_subject_group.collect():\n",
    "#     group_id = row['subjectGroupId']\n",
    "#     subjects = [s.strip() for s in row['subjectCombination'].split('-')]\n",
    "#     subject_groups_dict[group_id] = subjects\n",
    "\n",
    "# print(f\"  ‚úì S·ªë l∆∞·ª£ng kh·ªëi thi: {len(subject_groups_dict)}\")\n",
    "# print(f\"  ‚úì V√≠ d·ª• kh·ªëi thi: {dict(list(subject_groups_dict.items())[:3])}\")\n",
    "\n",
    "# # Broadcast ƒë·ªÉ worker nodes truy c·∫≠p nhanh\n",
    "# from pyspark.sql.types import FloatType, MapType, StringType\n",
    "# subject_map_broadcast = spark.sparkContext.broadcast(subject_name_to_id)\n",
    "# subject_groups_broadcast = spark.sparkContext.broadcast(subject_groups_dict)\n",
    "\n",
    "# # ==================== UDF T√çNH ƒêI·ªÇM T·ªîNG THEO KH·ªêI ====================\n",
    "# print(\"\\n[B∆∞·ªõc 3] ƒê·ªãnh nghƒ©a UDF t√≠nh ƒëi·ªÉm t·ªïng theo kh·ªëi thi...\")\n",
    "\n",
    "# def create_subject_group_scores_map(scores_map):\n",
    "#     \"\"\"\n",
    "#     UDF ch√≠nh: T√≠nh ƒëi·ªÉm t·ªïng cho T·∫§T C·∫¢ c√°c kh·ªëi thi m√† th√≠ sinh c√≥ ƒë·ªß m√¥n\n",
    "    \n",
    "#     Input:  scores_map = Map<subjectId, score> c·ªßa 1 th√≠ sinh\n",
    "#             VD: {1: 9.0, 2: 8.5, 3: 7.0, 4: 6.0}\n",
    "#                 (To√°n: 9, L√Ω: 8.5, H√≥a: 7, Sinh: 6)\n",
    "    \n",
    "#     Output: Map<subjectGroupId, totalScore> cho c√°c kh·ªëi th√≠ sinh c√≥ ƒë·ªß m√¥n\n",
    "#             VD: {\"A00\": 24.5, \"B00\": 22.0}\n",
    "    \n",
    "#     Logic:\n",
    "#     - Duy·ªát qua T·∫§T C·∫¢ c√°c kh·ªëi thi\n",
    "#     - V·ªõi m·ªói kh·ªëi, ki·ªÉm tra xem th√≠ sinh c√≥ ƒë·ªß ƒëi·ªÉm c√°c m√¥n trong kh·ªëi kh√¥ng\n",
    "#     - N·∫øu ƒë·ªß ‚Üí C·ªông t·ªïng ƒëi·ªÉm c√°c m√¥n ‚Üí L∆∞u v√†o result\n",
    "#     - N·∫øu thi·∫øu b·∫•t k·ª≥ m√¥n n√†o ‚Üí B·ªè qua kh·ªëi ƒë√≥\n",
    "#     \"\"\"\n",
    "#     if scores_map is None:\n",
    "#         return {}\n",
    "    \n",
    "#     subject_name_to_id_map = subject_map_broadcast.value\n",
    "#     subject_id_to_name_map = {v: k for k, v in subject_name_to_id_map.items()}\n",
    "#     subject_groups = subject_groups_broadcast.value\n",
    "    \n",
    "#     result = {}\n",
    "    \n",
    "#     # V·ªõi m·ªói kh·ªëi thi, ki·ªÉm tra xem th√≠ sinh c√≥ ƒë·ªß m√¥n kh√¥ng\n",
    "#     for group_id, required_subjects in subject_groups.items():\n",
    "#         total_score = 0.0\n",
    "#         has_all_subjects = True\n",
    "        \n",
    "#         # Ki·ªÉm tra t·ª´ng m√¥n trong kh·ªëi\n",
    "#         for subject_name in required_subjects:\n",
    "#             subject_id = subject_name_to_id_map.get(subject_name)\n",
    "            \n",
    "#             # N·∫øu thi·∫øu m√¥n ho·∫∑c kh√¥ng c√≥ ƒëi·ªÉm ‚Üí B·ªè qua kh·ªëi n√†y\n",
    "#             if subject_id is None or subject_id not in scores_map:\n",
    "#                 has_all_subjects = False\n",
    "#                 break\n",
    "            \n",
    "#             # C·ªông d·ªìn ƒëi·ªÉm\n",
    "#             total_score += scores_map[subject_id]\n",
    "        \n",
    "#         # Ch·ªâ th√™m v√†o result n·∫øu th√≠ sinh c√≥ ƒë·ªß T·∫§T C·∫¢ c√°c m√¥n trong kh·ªëi\n",
    "#         if has_all_subjects:\n",
    "#             result[group_id] = total_score\n",
    "    \n",
    "#     return result\n",
    "\n",
    "# create_group_scores_udf = udf(create_subject_group_scores_map, MapType(StringType(), FloatType()))\n",
    "# print(\"  ‚úì UDF ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")\n",
    "\n",
    "# # ==================== √ÅP D·ª§NG UDF ====================\n",
    "# print(\"\\n[B∆∞·ªõc 4] √Åp d·ª•ng UDF ƒë·ªÉ t√≠nh ƒëi·ªÉm t·ªïng cho t·ª´ng th√≠ sinh...\")\n",
    "# print(\"  ‚è≥ ƒêang x·ª≠ l√Ω... (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)\")\n",
    "\n",
    "# df_student_with_group_scores = df_student_scores.withColumn(\n",
    "#     \"subject_group_scores\",  # C·ªôt m·ªõi: Map<subjectGroupId, totalScore>\n",
    "#     create_group_scores_udf(col(\"scores\"))\n",
    "# )\n",
    "\n",
    "# print(\"  ‚úì Ho√†n th√†nh!\\n\")\n",
    "\n",
    "# # ==================== HI·ªÇN TH·ªä M·∫™U ====================\n",
    "# print(\"[B∆∞·ªõc 5] Xem k·∫øt qu·∫£ m·∫´u:\")\n",
    "# print(\"\\nC·∫•u tr√∫c: M·ªói th√≠ sinh c√≥ Map ch·ª©a t·∫•t c·∫£ kh·ªëi thi m√† h·ªç c√≥ th·ªÉ x√©t:\")\n",
    "# df_student_with_group_scores.select(\n",
    "#     \"studentId\", \n",
    "#     \"regionId\", \n",
    "#     \"year\", \n",
    "#     \"subject_group_scores\"\n",
    "# ).show(5, truncate=False)\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 80)\n",
    "# print(\"‚úÖ HO√ÄN TH√ÄNH PH·∫¶N 1: DataFrame 'df_student_with_group_scores' ƒë√£ s·∫µn s√†ng\")\n",
    "# print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee402c",
   "metadata": {},
   "source": [
    "#### üìä PH·∫¶N 2: X·ª≠ l√Ω v√† t·∫°o Fact Table t·ª´ d·ªØ li·ªáu ƒë√£ t√≠nh to√°n\n",
    "\n",
    "**M·ª•c ti√™u:** Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu t·ªï h·ª£p kh·ªëi thi th√†nh Fact table v·ªõi ph√¢n b·ªë ƒëi·ªÉm theo kho·∫£ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1fff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PH·∫¶N 2: X·ª¨ L√ù V√Ä T·∫†O FACT TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==================== EXPLODE MAP TH√ÄNH ROWS ====================\n",
    "print(\"\\n[B∆∞·ªõc 1] Explode Map th√†nh rows...\")\n",
    "print(\"  Chuy·ªÉn t·ª´: 1 row/th√≠ sinh ‚Üí nhi·ªÅu rows (1 row cho m·ªói kh·ªëi thi)\")\n",
    "\n",
    "df_scores_exploded = df_student_with_group_scores.select(\n",
    "    \"regionId\",\n",
    "    \"year\",\n",
    "    explode(\"subject_group_scores\").alias(\"subjectGroupId\", \"totalScore\")\n",
    ")\n",
    "\n",
    "print(f\"  ‚úì T·ªïng s·ªë d√≤ng sau explode: {df_scores_exploded.count():,}\")\n",
    "print(\"\\nM·∫´u d·ªØ li·ªáu sau explode:\")\n",
    "df_scores_exploded.show(10)\n",
    "\n",
    "# ==================== PH√ÇN LO·∫†I ƒêI·ªÇM V√ÄO KHO·∫¢NG ====================\n",
    "print(\"\\n[B∆∞·ªõc 2] Ph√¢n lo·∫°i ƒëi·ªÉm v√†o c√°c kho·∫£ng...\")\n",
    "print(\"  L√†m tr√≤n xu·ªëng: 24.5 ‚Üí 24.0, 22.8 ‚Üí 22.0\")\n",
    "print(\"  T·∫°o kho·∫£ng: [0-1), [1-2), [2-3), ..., [29-30)\")\n",
    "\n",
    "df_scores_exploded = df_scores_exploded.withColumn(\n",
    "    \"scoreRange\",\n",
    "    floor(col(\"totalScore\")).cast(\"float\")\n",
    ")\n",
    "\n",
    "print(\"  ‚úì Ho√†n th√†nh ph√¢n lo·∫°i ƒëi·ªÉm\")\n",
    "\n",
    "# ==================== ƒê·∫æM S·ªê L∆Ø·ª¢NG H·ªåC SINH ====================\n",
    "print(\"\\n[B∆∞·ªõc 3] ƒê·∫øm s·ªë l∆∞·ª£ng h·ªçc sinh theo t·ª´ng nh√≥m...\")\n",
    "print(\"  GROUP BY: regionId, subjectGroupId, year, scoreRange\")\n",
    "\n",
    "df_distribution = df_scores_exploded.groupBy(\n",
    "    \"regionId\",\n",
    "    \"subjectGroupId\",\n",
    "    \"year\",\n",
    "    \"scoreRange\"\n",
    ").agg(\n",
    "    count(\"*\").alias(\"quantity\")\n",
    ")\n",
    "\n",
    "print(f\"  ‚úì S·ªë d√≤ng distribution: {df_distribution.count():,}\")\n",
    "print(\"\\nM·∫´u distribution:\")\n",
    "df_distribution.orderBy(desc(\"quantity\")).show(10)\n",
    "\n",
    "# ==================== MAPPING SANG SURROGATE KEYS ====================\n",
    "print(\"\\n[B∆∞·ªõc 4] Mapping natural keys sang surrogate keys...\")\n",
    "print(\"  S·ª≠ d·ª•ng Dictionary lookup (nhanh h∆°n JOIN)\")\n",
    "\n",
    "# ƒê·ªçc dimension tables\n",
    "df_dim_time_gold = spark.table(\"nessie.gold_tables.dim_time\")\n",
    "df_dim_region_gold = spark.table(\"nessie.gold_tables.dim_region\")\n",
    "df_dim_subject_group_gold = spark.table(\"nessie.gold_tables.dim_subject_group\")\n",
    "\n",
    "# T·∫°o mapping dictionaries\n",
    "region_id_to_key = {row['regionId']: row['regionKey'] \n",
    "                    for row in df_dim_region_gold.select(\"regionId\", \"regionKey\").collect()}\n",
    "subject_group_id_to_key = {row['subjectGroupId']: row['subjectGroupKey'] \n",
    "                           for row in df_dim_subject_group_gold.select(\"subjectGroupId\", \"subjectGroupKey\").collect()}\n",
    "year_to_time_key = {row['year']: row['timeKey'] \n",
    "                    for row in df_dim_time_gold.select(\"year\", \"timeKey\").collect()}\n",
    "\n",
    "print(f\"  ‚úì Region mappings: {len(region_id_to_key)}\")\n",
    "print(f\"  ‚úì Subject Group mappings: {len(subject_group_id_to_key)}\")\n",
    "print(f\"  ‚úì Time mappings: {len(year_to_time_key)}\")\n",
    "\n",
    "# Broadcast mappings\n",
    "region_map_bc = spark.sparkContext.broadcast(region_id_to_key)\n",
    "subject_group_map_bc = spark.sparkContext.broadcast(subject_group_id_to_key)\n",
    "year_map_bc = spark.sparkContext.broadcast(year_to_time_key)\n",
    "\n",
    "# UDF ƒë·ªÉ map keys\n",
    "def map_to_keys(regionId, subjectGroupId, year):\n",
    "    region_key = region_map_bc.value.get(regionId)\n",
    "    subject_group_key = subject_group_map_bc.value.get(subjectGroupId)\n",
    "    time_key = year_map_bc.value.get(year)\n",
    "    return (region_key, subject_group_key, time_key)\n",
    "\n",
    "map_keys_udf = udf(map_to_keys, StructType([\n",
    "    StructField(\"regionKey\", IntegerType(), True),\n",
    "    StructField(\"subjectGroupKey\", IntegerType(), True),\n",
    "    StructField(\"timeKey\", IntegerType(), True)\n",
    "]))\n",
    "\n",
    "print(\"\\n[B∆∞·ªõc 5] √Åp d·ª•ng mapping...\")\n",
    "\n",
    "# √Åp d·ª•ng mapping\n",
    "df_with_keys = df_distribution.withColumn(\n",
    "    \"keys\",\n",
    "    map_keys_udf(col(\"regionId\"), col(\"subjectGroupId\"), col(\"year\"))\n",
    ")\n",
    "\n",
    "# Extract keys t·ª´ struct\n",
    "df_with_keys = df_with_keys.withColumn(\"regionKey\", col(\"keys.regionKey\")) \\\n",
    "    .withColumn(\"subjectGroupKey\", col(\"keys.subjectGroupKey\")) \\\n",
    "    .withColumn(\"timeKey\", col(\"keys.timeKey\")) \\\n",
    "    .drop(\"keys\", \"regionId\", \"subjectGroupId\", \"year\")\n",
    "\n",
    "print(\"  ‚úì Mapping ho√†n th√†nh\")\n",
    "\n",
    "# ==================== T·∫†O SURROGATE KEY ====================\n",
    "print(\"\\n[B∆∞·ªõc 6] T·∫°o primary key (SDBSGKey)...\")\n",
    "\n",
    "window_spec = Window.orderBy(\"timeKey\", \"regionKey\", \"subjectGroupKey\", \"scoreRange\")\n",
    "df_with_keys = df_with_keys.withColumn(\"SDBSGKey\", row_number().over(window_spec))\n",
    "\n",
    "print(\"  ‚úì Primary key ƒë√£ ƒë∆∞·ª£c t·∫°o\")\n",
    "\n",
    "# ==================== SELECT C√ÅC C·ªòT CU·ªêI C√ôNG ====================\n",
    "print(\"\\n[B∆∞·ªõc 7] Chu·∫©n b·ªã schema cu·ªëi c√πng...\")\n",
    "\n",
    "df_fact_score_dist_group = df_with_keys.select(\n",
    "    \"SDBSGKey\",\n",
    "    \"regionKey\",\n",
    "    \"subjectGroupKey\",\n",
    "    \"timeKey\",\n",
    "    col(\"scoreRange\").cast(\"float\"),\n",
    "    col(\"quantity\").cast(\"float\")\n",
    ")\n",
    "\n",
    "print(\"  ‚úì Schema:\")\n",
    "df_fact_score_dist_group.printSchema()\n",
    "\n",
    "# ==================== GHI V√ÄO GOLD LAYER ====================\n",
    "print(\"\\n[B∆∞·ªõc 8] Ghi v√†o Gold layer...\")\n",
    "print(\"  ‚è≥ ƒêang ghi... (partitioned by timeKey)\")\n",
    "\n",
    "df_fact_score_dist_group.writeTo(\"nessie.gold_tables.fact_score_distribution_by_subject_group\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .partitionedBy(\"timeKey\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "final_count = df_fact_score_dist_group.count()\n",
    "print(f\"  ‚úì ƒê√£ ghi {final_count:,} d√≤ng v√†o fact_score_distribution_by_subject_group\")\n",
    "\n",
    "# ==================== HI·ªÇN TH·ªä K·∫æT QU·∫¢ ====================\n",
    "print(\"\\n[B∆∞·ªõc 9] Xem k·∫øt qu·∫£ cu·ªëi c√πng:\")\n",
    "print(\"\\nTop 20 d√≤ng (s·∫Øp x·∫øp theo timeKey, regionKey):\")\n",
    "df_fact_score_dist_group.orderBy(\"timeKey\", \"regionKey\", \"subjectGroupKey\", \"scoreRange\").show(20)\n",
    "\n",
    "print(\"\\nTh·ªëng k√™ theo nƒÉm:\")\n",
    "spark.table(\"nessie.gold_tables.fact_score_distribution_by_subject_group\") \\\n",
    "    .groupBy(\"timeKey\") \\\n",
    "    .agg(\n",
    "        countDistinct(\"regionKey\").alias(\"s·ªë_khu_v·ª±c\"),\n",
    "        countDistinct(\"subjectGroupKey\").alias(\"s·ªë_kh·ªëi_thi\"),\n",
    "        sum(\"quantity\").alias(\"t·ªïng_s·ªë_h·ªçc_sinh\"),\n",
    "        count(\"*\").alias(\"s·ªë_records\")\n",
    "    ) \\\n",
    "    .orderBy(\"timeKey\") \\\n",
    "    .show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ HO√ÄN TH√ÄNH PH·∫¶N 2: Fact table ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7012db44",
   "metadata": {},
   "source": [
    "## 4. T·ªïng K·∫øt v√† Ki·ªÉm Tra\n",
    "\n",
    "### 4.1. Ki·ªÉm tra s·ªë l∆∞·ª£ng d√≤ng trong c√°c b·∫£ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20350c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== T·ªîNG K·∫æT S·ªê L∆Ø·ª¢NG D√íNG TRONG C√ÅC B·∫¢NG ===\\n\")\n",
    "\n",
    "tables = [\n",
    "    \"dim_time\",\n",
    "    \"dim_region\", \n",
    "    \"dim_school\",\n",
    "    \"dim_major\",\n",
    "    \"dim_subject\",\n",
    "    \"dim_subject_group\",\n",
    "    \"dim_selection_method\",\n",
    "    \"fact_benchmark\",\n",
    "    \"fact_score_distribution_by_subject\",\n",
    "    \"fact_score_distribution_by_subject_group\"\n",
    "]\n",
    "\n",
    "for table_name in tables:\n",
    "    try:\n",
    "        count = spark.table(f\"nessie.gold_tables.{table_name}\").count()\n",
    "        print(f\"{table_name}: {count:,} d√≤ng\")\n",
    "    except Exception as e:\n",
    "        print(f\"{table_name}: L·ªói - {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
