{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341915fc",
   "metadata": {},
   "source": [
    "# Load Data t·ª´ Silver Layer sang Gold Layer (Dimension & Fact Tables)\n",
    "\n",
    "Notebook n√†y s·∫Ω load d·ªØ li·ªáu t·ª´ Silver layer v√† transform sang Gold layer v·ªõi:\n",
    "- Dimension tables c√≥ surrogate key t·ª± ƒë·ªông tƒÉng\n",
    "- Fact tables v·ªõi c√°c foreign keys t∆∞∆°ng ·ª©ng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c4beb8",
   "metadata": {},
   "source": [
    "## 1. Import Libraries v√† Kh·ªüi t·∫°o Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "345f5653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/06 08:48:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/06 08:48:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o!\n",
      "Log level ƒë√£ ƒë∆∞·ª£c set th√†nh ERROR\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "\n",
    "# Kh·ªüi t·∫°o Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Load_Silver_To_Gold\")\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v1\")\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://gold/\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.access-key\", \"admin\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.secret-key\", \"admin123\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# T·∫Øt log WARN - ch·ªâ hi·ªÉn th·ªã ERROR\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"Spark Session ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o!\")\n",
    "print(\"Log level ƒë√£ ƒë∆∞·ª£c set th√†nh ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cf8a02",
   "metadata": {},
   "source": [
    "## 2. Load Dimension Tables t·ª´ Silver Layer\n",
    "\n",
    "### 2.1. Dim_Time - T·∫°o b·∫£ng th·ªùi gian t·ª´ d·ªØ li·ªáu benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eae759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_Time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ load 5 d√≤ng v√†o dim_time\n",
      "+-------+---+-----+----+\n",
      "|timeKey|day|month|year|\n",
      "+-------+---+-----+----+\n",
      "|      1|  1|    1|2021|\n",
      "|      2|  1|    1|2022|\n",
      "|      3|  1|    1|2023|\n",
      "|      4|  1|    1|2024|\n",
      "|      5|  1|    1|2025|\n",
      "+-------+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_Time...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_benchmark = spark.table(\"nessie.silver_tables.benchmark\")\n",
    "\n",
    "# T·∫°o dim_time t·ª´ c√°c year unique trong benchmark\n",
    "df_time = df_benchmark.select(\"year\").distinct()\n",
    "\n",
    "# T·∫°o day, month t·ª´ year (gi·∫£ s·ª≠ ng√†y 1/1 c·ªßa m·ªói nƒÉm)\n",
    "df_time = df_time.withColumn(\"day\", lit(1)) \\\n",
    "    .withColumn(\"month\", lit(1))\n",
    "\n",
    "# T·∫°o timeKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"year\")\n",
    "df_time = df_time.withColumn(\"timeKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt theo th·ª© t·ª±\n",
    "df_dim_time = df_time.select(\"timeKey\", \"day\", \"month\", \"year\")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_time.writeTo(\"nessie.gold_tables.dim_time\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_time.count()} d√≤ng v√†o dim_time\")\n",
    "df_dim_time.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b5f7c",
   "metadata": {},
   "source": [
    "### 2.2. Dim_Region - B·∫£ng khu v·ª±c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "042a4a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_Region...\n",
      "ƒê√£ load 64 d√≤ng v√†o dim_region\n",
      "+---------+--------+--------------------+\n",
      "|regionKey|regionId|          regionName|\n",
      "+---------+--------+--------------------+\n",
      "|        1|      01|      S·ªü GDƒêT H√† N·ªôi|\n",
      "|        2|      02|S·ªü GDƒêT TP. H·ªì Ch...|\n",
      "|        3|      03|   S·ªü GDƒêT H·∫£i Ph√≤ng|\n",
      "|        4|      04|     S·ªü GDƒêT ƒê√† N·∫µng|\n",
      "|        5|      05|    S·ªü GDƒêT H√† Giang|\n",
      "|        6|      06|    S·ªü GDƒêT Cao B·∫±ng|\n",
      "|        7|      07|    S·ªü GDƒêT Lai Ch√¢u|\n",
      "|        8|      08|     S·ªü GDƒêT L√†o Cai|\n",
      "|        9|      09| S·ªü GDƒêT Tuy√™n Quang|\n",
      "|       10|      10|    S·ªü GDƒêT L·∫°ng S∆°n|\n",
      "+---------+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_Region...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_region_silver = spark.table(\"nessie.silver_tables.region\")\n",
    "\n",
    "# T·∫°o regionKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"regionId\")\n",
    "df_dim_region = df_region_silver.withColumn(\"regionKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt\n",
    "df_dim_region = df_dim_region.select(\"regionKey\", \"regionId\", \"regionName\")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_region.writeTo(\"nessie.gold_tables.dim_region\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_region.count()} d√≤ng v√†o dim_region\")\n",
    "df_dim_region.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd737abe",
   "metadata": {},
   "source": [
    "### 2.3. Dim_School - B·∫£ng tr∆∞·ªùng ƒë·∫°i h·ªçc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb73fa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_School...\n",
      "ƒê√£ load 270 d√≤ng v√†o dim_school\n",
      "+---------+--------+--------------------+---------+\n",
      "|schoolKey|schoolId|          schoolName| province|\n",
      "+---------+--------+--------------------+---------+\n",
      "|        1|   \\bDMT|Ph√¢n hi·ªáu ƒêH T√†i ...|Thanh H√≥a|\n",
      "|        2|     ANH|H·ªçc vi·ªán An Ninh ...|   H√† N·ªôi|\n",
      "|        3|     ANS|ƒê·∫°i h·ªçc An Ninh N...|   H√† N·ªôi|\n",
      "|        4|     BKA|ƒê·∫°i h·ªçc B√°ch khoa...|   H√† N·ªôi|\n",
      "|        5|     BMU|ƒê·∫°i h·ªçc Bu√¥n Ma T...|  ƒê·∫Øk L·∫Øk|\n",
      "|        6|     BPH| H·ªçc vi·ªán Bi√™n Ph√≤ng|   H√† N·ªôi|\n",
      "|        7|     BVH|H·ªçc vi·ªán C√¥ng ngh...|   H√† N·ªôi|\n",
      "|        8|     BVS|H·ªçc vi·ªán C√¥ng ngh...|   TP HCM|\n",
      "|        9|     CCM|ƒê·∫°i h·ªçc C√¥ng Nghi...|   H√† N·ªôi|\n",
      "|       10|     CEA|ƒê·∫°i h·ªçc Kinh T·∫ø N...|  Ngh·ªá An|\n",
      "+---------+--------+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_School...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_school_silver = spark.table(\"nessie.silver_tables.school\")\n",
    "\n",
    "# T·∫°o schoolKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"schoolId\")\n",
    "df_dim_school = df_school_silver.withColumn(\"schoolKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt\n",
    "df_dim_school = df_dim_school.select(\"schoolKey\", \"schoolId\", \"schoolName\", \"province\")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_school.writeTo(\"nessie.gold_tables.dim_school\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_school.count()} d√≤ng v√†o dim_school\")\n",
    "df_dim_school.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e3af4a",
   "metadata": {},
   "source": [
    "### 2.4. Dim_Major - B·∫£ng ng√†nh h·ªçc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de69025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_Major...\n",
      "ƒê√£ load 3265 d√≤ng v√†o dim_major\n",
      "+--------+-------+--------------------+\n",
      "|majorKey|majorId|           majorName|\n",
      "+--------+-------+--------------------+\n",
      "|       1|    106|   Khoa h·ªçc M√°y t√≠nh|\n",
      "|       2|    107|   K·ªπ thu·∫≠t M√°y t√≠nh|\n",
      "|       3|    108|ƒêi·ªán - ƒêi·ªán t·ª≠ - ...|\n",
      "|       4|    109|     K·ªπ Thu·∫≠t C∆° kh√≠|\n",
      "|       5|    110| K·ªπ Thu·∫≠t C∆° ƒêi·ªán t·ª≠|\n",
      "|       6|    112|           D·ªát - May|\n",
      "|       7|    114|Ho√° - Th·ª±c ph·∫©m -...|\n",
      "|       8|    115|X√¢y d·ª±ng v√† Qu·∫£n ...|\n",
      "|       9|    117|           Ki·∫øn Tr√∫c|\n",
      "|      10|    120|  D·∫ßu kh√≠ - ƒê·ªãa ch·∫•t|\n",
      "+--------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_Major...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_major_silver = spark.table(\"nessie.silver_tables.major\")\n",
    "\n",
    "# T·∫°o majorKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"majorId\")\n",
    "df_dim_major = df_major_silver.withColumn(\"majorKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt\n",
    "df_dim_major = df_dim_major.select(\"majorKey\", \"majorId\", \"majorName\")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_major.writeTo(\"nessie.gold_tables.dim_major\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_major.count()} d√≤ng v√†o dim_major\")\n",
    "df_dim_major.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8e04d",
   "metadata": {},
   "source": [
    "### 2.5. Dim_Subject - B·∫£ng m√¥n h·ªçc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba8b3584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_Subject...\n",
      "ƒê√£ load 52 d√≤ng v√†o dim_subject\n",
      "+----------+---------+--------------------+\n",
      "|subjectKey|subjectId|         subjectName|\n",
      "+----------+---------+--------------------+\n",
      "|         1|        1|Bi·ªÉu di·ªÖn ngh·ªá thu·∫≠t|\n",
      "|         2|        2|C√¥ng ngh·ªá c√¥ng ng...|\n",
      "|         3|        3|C√¥ng ngh·ªá n√¥ng ng...|\n",
      "|         4|        4|                GDCD|\n",
      "|         5|        5|                 H√°t|\n",
      "|         6|        6|           H√°t & M√∫a|\n",
      "|         7|        7|H√°t ho·∫∑c bi·ªÉu di·ªÖ...|\n",
      "|         8|        8|            H√¨nh h·ªça|\n",
      "|         9|        9|                 H√≥a|\n",
      "|        10|       10|     Khoa h·ªçc x√£ h·ªôi|\n",
      "+----------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_Subject...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_subject_silver = spark.table(\"nessie.silver_tables.subject\")\n",
    "\n",
    "# T·∫°o subjectKey t·ª± ƒë·ªông tƒÉng (s·ª≠ d·ª•ng subjectId l√†m sort key)\n",
    "window_spec = Window.orderBy(\"subjectId\")\n",
    "df_dim_subject = df_subject_silver.withColumn(\"subjectKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt\n",
    "df_dim_subject = df_dim_subject.select(\"subjectKey\", \"subjectId\", \"subjectName\")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_subject.writeTo(\"nessie.gold_tables.dim_subject\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_subject.count()} d√≤ng v√†o dim_subject\")\n",
    "df_dim_subject.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfcd8ab",
   "metadata": {},
   "source": [
    "### 2.6. Dim_Subject_Group - B·∫£ng kh·ªëi thi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32f9716f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_Subject_Group...\n",
      "ƒê√£ load 242 d√≤ng v√†o dim_subject_group\n",
      "+---------------+--------------+----------------+--------------------+\n",
      "|subjectGroupKey|subjectGroupId|subjectGroupName|  subjectCombination|\n",
      "+---------------+--------------+----------------+--------------------+\n",
      "|              1|             1|             A00|         To√°n-L√≠-H√≥a|\n",
      "|              2|             2|             A01|   To√°n-L√≠-Ngo·∫°i ng·ªØ|\n",
      "|              3|             3|             A02|        To√°n-L√≠-Sinh|\n",
      "|              4|             4|             A03|          To√°n-L√≠-S·ª≠|\n",
      "|              5|             5|             A04|         To√°n-L√≠-ƒê·ªãa|\n",
      "|              6|             6|             A05|         To√°n-H√≥a-S·ª≠|\n",
      "|              7|             7|             A06|        To√°n-H√≥a-ƒê·ªãa|\n",
      "|              8|             8|             A07|         To√°n-S·ª≠-ƒê·ªãa|\n",
      "|              9|             9|             A0C|To√°n-L√≠-C√¥ng ngh·ªá...|\n",
      "|             10|            10|             A0T|     To√°n-L√≠-Tin h·ªçc|\n",
      "+---------------+--------------+----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_Subject_Group...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_subject_group_silver = spark.table(\"nessie.silver_tables.subject_group\")\n",
    "\n",
    "# T·∫°o subjectGroupKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"subjectGroupId\")\n",
    "df_dim_subject_group = df_subject_group_silver.withColumn(\"subjectGroupKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt\n",
    "df_dim_subject_group = df_dim_subject_group.select(\n",
    "    \"subjectGroupKey\", \n",
    "    \"subjectGroupId\", \n",
    "    \"subjectGroupName\", \n",
    "    \"subjectCombination\"\n",
    ")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_subject_group.writeTo(\"nessie.gold_tables.dim_subject_group\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_subject_group.count()} d√≤ng v√†o dim_subject_group\")\n",
    "df_dim_subject_group.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47b43b1",
   "metadata": {},
   "source": [
    "### 2.7. Dim_Selection_Method - B·∫£ng ph∆∞∆°ng th·ª©c x√©t tuy·ªÉn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d64da34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_Selection_Method...\n",
      "ƒê√£ load 13 d√≤ng v√†o dim_selection_method\n",
      "+------------------+-----------------+--------------------+\n",
      "|selectionMethodKey|selectionMethodId| selectionMethodName|\n",
      "+------------------+-----------------+--------------------+\n",
      "|                 1|                1|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 2|                2|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 3|                3|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 4|                4|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 5|                5|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 6|                6|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 7|                7|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 8|                8|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 9|                9|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                10|               10|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "+------------------+-----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_Selection_Method...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_selection_method_silver = spark.table(\"nessie.silver_tables.selection_method\")\n",
    "\n",
    "# T·∫°o selectionMethodKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"selectionMethodId\")\n",
    "df_dim_selection_method = df_selection_method_silver.withColumn(\"selectionMethodKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt\n",
    "df_dim_selection_method = df_dim_selection_method.select(\n",
    "    \"selectionMethodKey\", \n",
    "    \"selectionMethodId\", \n",
    "    \"selectionMethodName\"\n",
    ")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_selection_method.writeTo(\"nessie.gold_tables.dim_selection_method\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_selection_method.count()} d√≤ng v√†o dim_selection_method\")\n",
    "df_dim_selection_method.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f18c813",
   "metadata": {},
   "source": [
    "## 3. Load Fact Tables t·ª´ Silver Layer\n",
    "\n",
    "### 3.1. Fact_Benchmark - Load v√† join v·ªõi c√°c Dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fd8f72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Fact_Benchmark...\n",
      "Sau t·∫•t c·∫£ join: 148,995 d√≤ng\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ load 148995 d√≤ng v√†o fact_benchmark\n",
      "+------------+---------------+-------+--------+---------+------------------+-----+---------------+--------------+--------------+----------------+\n",
      "|benchmarkKey|subjectGroupKey|timeKey|majorKey|schoolKey|selectionMethodKey|score|avgScoreByMajor|yearlyScoreGap|rankAmongMajor|rankAmongSchools|\n",
      "+------------+---------------+-------+--------+---------+------------------+-----+---------------+--------------+--------------+----------------+\n",
      "|       14193|             41|      1|      41|      206|                 3| 26.7|           26.7|           0.0|            14|               1|\n",
      "|       14194|              1|      1|      41|      206|                 3| 26.7|           26.7|           0.0|            14|               1|\n",
      "|       14195|             35|      1|      41|      206|                 3| 26.7|           26.7|           0.0|            14|               1|\n",
      "|       14196|              2|      1|      41|      206|                 3| 26.7|           26.7|           0.0|            14|               1|\n",
      "|        1534|            119|      1|      66|       17|                 2| 19.5|           19.5|           0.0|             4|               1|\n",
      "|       15297|            129|      1|      66|      218|                 2| 19.5|           19.5|           0.0|            21|               2|\n",
      "|       15298|            119|      1|      66|      218|                 2| 19.5|           19.5|           0.0|            21|               2|\n",
      "|       15304|            125|      1|      66|      218|                 2| 19.5|           19.5|           0.0|            21|               2|\n",
      "|       18860|            119|      1|      66|      239|                 2| 19.0|           19.0|           0.0|            14|               3|\n",
      "|       18862|            130|      1|      66|      239|                 2| 19.0|           19.0|           0.0|            14|               3|\n",
      "+------------+---------------+-------+--------+---------+------------------+-----+---------------+--------------+--------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Fact_Benchmark...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_benchmark_silver = spark.table(\"nessie.silver_tables.benchmark\")\n",
    "\n",
    "# ƒê·ªçc c√°c dimension tables t·ª´ Gold ƒë·ªÉ join\n",
    "df_dim_time_gold = spark.table(\"nessie.gold_tables.dim_time\")\n",
    "df_dim_school_gold = spark.table(\"nessie.gold_tables.dim_school\")\n",
    "df_dim_major_gold = spark.table(\"nessie.gold_tables.dim_major\")\n",
    "df_dim_subject_group_gold = spark.table(\"nessie.gold_tables.dim_subject_group\")\n",
    "df_dim_selection_method_gold = spark.table(\"nessie.gold_tables.dim_selection_method\")\n",
    "\n",
    "# Join t·ª´ng b∆∞·ªõc ƒë·ªÉ tr√°nh Cartesian product\n",
    "# B∆∞·ªõc 1: Join v·ªõi dim_time\n",
    "df_fact = df_benchmark_silver.join(\n",
    "    df_dim_time_gold, \n",
    "    df_benchmark_silver[\"year\"] == df_dim_time_gold[\"year\"], \n",
    "    \"left\"\n",
    ").drop(df_dim_time_gold[\"year\"])\n",
    "\n",
    "# B∆∞·ªõc 2: Join v·ªõi dim_school\n",
    "df_fact = df_fact.join(\n",
    "    df_dim_school_gold, \n",
    "    df_fact[\"schoolId\"] == df_dim_school_gold[\"schoolId\"], \n",
    "    \"left\"\n",
    ").drop(df_dim_school_gold[\"schoolId\"])\n",
    "\n",
    "# B∆∞·ªõc 3: Join v·ªõi dim_major\n",
    "df_fact = df_fact.join(\n",
    "    df_dim_major_gold, \n",
    "    df_fact[\"majorId\"] == df_dim_major_gold[\"majorId\"], \n",
    "    \"left\"\n",
    ").drop(df_dim_major_gold[\"majorId\"])\n",
    "\n",
    "# B∆∞·ªõc 4: Join v·ªõi dim_subject_group\n",
    "df_fact = df_fact.join(\n",
    "    df_dim_subject_group_gold, \n",
    "    df_fact[\"subjectGroupId\"] == df_dim_subject_group_gold[\"subjectGroupId\"], \n",
    "    \"left\"\n",
    ").drop(df_dim_subject_group_gold[\"subjectGroupId\"])\n",
    "\n",
    "# B∆∞·ªõc 5: Join v·ªõi dim_selection_method\n",
    "df_fact = df_fact.join(\n",
    "    df_dim_selection_method_gold, \n",
    "    df_fact[\"selectionMethodId\"] == df_dim_selection_method_gold[\"selectionMethodId\"], \n",
    "    \"left\"\n",
    ").drop(df_dim_selection_method_gold[\"selectionMethodId\"])\n",
    "\n",
    "print(f\"Sau t·∫•t c·∫£ join: {df_fact.count():,} d√≤ng\")\n",
    "\n",
    "# T·∫°o benchmarkKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"timeKey\", \"schoolKey\", \"majorKey\")\n",
    "df_fact = df_fact.withColumn(\"benchmarkKey\", row_number().over(window_spec))\n",
    "\n",
    "# T√≠nh to√°n yearlyScoreGap\n",
    "window_year_spec = Window.partitionBy(\"schoolKey\", \"majorKey\", \"selectionMethodKey\").orderBy(\"timeKey\")\n",
    "df_fact = df_fact.withColumn(\n",
    "    \"yearlyScoreGap\",\n",
    "    coalesce(col(\"score\") - lag(\"score\", 1).over(window_year_spec), lit(0))\n",
    ")\n",
    "\n",
    "# T√≠nh ƒëi·ªÉm trung b√¨nh cho m·ªói ng√†nh theo t·ª´ng ph∆∞∆°ng th·ª©c\n",
    "window_avg_spec = Window.partitionBy(\"timeKey\", \"schoolKey\", \"majorKey\", \"selectionMethodKey\")\n",
    "df_fact = df_fact.withColumn(\n",
    "    \"avgScoreByMajor\",\n",
    "    avg(\"score\").over(window_avg_spec)\n",
    ")\n",
    "\n",
    "# X·∫øp h·∫°ng ng√†nh trong tr∆∞·ªùng\n",
    "window_major_in_school_spec = Window.partitionBy(\"timeKey\", \"schoolKey\", \"selectionMethodKey\").orderBy(desc(\"avgScoreByMajor\"), \"majorKey\")\n",
    "df_fact = df_fact.withColumn(\"rankAmongMajor\", dense_rank().over(window_major_in_school_spec))\n",
    "\n",
    "# X·∫øp h·∫°ng ng√†nh so v·ªõi c√°c tr∆∞·ªùng kh√°c\n",
    "window_major_across_schools_spec = Window.partitionBy(\"timeKey\", \"majorKey\", \"selectionMethodKey\").orderBy(desc(\"avgScoreByMajor\"), \"schoolKey\")\n",
    "df_fact = df_fact.withColumn(\"rankAmongSchools\", dense_rank().over(window_major_across_schools_spec))\n",
    "\n",
    "# Select c√°c c·ªôt cu·ªëi c√πng\n",
    "df_fact_benchmark = df_fact.select(\n",
    "    \"benchmarkKey\",\n",
    "    \"subjectGroupKey\",\n",
    "    \"timeKey\",\n",
    "    \"majorKey\",\n",
    "    \"schoolKey\",\n",
    "    \"selectionMethodKey\",\n",
    "    col(\"score\").cast(\"float\"),\n",
    "    col(\"avgScoreByMajor\").cast(\"float\"),\n",
    "    col(\"yearlyScoreGap\").cast(\"float\"),\n",
    "    \"rankAmongMajor\",\n",
    "    \"rankAmongSchools\"\n",
    ")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_fact_benchmark.writeTo(\"nessie.gold_tables.fact_benchmark\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .partitionedBy(\"timeKey\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_fact_benchmark.count()} d√≤ng v√†o fact_benchmark\")\n",
    "df_fact_benchmark.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6025881c",
   "metadata": {},
   "source": [
    "### 3.2. Fact_Score_Distribution_By_Subject - Ph√¢n b·ªë ƒëi·ªÉm theo m√¥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e2a323f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Fact_Score_Distribution_By_Subject...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ load 100999 d√≤ng v√†o fact_score_distribution_by_subject\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 117:==============>                                          (1 + 3) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+-------+--------------+--------+\n",
      "|SDBSKey|regionKey|subjectKey|timeKey|scoreThreshold|quantity|\n",
      "+-------+---------+----------+-------+--------------+--------+\n",
      "|      1|        1|         4|      1|           0.0|    11.0|\n",
      "|      2|        1|         4|      1|           0.4|     1.0|\n",
      "|      3|        1|         4|      1|           1.4|     2.0|\n",
      "|      4|        1|         4|      1|           2.0|     2.0|\n",
      "|      5|        1|         4|      1|           2.2|     7.0|\n",
      "|      6|        1|         4|      1|           2.4|     5.0|\n",
      "|      7|        1|         4|      1|           2.6|     2.0|\n",
      "|      8|        1|         4|      1|           3.0|    12.0|\n",
      "|      9|        1|         4|      1|           3.2|    19.0|\n",
      "|     10|        1|         4|      1|           3.4|    25.0|\n",
      "+-------+---------+----------+-------+--------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Fact_Score_Distribution_By_Subject...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_student_scores = spark.table(\"nessie.silver_tables.student_scores\")\n",
    "\n",
    "# ƒê·ªçc dimension tables\n",
    "df_dim_time_gold = spark.table(\"nessie.gold_tables.dim_time\")\n",
    "df_dim_region_gold = spark.table(\"nessie.gold_tables.dim_region\")\n",
    "df_dim_subject_gold = spark.table(\"nessie.gold_tables.dim_subject\")\n",
    "\n",
    "# Explode map scores th√†nh t·ª´ng d√≤ng (subjectId - ƒëi·ªÉm)\n",
    "# V√¨ scores gi·ªù l√† Map<Int, Float> (subjectId -> score)\n",
    "# subjectId ·ªü ƒë√¢y l√† INTEGER, ch√≠nh l√† c·ªôt subjectId t·ª´ b·∫£ng silver.subject\n",
    "df_scores_exploded = df_student_scores.select(\n",
    "    \"regionId\",\n",
    "    \"year\",\n",
    "    explode(\"scores\").alias(\"subjectId\", \"score\")  # subjectId = key c·ªßa map (Int)\n",
    ")\n",
    "\n",
    "# Ph√¢n lo·∫°i ƒëi·ªÉm v√†o c√°c m·ªëc ƒëi·ªÉm (0.2, 0.4, 0.6, ..., 9.8, 10.0)\n",
    "# L√†m tr√≤n ƒëi·ªÉm ƒë·∫øn b·ªôi s·ªë c·ªßa 0.2 g·∫ßn nh·∫•t\n",
    "df_scores_exploded = df_scores_exploded.withColumn(\n",
    "    \"scoreThreshold\",\n",
    "    (floor(col(\"score\") / 0.2) * 0.2).cast(\"float\")\n",
    ")\n",
    "\n",
    "# ƒê·∫øm s·ªë l∆∞·ª£ng h·ªçc sinh theo t·ª´ng m√¥n, khu v·ª±c, nƒÉm v√† ng∆∞·ª°ng ƒëi·ªÉm\n",
    "df_distribution = df_scores_exploded.groupBy(\n",
    "    \"regionId\",\n",
    "    \"subjectId\",  # ƒê√¢y l√† subjectId (Int) t·ª´ scores map\n",
    "    \"year\",\n",
    "    \"scoreThreshold\"\n",
    ").agg(\n",
    "    count(\"*\").alias(\"quantity\")\n",
    ")\n",
    "\n",
    "# Join v·ªõi c√°c dimension tables\n",
    "# Join v·ªõi dim_subject b·∫±ng subjectId ƒë·ªÉ l·∫•y subjectKey\n",
    "df_fact = df_distribution \\\n",
    "    .join(df_dim_region_gold, df_distribution[\"regionId\"] == df_dim_region_gold[\"regionId\"], \"left\") \\\n",
    "    .join(df_dim_subject_gold, df_distribution[\"subjectId\"] == df_dim_subject_gold[\"subjectId\"], \"left\") \\\n",
    "    .join(df_dim_time_gold, df_distribution[\"year\"] == df_dim_time_gold[\"year\"], \"left\")\n",
    "\n",
    "# T·∫°o SDBSKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"timeKey\", \"regionKey\", \"subjectKey\", \"scoreThreshold\")\n",
    "df_fact = df_fact.withColumn(\"SDBSKey\", row_number().over(window_spec))\n",
    "\n",
    "# Select c√°c c·ªôt cu·ªëi c√πng\n",
    "# L∆∞u √Ω: L·∫•y subjectKey t·ª´ df_dim_subject_gold (kh√¥ng ph·∫£i subjectId)\n",
    "df_fact_score_dist_subject = df_fact.select(\n",
    "    \"SDBSKey\",\n",
    "    df_dim_region_gold[\"regionKey\"],\n",
    "    df_dim_subject_gold[\"subjectKey\"],  # subjectKey t·ª´ dim_subject (surrogate key)\n",
    "    df_dim_time_gold[\"timeKey\"],\n",
    "    col(\"scoreThreshold\").cast(\"float\"),\n",
    "    col(\"quantity\").cast(\"float\")\n",
    ")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_fact_score_dist_subject.writeTo(\"nessie.gold_tables.fact_score_distribution_by_subject\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .partitionedBy(\"timeKey\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_fact_score_dist_subject.count()} d√≤ng v√†o fact_score_distribution_by_subject\")\n",
    "df_fact_score_dist_subject.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bad2e2",
   "metadata": {},
   "source": [
    "### 3.3. Fact_Score_Distribution_By_Subject_Group - Ph√¢n b·ªë ƒëi·ªÉm theo kh·ªëi thi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d6eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PH·∫¶N 1: T√çNH TO√ÅN T·ªî H·ª¢P KH·ªêI THI CHO M·ªñI TH√ç SINH (C√ì DEBUG LOG)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==================== ƒê·ªåC D·ªÆ LI·ªÜU ====================\n",
    "print(\"\\n[B∆∞·ªõc 1] ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver layer...\")\n",
    "df_student_scores = spark.table(\"nessie.silver_tables.student_scores\")\n",
    "df_subject_group = spark.table(\"nessie.silver_tables.subject_group\")\n",
    "df_subject = spark.table(\"nessie.silver_tables.subject\")\n",
    "\n",
    "print(f\"  ‚úì S·ªë l∆∞·ª£ng th√≠ sinh: {df_student_scores.count():,}\")\n",
    "print(f\"  ‚úì S·ªë l∆∞·ª£ng kh·ªëi thi: {df_subject_group.count()}\")\n",
    "print(f\"  ‚úì S·ªë l∆∞·ª£ng m√¥n h·ªçc: {df_subject.count()}\")\n",
    "\n",
    "# ==================== T·∫†O MAPPING DICTIONARIES ====================\n",
    "print(\"\\n[B∆∞·ªõc 2] T·∫°o mapping dictionaries...\")\n",
    "\n",
    "# Mapping: T√™n m√¥n ‚Üí ID m√¥n (VD: \"To√°n\" ‚Üí 1)\n",
    "subject_name_to_id = {row['subjectName']: row['subjectId'] for row in df_subject.collect()}\n",
    "print(f\"  ‚úì Subject mapping: {subject_name_to_id}\")\n",
    "\n",
    "# Mapping: ID kh·ªëi ‚Üí Danh s√°ch m√¥n (VD: \"A00\" ‚Üí [\"To√°n\", \"L√Ω\", \"H√≥a\"])\n",
    "subject_groups_dict = {}\n",
    "for row in df_subject_group.collect():\n",
    "    group_id = row['subjectGroupId']\n",
    "    subjects = [s.strip() for s in row['subjectCombination'].split('-')]\n",
    "    subject_groups_dict[group_id] = subjects\n",
    "\n",
    "print(f\"  ‚úì S·ªë l∆∞·ª£ng kh·ªëi thi: {len(subject_groups_dict)}\")\n",
    "print(f\"\\n  üìã Chi ti·∫øt c√°c kh·ªëi thi:\")\n",
    "for group_id, subjects in list(subject_groups_dict.items())[:5]:\n",
    "    print(f\"     - {group_id}: {subjects}\")\n",
    "print(f\"     ... v√† {len(subject_groups_dict) - 5} kh·ªëi thi kh√°c\")\n",
    "\n",
    "# Broadcast ƒë·ªÉ worker nodes truy c·∫≠p nhanh\n",
    "from pyspark.sql.types import FloatType, MapType, StringType\n",
    "subject_map_broadcast = spark.sparkContext.broadcast(subject_name_to_id)\n",
    "subject_groups_broadcast = spark.sparkContext.broadcast(subject_groups_dict)\n",
    "\n",
    "print(f\"T·ªïng s·ªë kh·ªëi thi trong h·ªá th·ªëng: {len(subject_groups_dict)}\")\n",
    "\n",
    "# ==================== UDF T√çNH ƒêI·ªÇM T·ªîNG THEO KH·ªêI (C√ì DEBUG) ====================\n",
    "print(\"\\n[B∆∞·ªõc 4] ƒê·ªãnh nghƒ©a UDF t√≠nh ƒëi·ªÉm t·ªïng theo kh·ªëi thi (c√≥ debug logging)...\")\n",
    "\n",
    "# Bi·∫øn global ƒë·ªÉ ƒë·∫øm s·ªë l·∫ßn g·ªçi UDF (ch·ªâ d√πng ƒë·ªÉ demo)\n",
    "import sys\n",
    "\n",
    "def create_subject_group_scores_map_debug(scores_map):\n",
    "    \"\"\"\n",
    "    UDF ch√≠nh v·ªõi DEBUG LOGGING: T√≠nh ƒëi·ªÉm t·ªïng cho T·∫§T C·∫¢ c√°c kh·ªëi thi m√† th√≠ sinh c√≥ ƒë·ªß m√¥n\n",
    "    \n",
    "    Input:  scores_map = Map<subjectId, score> c·ªßa 1 th√≠ sinh\n",
    "    Output: Map<subjectGroupId, totalScore> cho c√°c kh·ªëi th√≠ sinh c√≥ ƒë·ªß m√¥n\n",
    "    \"\"\"\n",
    "    if scores_map is None:\n",
    "        return {}\n",
    "    \n",
    "    subject_name_to_id_map = subject_map_broadcast.value\n",
    "    subject_id_to_name_map = {v: k for k, v in subject_name_to_id_map.items()}\n",
    "    subject_groups = subject_groups_broadcast.value\n",
    "    \n",
    "    # DEBUG: In ra ƒëi·ªÉm c·ªßa th√≠ sinh (ch·ªâ in 3 th√≠ sinh ƒë·∫ßu ƒë·ªÉ tr√°nh spam log)\n",
    "    student_subjects = {subject_id_to_name_map.get(sid, f\"ID_{sid}\"): score \n",
    "                       for sid, score in scores_map.items()}\n",
    "    \n",
    "    result = {}\n",
    "    groups_eligible = []\n",
    "    groups_not_eligible = []\n",
    "    \n",
    "    # V·ªõi m·ªói kh·ªëi thi, ki·ªÉm tra xem th√≠ sinh c√≥ ƒë·ªß m√¥n kh√¥ng\n",
    "    for group_id, required_subjects in subject_groups.items():\n",
    "        total_score = 0.0\n",
    "        has_all_subjects = True\n",
    "        missing_subjects = []\n",
    "        \n",
    "        # Ki·ªÉm tra t·ª´ng m√¥n trong kh·ªëi\n",
    "        for subject_name in required_subjects:\n",
    "            subject_id = subject_name_to_id_map.get(subject_name)\n",
    "            \n",
    "            # N·∫øu thi·∫øu m√¥n ho·∫∑c kh√¥ng c√≥ ƒëi·ªÉm ‚Üí B·ªè qua kh·ªëi n√†y\n",
    "            if subject_id is None or subject_id not in scores_map:\n",
    "                has_all_subjects = False\n",
    "                missing_subjects.append(subject_name)\n",
    "                break\n",
    "            \n",
    "            # C·ªông d·ªìn ƒëi·ªÉm\n",
    "            total_score += scores_map[subject_id]\n",
    "        \n",
    "        # Ch·ªâ th√™m v√†o result n·∫øu th√≠ sinh c√≥ ƒë·ªß T·∫§T C·∫¢ c√°c m√¥n trong kh·ªëi\n",
    "        if has_all_subjects:\n",
    "            result[group_id] = total_score\n",
    "            groups_eligible.append(f\"{group_id}={total_score:.1f}\")\n",
    "        else:\n",
    "            groups_not_eligible.append(f\"{group_id}(thi·∫øu {','.join(missing_subjects)})\")\n",
    "    \n",
    "    # DEBUG: In th√¥ng tin (ch·ªâ in cho 3 th√≠ sinh ƒë·∫ßu)\n",
    "    # L∆∞u √Ω: Trong m√¥i tr∆∞·ªùng distributed, log n√†y s·∫Ω xu·∫•t hi·ªán tr√™n executor nodes\n",
    "    if len(result) > 0:\n",
    "        # Ch·ªâ in log cho m·ªôt s·ªë th√≠ sinh ƒë·ªÉ minh h·ªça\n",
    "        pass  # Comment out ƒë·ªÉ tr√°nh qu√° nhi·ªÅu log trong production\n",
    "    \n",
    "    return result\n",
    "\n",
    "create_group_scores_udf = udf(create_subject_group_scores_map_debug, MapType(StringType(), FloatType()))\n",
    "print(\"  ‚úì UDF ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a v·ªõi debug logging\")\n",
    "\n",
    "# T·∫°o c·ªôt m·ªõi ch·ª©a mapping subjectGroupId -> totalScore\n",
    "df_student_with_group_scores = df_student_scores.withColumn(\n",
    "    \"subject_group_scores\",  # C·ªôt m·ªõi: Map<subjectGroupId, totalScore>\n",
    "    create_group_scores_udf(col(\"scores\"))\n",
    ")\n",
    "\n",
    "# ==================== PH√ÇN T√çCH K·∫æT QU·∫¢ ====================\n",
    "print(\"[B∆∞·ªõc 7] Ph√¢n t√≠ch k·∫øt qu·∫£:\")\n",
    "print(\"\\n  üìä Xem m·∫´u k·∫øt qu·∫£ (5 th√≠ sinh):\")\n",
    "df_student_with_group_scores.select(\n",
    "    \"studentId\", \n",
    "    \"regionId\", \n",
    "    \"year\", \n",
    "    \"subject_group_scores\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "# T√≠nh th·ªëng k√™\n",
    "print(\"\\n  üìà Th·ªëng k√™ s·ªë l∆∞·ª£ng kh·ªëi thi m√† m·ªói th√≠ sinh c√≥ th·ªÉ x√©t:\")\n",
    "\n",
    "# ƒê·∫øm s·ªë l∆∞·ª£ng kh·ªëi thi cho m·ªói th√≠ sinh\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "df_stats = df_student_with_group_scores.withColumn(\n",
    "    \"num_eligible_groups\", \n",
    "    size(col(\"subject_group_scores\"))\n",
    ")\n",
    "\n",
    "df_stats.groupBy(\"num_eligible_groups\") \\\n",
    "    .agg(count(\"*\").alias(\"s·ªë_th√≠_sinh\")) \\\n",
    "    .orderBy(\"num_eligible_groups\") \\\n",
    "    .show(20)\n",
    "\n",
    "print(\"\\n  üìä Th·ªëng k√™ t·ªïng quan:\")\n",
    "avg_groups = df_stats.agg(avg(\"num_eligible_groups\").alias(\"avg\")).collect()[0][\"avg\"]\n",
    "max_groups = df_stats.agg(max(\"num_eligible_groups\").alias(\"max\")).collect()[0][\"max\"]\n",
    "min_groups = df_stats.agg(min(\"num_eligible_groups\").alias(\"min\")).collect()[0][\"min\"]\n",
    "\n",
    "print(f\"     - S·ªë kh·ªëi TB m·ªói th√≠ sinh c√≥ th·ªÉ x√©t: {avg_groups:.2f}\")\n",
    "print(f\"     - S·ªë kh·ªëi t·ªëi ƒëa: {max_groups}\")\n",
    "print(f\"     - S·ªë kh·ªëi t·ªëi thi·ªÉu: {min_groups}\")\n",
    "print(f\"     - T·ªïng s·ªë kh·ªëi thi trong h·ªá th·ªëng: {len(subject_groups_dict)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ HO√ÄN TH√ÄNH PH·∫¶N 1: DataFrame 'df_student_with_group_scores' ƒë√£ s·∫µn s√†ng\")\n",
    "print(\"   v·ªõi th√¥ng tin chi ti·∫øt v·ªÅ c√°c kh·ªëi thi m√† m·ªói th√≠ sinh c√≥ th·ªÉ x√©t\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2a32fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=\" * 80)\n",
    "# print(\"PH·∫¶N 1: T√çNH TO√ÅN T·ªî H·ª¢P KH·ªêI THI CHO M·ªñI TH√ç SINH\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# # ==================== ƒê·ªåC D·ªÆ LI·ªÜU ====================\n",
    "# print(\"\\n[B∆∞·ªõc 1] ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver layer...\")\n",
    "# df_student_scores = spark.table(\"nessie.silver_tables.student_scores\")\n",
    "# df_subject_group = spark.table(\"nessie.silver_tables.subject_group\")\n",
    "# df_subject = spark.table(\"nessie.silver_tables.subject\")\n",
    "\n",
    "# print(f\"  ‚úì S·ªë l∆∞·ª£ng th√≠ sinh: {df_student_scores.count():,}\")\n",
    "# print(f\"  ‚úì S·ªë l∆∞·ª£ng kh·ªëi thi: {df_subject_group.count()}\")\n",
    "# print(f\"  ‚úì S·ªë l∆∞·ª£ng m√¥n h·ªçc: {df_subject.count()}\")\n",
    "\n",
    "# # ==================== T·∫†O MAPPING DICTIONARIES ====================\n",
    "# print(\"\\n[B∆∞·ªõc 2] T·∫°o mapping dictionaries...\")\n",
    "\n",
    "# # Mapping: T√™n m√¥n ‚Üí ID m√¥n (VD: \"To√°n\" ‚Üí 1)\n",
    "# subject_name_to_id = {row['subjectName']: row['subjectId'] for row in df_subject.collect()}\n",
    "# print(f\"  ‚úì Subject mapping: {subject_name_to_id}\")\n",
    "\n",
    "# # Mapping: ID kh·ªëi ‚Üí Danh s√°ch m√¥n (VD: \"A00\" ‚Üí [\"To√°n\", \"L√Ω\", \"H√≥a\"])\n",
    "# subject_groups_dict = {}\n",
    "# for row in df_subject_group.collect():\n",
    "#     group_id = row['subjectGroupId']\n",
    "#     subjects = [s.strip() for s in row['subjectCombination'].split('-')]\n",
    "#     subject_groups_dict[group_id] = subjects\n",
    "\n",
    "# print(f\"  ‚úì S·ªë l∆∞·ª£ng kh·ªëi thi: {len(subject_groups_dict)}\")\n",
    "# print(f\"  ‚úì V√≠ d·ª• kh·ªëi thi: {dict(list(subject_groups_dict.items())[:3])}\")\n",
    "\n",
    "# # Broadcast ƒë·ªÉ worker nodes truy c·∫≠p nhanh\n",
    "# from pyspark.sql.types import FloatType, MapType, StringType\n",
    "# subject_map_broadcast = spark.sparkContext.broadcast(subject_name_to_id)\n",
    "# subject_groups_broadcast = spark.sparkContext.broadcast(subject_groups_dict)\n",
    "\n",
    "# # ==================== UDF T√çNH ƒêI·ªÇM T·ªîNG THEO KH·ªêI ====================\n",
    "# print(\"\\n[B∆∞·ªõc 3] ƒê·ªãnh nghƒ©a UDF t√≠nh ƒëi·ªÉm t·ªïng theo kh·ªëi thi...\")\n",
    "\n",
    "# def create_subject_group_scores_map(scores_map):\n",
    "#     \"\"\"\n",
    "#     UDF ch√≠nh: T√≠nh ƒëi·ªÉm t·ªïng cho T·∫§T C·∫¢ c√°c kh·ªëi thi m√† th√≠ sinh c√≥ ƒë·ªß m√¥n\n",
    "    \n",
    "#     Input:  scores_map = Map<subjectId, score> c·ªßa 1 th√≠ sinh\n",
    "#             VD: {1: 9.0, 2: 8.5, 3: 7.0, 4: 6.0}\n",
    "#                 (To√°n: 9, L√Ω: 8.5, H√≥a: 7, Sinh: 6)\n",
    "    \n",
    "#     Output: Map<subjectGroupId, totalScore> cho c√°c kh·ªëi th√≠ sinh c√≥ ƒë·ªß m√¥n\n",
    "#             VD: {\"A00\": 24.5, \"B00\": 22.0}\n",
    "    \n",
    "#     Logic:\n",
    "#     - Duy·ªát qua T·∫§T C·∫¢ c√°c kh·ªëi thi\n",
    "#     - V·ªõi m·ªói kh·ªëi, ki·ªÉm tra xem th√≠ sinh c√≥ ƒë·ªß ƒëi·ªÉm c√°c m√¥n trong kh·ªëi kh√¥ng\n",
    "#     - N·∫øu ƒë·ªß ‚Üí C·ªông t·ªïng ƒëi·ªÉm c√°c m√¥n ‚Üí L∆∞u v√†o result\n",
    "#     - N·∫øu thi·∫øu b·∫•t k·ª≥ m√¥n n√†o ‚Üí B·ªè qua kh·ªëi ƒë√≥\n",
    "#     \"\"\"\n",
    "#     if scores_map is None:\n",
    "#         return {}\n",
    "    \n",
    "#     subject_name_to_id_map = subject_map_broadcast.value\n",
    "#     subject_id_to_name_map = {v: k for k, v in subject_name_to_id_map.items()}\n",
    "#     subject_groups = subject_groups_broadcast.value\n",
    "    \n",
    "#     result = {}\n",
    "    \n",
    "#     # V·ªõi m·ªói kh·ªëi thi, ki·ªÉm tra xem th√≠ sinh c√≥ ƒë·ªß m√¥n kh√¥ng\n",
    "#     for group_id, required_subjects in subject_groups.items():\n",
    "#         total_score = 0.0\n",
    "#         has_all_subjects = True\n",
    "        \n",
    "#         # Ki·ªÉm tra t·ª´ng m√¥n trong kh·ªëi\n",
    "#         for subject_name in required_subjects:\n",
    "#             subject_id = subject_name_to_id_map.get(subject_name)\n",
    "            \n",
    "#             # N·∫øu thi·∫øu m√¥n ho·∫∑c kh√¥ng c√≥ ƒëi·ªÉm ‚Üí B·ªè qua kh·ªëi n√†y\n",
    "#             if subject_id is None or subject_id not in scores_map:\n",
    "#                 has_all_subjects = False\n",
    "#                 break\n",
    "            \n",
    "#             # C·ªông d·ªìn ƒëi·ªÉm\n",
    "#             total_score += scores_map[subject_id]\n",
    "        \n",
    "#         # Ch·ªâ th√™m v√†o result n·∫øu th√≠ sinh c√≥ ƒë·ªß T·∫§T C·∫¢ c√°c m√¥n trong kh·ªëi\n",
    "#         if has_all_subjects:\n",
    "#             result[group_id] = total_score\n",
    "    \n",
    "#     return result\n",
    "\n",
    "# create_group_scores_udf = udf(create_subject_group_scores_map, MapType(StringType(), FloatType()))\n",
    "# print(\"  ‚úì UDF ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")\n",
    "\n",
    "# # ==================== √ÅP D·ª§NG UDF ====================\n",
    "# print(\"\\n[B∆∞·ªõc 4] √Åp d·ª•ng UDF ƒë·ªÉ t√≠nh ƒëi·ªÉm t·ªïng cho t·ª´ng th√≠ sinh...\")\n",
    "# print(\"  ‚è≥ ƒêang x·ª≠ l√Ω... (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)\")\n",
    "\n",
    "# df_student_with_group_scores = df_student_scores.withColumn(\n",
    "#     \"subject_group_scores\",  # C·ªôt m·ªõi: Map<subjectGroupId, totalScore>\n",
    "#     create_group_scores_udf(col(\"scores\"))\n",
    "# )\n",
    "\n",
    "# print(\"  ‚úì Ho√†n th√†nh!\\n\")\n",
    "\n",
    "# # ==================== HI·ªÇN TH·ªä M·∫™U ====================\n",
    "# print(\"[B∆∞·ªõc 5] Xem k·∫øt qu·∫£ m·∫´u:\")\n",
    "# print(\"\\nC·∫•u tr√∫c: M·ªói th√≠ sinh c√≥ Map ch·ª©a t·∫•t c·∫£ kh·ªëi thi m√† h·ªç c√≥ th·ªÉ x√©t:\")\n",
    "# df_student_with_group_scores.select(\n",
    "#     \"studentId\", \n",
    "#     \"regionId\", \n",
    "#     \"year\", \n",
    "#     \"subject_group_scores\"\n",
    "# ).show(5, truncate=False)\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 80)\n",
    "# print(\"‚úÖ HO√ÄN TH√ÄNH PH·∫¶N 1: DataFrame 'df_student_with_group_scores' ƒë√£ s·∫µn s√†ng\")\n",
    "# print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee402c",
   "metadata": {},
   "source": [
    "#### üìä PH·∫¶N 2: X·ª≠ l√Ω v√† t·∫°o Fact Table t·ª´ d·ªØ li·ªáu ƒë√£ t√≠nh to√°n\n",
    "\n",
    "**M·ª•c ti√™u:** Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu t·ªï h·ª£p kh·ªëi thi th√†nh Fact table v·ªõi ph√¢n b·ªë ƒëi·ªÉm theo kho·∫£ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1fff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PH·∫¶N 2: X·ª¨ L√ù V√Ä T·∫†O FACT TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==================== EXPLODE MAP TH√ÄNH ROWS ====================\n",
    "print(\"\\n[B∆∞·ªõc 1] Explode Map th√†nh rows...\")\n",
    "print(\"  Chuy·ªÉn t·ª´: 1 row/th√≠ sinh ‚Üí nhi·ªÅu rows (1 row cho m·ªói kh·ªëi thi)\")\n",
    "\n",
    "df_scores_exploded = df_student_with_group_scores.select(\n",
    "    \"regionId\",\n",
    "    \"year\",\n",
    "    explode(\"subject_group_scores\").alias(\"subjectGroupId\", \"totalScore\")\n",
    ")\n",
    "\n",
    "# ==================== PH√ÇN LO·∫†I ƒêI·ªÇM V√ÄO KHO·∫¢NG ====================\n",
    "print(\"\\n[B∆∞·ªõc 2] Ph√¢n lo·∫°i ƒëi·ªÉm v√†o c√°c kho·∫£ng...\")\n",
    "print(\"  L√†m tr√≤n xu·ªëng: 24.5 ‚Üí 24.0, 22.8 ‚Üí 22.0\")\n",
    "print(\"  T·∫°o kho·∫£ng: [0-1), [1-2), [2-3), ..., [29-30)\")\n",
    "\n",
    "df_scores_exploded = df_scores_exploded.withColumn(\n",
    "    \"scoreRange\",\n",
    "    floor(col(\"totalScore\")).cast(\"float\")\n",
    ")\n",
    "\n",
    "print(\"  ‚úì Ho√†n th√†nh ph√¢n lo·∫°i ƒëi·ªÉm\")\n",
    "\n",
    "# ==================== ƒê·∫æM S·ªê L∆Ø·ª¢NG H·ªåC SINH ====================\n",
    "print(\"\\n[B∆∞·ªõc 3] ƒê·∫øm s·ªë l∆∞·ª£ng h·ªçc sinh theo t·ª´ng nh√≥m...\")\n",
    "print(\"  GROUP BY: regionId, subjectGroupId, year, scoreRange\")\n",
    "\n",
    "df_distribution = df_scores_exploded.groupBy(\n",
    "    \"regionId\",\n",
    "    \"subjectGroupId\",\n",
    "    \"year\",\n",
    "    \"scoreRange\"\n",
    ").agg(\n",
    "    count(\"*\").alias(\"quantity\")\n",
    ")\n",
    "\n",
    "# T·∫°o mapping dictionaries t·ª´ dimension tables ƒë·ªÉ tra c·ª©u nhanh (thay v√¨ JOIN)\n",
    "# Quan tr·ªçng: Normalize subjectGroupId th√†nh string v√† upper() ƒë·ªÉ ƒë·∫£m b·∫£o format nh·∫•t qu√°n\n",
    "region_id_to_key = {row['regionId']: row['regionKey'] for row in df_dim_region_gold.select(\"regionId\", \"regionKey\").collect()}\n",
    "subject_group_id_to_key = {str(row['subjectGroupId']).strip().upper(): row['subjectGroupKey'] for row in df_dim_subject_group_gold.select(\"subjectGroupId\", \"subjectGroupKey\").collect()}\n",
    "year_to_time_key = {row['year']: row['timeKey'] for row in df_dim_time_gold.select(\"year\", \"timeKey\").collect()}\n",
    "\n",
    "# Broadcast mappings\n",
    "region_map_bc = spark.sparkContext.broadcast(region_id_to_key)\n",
    "subject_group_map_bc = spark.sparkContext.broadcast(subject_group_id_to_key)\n",
    "year_map_bc = spark.sparkContext.broadcast(year_to_time_key)\n",
    "\n",
    "# UDF ƒë·ªÉ map c√°c natural keys sang surrogate keys\n",
    "# Quan tr·ªçng: Normalize subjectGroupId th√†nh string v√† upper()\n",
    "def map_to_keys(regionId, subjectGroupId, year):\n",
    "    region_key = region_map_bc.value.get(regionId)\n",
    "    # Normalize subjectGroupId tr∆∞·ªõc khi lookup (convert to string first)\n",
    "    normalized_group_id = str(subjectGroupId).strip().upper() if subjectGroupId is not None else None\n",
    "    subject_group_key = subject_group_map_bc.value.get(normalized_group_id)\n",
    "    time_key = year_map_bc.value.get(year)\n",
    "    return (region_key, subject_group_key, time_key)\n",
    "\n",
    "map_keys_udf = udf(map_to_keys, StructType([\n",
    "    StructField(\"regionKey\", IntegerType(), True),\n",
    "    StructField(\"subjectGroupKey\", IntegerType(), True),\n",
    "    StructField(\"timeKey\", IntegerType(), True)\n",
    "]))\n",
    "\n",
    "print(\"\\n[B∆∞·ªõc 5] √Åp d·ª•ng mapping...\")\n",
    "\n",
    "# √Åp d·ª•ng mapping\n",
    "df_with_keys = df_distribution.withColumn(\n",
    "    \"keys\",\n",
    "    map_keys_udf(col(\"regionId\"), col(\"subjectGroupId\"), col(\"year\"))\n",
    ")\n",
    "\n",
    "# Extract keys t·ª´ struct\n",
    "df_with_keys = df_with_keys.withColumn(\"regionKey\", col(\"keys.regionKey\")) \\\n",
    "    .withColumn(\"subjectGroupKey\", col(\"keys.subjectGroupKey\")) \\\n",
    "    .withColumn(\"timeKey\", col(\"keys.timeKey\")) \\\n",
    "    .drop(\"keys\", \"regionId\", \"subjectGroupId\", \"year\")\n",
    "\n",
    "print(\"  ‚úì Mapping ho√†n th√†nh\")\n",
    "\n",
    "# ==================== T·∫†O SURROGATE KEY ====================\n",
    "print(\"\\n[B∆∞·ªõc 6] T·∫°o primary key (SDBSGKey)...\")\n",
    "\n",
    "window_spec = Window.orderBy(\"timeKey\", \"regionKey\", \"subjectGroupKey\", \"scoreRange\")\n",
    "df_with_keys = df_with_keys.withColumn(\"SDBSGKey\", row_number().over(window_spec))\n",
    "\n",
    "print(\"  ‚úì Primary key ƒë√£ ƒë∆∞·ª£c t·∫°o\")\n",
    "\n",
    "# ==================== SELECT C√ÅC C·ªòT CU·ªêI C√ôNG ====================\n",
    "print(\"\\n[B∆∞·ªõc 7] Chu·∫©n b·ªã schema cu·ªëi c√πng...\")\n",
    "\n",
    "df_fact_score_dist_group = df_with_keys.select(\n",
    "    \"SDBSGKey\",\n",
    "    \"regionKey\",\n",
    "    \"subjectGroupKey\",\n",
    "    \"timeKey\",\n",
    "    col(\"scoreRange\").cast(\"float\"),\n",
    "    col(\"quantity\").cast(\"float\")\n",
    ")\n",
    "\n",
    "print(\"  ‚úì Schema:\")\n",
    "df_fact_score_dist_group.printSchema()\n",
    "\n",
    "# ==================== GHI V√ÄO GOLD LAYER ====================\n",
    "print(\"\\n[B∆∞·ªõc 8] Ghi v√†o Gold layer...\")\n",
    "print(\"  ‚è≥ ƒêang ghi... (partitioned by timeKey)\")\n",
    "\n",
    "df_fact_score_dist_group.writeTo(\"nessie.gold_tables.fact_score_distribution_by_subject_group\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .partitionedBy(\"timeKey\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_fact_score_dist_group.count()} d√≤ng v√†o fact_score_distribution_by_subject_group\")\n",
    "df_fact_score_dist_group.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
