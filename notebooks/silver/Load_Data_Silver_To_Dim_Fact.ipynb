{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341915fc",
   "metadata": {},
   "source": [
    "# Load Data t·ª´ Silver Layer sang Gold Layer (Dimension & Fact Tables)\n",
    "\n",
    "Notebook n√†y s·∫Ω load d·ªØ li·ªáu t·ª´ Silver layer v√† transform sang Gold layer v·ªõi:\n",
    "- Dimension tables c√≥ surrogate key t·ª± ƒë·ªông tƒÉng\n",
    "- Fact tables v·ªõi c√°c foreign keys t∆∞∆°ng ·ª©ng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c4beb8",
   "metadata": {},
   "source": [
    "## 1. Import Libraries v√† Kh·ªüi t·∫°o Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "345f5653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o!\n",
      "Log level ƒë√£ ƒë∆∞·ª£c set th√†nh ERROR\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "\n",
    "# Kh·ªüi t·∫°o Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Load_Silver_To_Gold\")\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v2\")\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://gold/\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.access-key\", \"admin\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.secret-key\", \"admin123\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# T·∫Øt log WARN - ch·ªâ hi·ªÉn th·ªã ERROR\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"Spark Session ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o!\")\n",
    "print(\"Log level ƒë√£ ƒë∆∞·ª£c set th√†nh ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cf8a02",
   "metadata": {},
   "source": [
    "## 2. Load Dimension Tables t·ª´ Silver Layer\n",
    "\n",
    "### 2.1. Dim_Time - T·∫°o b·∫£ng th·ªùi gian t·ª´ d·ªØ li·ªáu benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3eae759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_Time...\n",
      "ƒê√£ load 5 d√≤ng v√†o dim_time\n",
      "+-------+---+-----+----+\n",
      "|timeKey|day|month|year|\n",
      "+-------+---+-----+----+\n",
      "|      1|  1|    1|2021|\n",
      "|      2|  1|    1|2022|\n",
      "|      3|  1|    1|2023|\n",
      "|      4|  1|    1|2024|\n",
      "|      5|  1|    1|2025|\n",
      "+-------+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_Time...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_benchmark = spark.table(\"nessie.silver_tables.benchmark\")\n",
    "\n",
    "# T·∫°o dim_time t·ª´ c√°c year unique trong benchmark\n",
    "df_time = df_benchmark.select(\"year\").distinct()\n",
    "\n",
    "# T·∫°o day, month t·ª´ year (gi·∫£ s·ª≠ ng√†y 1/1 c·ªßa m·ªói nƒÉm)\n",
    "df_time = df_time.withColumn(\"day\", lit(1)) \\\n",
    "    .withColumn(\"month\", lit(1))\n",
    "\n",
    "# T·∫°o timeKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"year\")\n",
    "df_time = df_time.withColumn(\"timeKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt theo th·ª© t·ª±\n",
    "df_dim_time = df_time.select(\"timeKey\", \"day\", \"month\", \"year\")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_time.writeTo(\"nessie.gold_tables.dim_time\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_time.count()} d√≤ng v√†o dim_time\")\n",
    "df_dim_time.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b5f7c",
   "metadata": {},
   "source": [
    "### 2.2. Dim_Region - B·∫£ng khu v·ª±c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "042a4a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_Region...\n",
      "ƒê√£ load 64 d√≤ng v√†o dim_region\n",
      "+---------+--------+--------------------+\n",
      "|regionKey|regionId|          regionName|\n",
      "+---------+--------+--------------------+\n",
      "|        1|      01|      S·ªü GDƒêT H√† N·ªôi|\n",
      "|        2|      02|S·ªü GDƒêT TP. H·ªì Ch...|\n",
      "|        3|      03|   S·ªü GDƒêT H·∫£i Ph√≤ng|\n",
      "|        4|      04|     S·ªü GDƒêT ƒê√† N·∫µng|\n",
      "|        5|      05|    S·ªü GDƒêT H√† Giang|\n",
      "|        6|      06|    S·ªü GDƒêT Cao B·∫±ng|\n",
      "|        7|      07|    S·ªü GDƒêT Lai Ch√¢u|\n",
      "|        8|      08|     S·ªü GDƒêT L√†o Cai|\n",
      "|        9|      09| S·ªü GDƒêT Tuy√™n Quang|\n",
      "|       10|      10|    S·ªü GDƒêT L·∫°ng S∆°n|\n",
      "+---------+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_Region...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_region_silver = spark.table(\"nessie.silver_tables.region\")\n",
    "\n",
    "# T·∫°o regionKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"regionId\")\n",
    "df_dim_region = df_region_silver.withColumn(\"regionKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt\n",
    "df_dim_region = df_dim_region.select(\"regionKey\", \"regionId\", \"regionName\")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_region.writeTo(\"nessie.gold_tables.dim_region\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_region.count()} d√≤ng v√†o dim_region\")\n",
    "df_dim_region.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd737abe",
   "metadata": {},
   "source": [
    "### 2.3. Dim_School - B·∫£ng tr∆∞·ªùng ƒë·∫°i h·ªçc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb73fa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_School...\n",
      "ƒê√£ load 265 d√≤ng v√†o dim_school\n",
      "+---------+--------+--------------------+--------+\n",
      "|schoolKey|schoolId|          schoolName|province|\n",
      "+---------+--------+--------------------+--------+\n",
      "|        1|     ANH|H·ªçc vi·ªán An Ninh ...|  H√† N·ªôi|\n",
      "|        2|     ANS|ƒê·∫°i h·ªçc An Ninh N...|  H√† N·ªôi|\n",
      "|        3|     BKA|ƒê·∫°i h·ªçc B√°ch khoa...|  H√† N·ªôi|\n",
      "|        4|     BMU|ƒê·∫°i h·ªçc Bu√¥n Ma T...| ƒê·∫Øk L·∫Øk|\n",
      "|        5|     BPH| H·ªçc vi·ªán Bi√™n Ph√≤ng|  H√† N·ªôi|\n",
      "|        6|     BVH|H·ªçc vi·ªán C√¥ng ngh...|  H√† N·ªôi|\n",
      "|        7|     BVS|H·ªçc vi·ªán C√¥ng ngh...|  TP HCM|\n",
      "|        8|     CCM|ƒê·∫°i h·ªçc C√¥ng Nghi...|  H√† N·ªôi|\n",
      "|        9|     CEA|ƒê·∫°i h·ªçc Kinh T·∫ø N...| Ngh·ªá An|\n",
      "|       10|     CSH|H·ªçc vi·ªán C·∫£nh s√°t...|  H√† N·ªôi|\n",
      "+---------+--------+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_School...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_school_silver = spark.table(\"nessie.silver_tables.school\")\n",
    "\n",
    "# T·∫°o schoolKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"schoolId\")\n",
    "df_dim_school = df_school_silver.withColumn(\"schoolKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt\n",
    "df_dim_school = df_dim_school.select(\"schoolKey\", \"schoolId\", \"schoolName\", \"province\")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_school.writeTo(\"nessie.gold_tables.dim_school\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_school.count()} d√≤ng v√†o dim_school\")\n",
    "df_dim_school.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e3af4a",
   "metadata": {},
   "source": [
    "### 2.4. Dim_Major - B·∫£ng ng√†nh h·ªçc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de69025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_Major...\n",
      "ƒê√£ load 3085 d√≤ng v√†o dim_major\n",
      "+--------+-------+--------------------+\n",
      "|majorKey|majorId|           majorName|\n",
      "+--------+-------+--------------------+\n",
      "|       1|    106|   Khoa h·ªçc M√°y t√≠nh|\n",
      "|       2|    107|   K·ªπ thu·∫≠t M√°y t√≠nh|\n",
      "|       3|    108|ƒêi·ªán - ƒêi·ªán t·ª≠ - ...|\n",
      "|       4|    109|     K·ªπ Thu·∫≠t C∆° kh√≠|\n",
      "|       5|    110| K·ªπ Thu·∫≠t C∆° ƒêi·ªán t·ª≠|\n",
      "|       6|    112|           D·ªát - May|\n",
      "|       7|    114|Ho√° - Th·ª±c ph·∫©m -...|\n",
      "|       8|    115|X√¢y d·ª±ng v√† Qu·∫£n ...|\n",
      "|       9|    117|           Ki·∫øn Tr√∫c|\n",
      "|      10|    120|  D·∫ßu kh√≠ - ƒê·ªãa ch·∫•t|\n",
      "+--------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_Major...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_major_silver = spark.table(\"nessie.silver_tables.major\")\n",
    "\n",
    "# T·∫°o majorKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"majorId\")\n",
    "df_dim_major = df_major_silver.withColumn(\"majorKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt\n",
    "df_dim_major = df_dim_major.select(\"majorKey\", \"majorId\", \"majorName\")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_major.writeTo(\"nessie.gold_tables.dim_major\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_major.count()} d√≤ng v√†o dim_major\")\n",
    "df_dim_major.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8e04d",
   "metadata": {},
   "source": [
    "### 2.5. Dim_Subject - B·∫£ng m√¥n h·ªçc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba8b3584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_Subject...\n",
      "ƒê√£ load 51 d√≤ng v√†o dim_subject\n",
      "+----------+---------+--------------------+\n",
      "|subjectKey|subjectId|         subjectName|\n",
      "+----------+---------+--------------------+\n",
      "|         1|        1|bi·ªÉu di·ªÖn ngh·ªá thu·∫≠t|\n",
      "|         2|        2|C√¥ng ngh·ªá c√¥ng ng...|\n",
      "|         3|        3|C√¥ng ngh·ªá n√¥ng ng...|\n",
      "|         4|        4|                GDCD|\n",
      "|         5|        5|                 H√°t|\n",
      "|         6|        6|           H√°t & M√∫a|\n",
      "|         7|        7|H√°t ho·∫∑c bi·ªÉu di·ªÖ...|\n",
      "|         8|        8|            H√¨nh h·ªça|\n",
      "|         9|        9|                 H√≥a|\n",
      "|        10|       10|     Khoa h·ªçc x√£ h·ªôi|\n",
      "+----------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_Subject...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_subject_silver = spark.table(\"nessie.silver_tables.subject\")\n",
    "\n",
    "# T·∫°o subjectKey t·ª± ƒë·ªông tƒÉng (s·ª≠ d·ª•ng subjectId l√†m sort key)\n",
    "window_spec = Window.orderBy(\"subjectId\")\n",
    "df_dim_subject = df_subject_silver.withColumn(\"subjectKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt\n",
    "df_dim_subject = df_dim_subject.select(\"subjectKey\", \"subjectId\", \"subjectName\")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_subject.writeTo(\"nessie.gold_tables.dim_subject\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_subject.count()} d√≤ng v√†o dim_subject\")\n",
    "df_dim_subject.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfcd8ab",
   "metadata": {},
   "source": [
    "### 2.6. Dim_Subject_Group - B·∫£ng kh·ªëi thi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32f9716f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_Subject_Group...\n",
      "ƒê√£ load 232 d√≤ng v√†o dim_subject_group\n",
      "+---------------+--------------+----------------+--------------------+\n",
      "|subjectGroupKey|subjectGroupId|subjectGroupName|  subjectCombination|\n",
      "+---------------+--------------+----------------+--------------------+\n",
      "|              1|             1|             A00|         To√°n-L√≠-H√≥a|\n",
      "|              2|             2|             A01|   To√°n-L√≠-Ngo·∫°i ng·ªØ|\n",
      "|              3|             3|             A02|        To√°n-L√≠-Sinh|\n",
      "|              4|             4|             A03|          To√°n-L√≠-S·ª≠|\n",
      "|              5|             5|             A04|         To√°n-L√≠-ƒê·ªãa|\n",
      "|              6|             6|             A05|         To√°n-H√≥a-S·ª≠|\n",
      "|              7|             7|             A06|        To√°n-H√≥a-ƒê·ªãa|\n",
      "|              8|             8|             A07|         To√°n-S·ª≠-ƒê·ªãa|\n",
      "|              9|             9|             A0C|To√°n-L√≠-C√¥ng ngh·ªá...|\n",
      "|             10|            10|             A0T|     To√°n-L√≠-Tin h·ªçc|\n",
      "+---------------+--------------+----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_Subject_Group...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_subject_group_silver = spark.table(\"nessie.silver_tables.subject_group\")\n",
    "\n",
    "# T·∫°o subjectGroupKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"subjectGroupId\")\n",
    "df_dim_subject_group = df_subject_group_silver.withColumn(\"subjectGroupKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt\n",
    "df_dim_subject_group = df_dim_subject_group.select(\n",
    "    \"subjectGroupKey\", \n",
    "    \"subjectGroupId\", \n",
    "    \"subjectGroupName\", \n",
    "    \"subjectCombination\"\n",
    ")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_subject_group.writeTo(\"nessie.gold_tables.dim_subject_group\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_subject_group.count()} d√≤ng v√†o dim_subject_group\")\n",
    "df_dim_subject_group.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47b43b1",
   "metadata": {},
   "source": [
    "### 2.7. Dim_Selection_Method - B·∫£ng ph∆∞∆°ng th·ª©c x√©t tuy·ªÉn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d64da34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_Selection_Method...\n",
      "ƒê√£ load 10 d√≤ng v√†o dim_selection_method\n",
      "+------------------+-----------------+--------------------+\n",
      "|selectionMethodKey|selectionMethodId| selectionMethodName|\n",
      "+------------------+-----------------+--------------------+\n",
      "|                 1|                1|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 2|                2|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 3|                3|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 4|                4|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 5|                5|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 6|                6|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 7|                7|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 8|                8|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                 9|                9|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "|                10|               10|ƒêi·ªÉm chu·∫©n theo p...|\n",
      "+------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_Selection_Method...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_selection_method_silver = spark.table(\"nessie.silver_tables.selection_method\")\n",
    "\n",
    "# T·∫°o selectionMethodKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"selectionMethodId\")\n",
    "df_dim_selection_method = df_selection_method_silver.withColumn(\"selectionMethodKey\", row_number().over(window_spec))\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i c·ªôt\n",
    "df_dim_selection_method = df_dim_selection_method.select(\n",
    "    \"selectionMethodKey\", \n",
    "    \"selectionMethodId\", \n",
    "    \"selectionMethodName\"\n",
    ")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_dim_selection_method.writeTo(\"nessie.gold_tables.dim_selection_method\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_selection_method.count()} d√≤ng v√†o dim_selection_method\")\n",
    "df_dim_selection_method.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53364cbc-b3a8-4617-85fe-cda6cec24e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Dim_Grading_Scale...\n",
      "ƒê√£ load 10 d√≤ng v√†o dim_grading_scale\n",
      "+---------------+--------------+------+---------------+\n",
      "|gradingScaleKey|gradingScaleId|value |description    |\n",
      "+---------------+--------------+------+---------------+\n",
      "|1              |0             |30.0  |Thang ƒëi·ªÉm 30  |\n",
      "|2              |1             |1200.0|Thang ƒëi·ªÉm 1200|\n",
      "|3              |2             |40.0  |Thang ƒëi·ªÉm 40  |\n",
      "|4              |3             |50.0  |Thang ƒëi·ªÉm 50  |\n",
      "|5              |4             |150.0 |Thang ƒëi·ªÉm 150 |\n",
      "|6              |5             |10.0  |Thang ƒëi·ªÉm 10  |\n",
      "|7              |6             |100.0 |Thang ƒëi·ªÉm 100 |\n",
      "|8              |7             |120.0 |Thang ƒëi·ªÉm 120 |\n",
      "|9              |8             |35.0  |Thang ƒëi·ªÉm 35  |\n",
      "|10             |9             |90.0  |Thang ƒëi·ªÉm 90  |\n",
      "+---------------+--------------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Dim_Grading_Scale...\")\n",
    "\n",
    "# 1. ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_grading_scale_silver = spark.table(\"nessie.silver_tables.grading_scale\")\n",
    "\n",
    "# 2. T·∫°o gradingScaleKey t·ª± ƒë·ªông tƒÉng (surrogate key)\n",
    "window_spec = Window.orderBy(\"gradingScaleId\")\n",
    "\n",
    "df_dim_grading_scale = df_grading_scale_silver.withColumn(\n",
    "    \"gradingScaleKey\",\n",
    "    row_number().over(window_spec)\n",
    ")\n",
    "\n",
    "# 3. S·∫Øp x·∫øp l·∫°i c·ªôt ƒë√∫ng th·ª© t·ª± trong GOLD schema\n",
    "df_dim_grading_scale = df_dim_grading_scale.select(\n",
    "    \"gradingScaleKey\",\n",
    "    \"gradingScaleId\",\n",
    "    \"value\",\n",
    "    \"description\"\n",
    ")\n",
    "\n",
    "# 4. Ghi v√†o Gold layer\n",
    "df_dim_grading_scale.writeTo(\"nessie.gold_tables.dim_grading_scale\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_dim_grading_scale.count()} d√≤ng v√†o dim_grading_scale\")\n",
    "df_dim_grading_scale.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f18c813",
   "metadata": {},
   "source": [
    "## 3. Load Fact Tables t·ª´ Silver Layer\n",
    "\n",
    "### 3.1. Fact_Benchmark - Load v√† join v·ªõi c√°c Dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2fd8f72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Fact_Benchmark...\n",
      "Sau t·∫•t c·∫£ join: 163,399 d√≤ng\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ load 163399 d√≤ng v√†o fact_benchmark\n",
      "+------------+---------------+-------+--------+---------+------------------+---------------+-----+---------------+--------------+---------------+----------------+\n",
      "|benchmarkKey|subjectGroupKey|timeKey|majorKey|schoolKey|selectionMethodKey|gradingScaleKey|score|avgScoreByMajor|yearlyScoreGap|rankAmongMajors|rankAmongSchools|\n",
      "+------------+---------------+-------+--------+---------+------------------+---------------+-----+---------------+--------------+---------------+----------------+\n",
      "|       14128|              1|      1|    NULL|      179|                 2|              1|23.25|          23.25|           0.0|              9|               1|\n",
      "|       14129|              2|      1|    NULL|      179|                 2|              1|23.25|          23.25|           0.0|              9|               1|\n",
      "|       14130|             35|      1|    NULL|      179|                 2|              1|23.25|          23.25|           0.0|              9|               1|\n",
      "|       14127|           NULL|      1|    NULL|      179|                 5|              2|722.0|          722.0|           0.0|             21|               1|\n",
      "|       15754|              1|      1|      41|      204|                 2|              1| 26.7|           26.7|           0.0|             14|               1|\n",
      "|       15755|              2|      1|      41|      204|                 2|              1| 26.7|           26.7|           0.0|             14|               1|\n",
      "|       15756|             35|      1|      41|      204|                 2|              1| 26.7|           26.7|           0.0|             14|               1|\n",
      "|       15757|             41|      1|      41|      204|                 2|              1| 26.7|           26.7|           0.0|             14|               1|\n",
      "|        1645|            113|      1|      66|       16|                 1|              1| 19.5|           19.5|           0.0|              4|               1|\n",
      "|       16963|           NULL|      1|      66|      215|                 1|              1| 19.5|           19.5|           0.0|             21|               2|\n",
      "+------------+---------------+-------+--------+---------+------------------+---------------+-----+---------------+--------------+---------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Fact_Benchmark...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_benchmark_silver = spark.table(\"nessie.silver_tables.benchmark\")\n",
    "\n",
    "# ƒê·ªçc c√°c dimension tables t·ª´ Gold ƒë·ªÉ join\n",
    "df_dim_time_gold = spark.table(\"nessie.gold_tables.dim_time\")\n",
    "df_dim_school_gold = spark.table(\"nessie.gold_tables.dim_school\")\n",
    "df_dim_major_gold = spark.table(\"nessie.gold_tables.dim_major\")\n",
    "df_dim_subject_group_gold = spark.table(\"nessie.gold_tables.dim_subject_group\")\n",
    "df_dim_selection_method_gold = spark.table(\"nessie.gold_tables.dim_selection_method\")\n",
    "df_dim_grading_scale_gold = spark.table(\"nessie.gold_tables.dim_grading_scale\")\n",
    "\n",
    "# Join t·ª´ng b∆∞·ªõc ƒë·ªÉ tr√°nh Cartesian product\n",
    "# B∆∞·ªõc 1: Join v·ªõi dim_time\n",
    "df_fact = df_benchmark_silver.join(\n",
    "    df_dim_time_gold, \n",
    "    df_benchmark_silver[\"year\"] == df_dim_time_gold[\"year\"], \n",
    "    \"left\"\n",
    ").drop(df_dim_time_gold[\"year\"])\n",
    "\n",
    "# B∆∞·ªõc 2: Join v·ªõi dim_school\n",
    "df_fact = df_fact.join(\n",
    "    df_dim_school_gold, \n",
    "    df_fact[\"schoolId\"] == df_dim_school_gold[\"schoolId\"], \n",
    "    \"left\"\n",
    ").drop(df_dim_school_gold[\"schoolId\"])\n",
    "\n",
    "# B∆∞·ªõc 3: Join v·ªõi dim_major\n",
    "df_fact = df_fact.join(\n",
    "    df_dim_major_gold, \n",
    "    df_fact[\"majorId\"] == df_dim_major_gold[\"majorId\"], \n",
    "    \"left\"\n",
    ").drop(df_dim_major_gold[\"majorId\"])\n",
    "\n",
    "# B∆∞·ªõc 4: Join v·ªõi dim_subject_group\n",
    "df_fact = df_fact.join(\n",
    "    df_dim_subject_group_gold, \n",
    "    df_fact[\"subjectGroupId\"] == df_dim_subject_group_gold[\"subjectGroupId\"], \n",
    "    \"left\"\n",
    ").drop(df_dim_subject_group_gold[\"subjectGroupId\"])\n",
    "\n",
    "# B∆∞·ªõc 5: Join v·ªõi dim_selection_method\n",
    "df_fact = df_fact.join(\n",
    "    df_dim_selection_method_gold, \n",
    "    df_fact[\"selectionMethodId\"] == df_dim_selection_method_gold[\"selectionMethodId\"], \n",
    "    \"left\"\n",
    ").drop(df_dim_selection_method_gold[\"selectionMethodId\"])\n",
    "\n",
    "# B∆∞·ªõc 6: Join v·ªõi dim_grading_scale\n",
    "df_fact = df_fact.join(\n",
    "    df_dim_grading_scale_gold,\n",
    "    df_fact[\"gradingScaleId\"] == df_dim_grading_scale_gold[\"gradingScaleId\"],\n",
    "    \"left\"\n",
    ").drop(df_dim_grading_scale_gold[\"gradingScaleId\"])\n",
    "\n",
    "print(f\"Sau t·∫•t c·∫£ join: {df_fact.count():,} d√≤ng\")\n",
    "\n",
    "# T·∫°o benchmarkKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"timeKey\", \"schoolKey\", \"majorKey\")\n",
    "df_fact = df_fact.withColumn(\"benchmarkKey\", row_number().over(window_spec))\n",
    "\n",
    "# T√≠nh ƒëi·ªÉm trung b√¨nh cho m·ªói ng√†nh theo t·ª´ng ph∆∞∆°ng th·ª©c\n",
    "window_avg_spec = Window.partitionBy(\"timeKey\", \"schoolKey\", \"majorKey\", \"selectionMethodKey\", \"gradingScaleKey\")\n",
    "df_fact = df_fact.withColumn(\n",
    "    \"avgScoreByMajor\",\n",
    "    round(avg(\"score\").over(window_avg_spec), 2)\n",
    ")\n",
    "\n",
    "# T√≠nh to√°n yearlyScoreGap\n",
    "window_year_spec = Window.partitionBy(\"schoolKey\", \"majorKey\", \"selectionMethodKey\", \"gradingScaleKey\",\"subjectGroupKey\").orderBy(\"timeKey\")\n",
    "df_fact = df_fact.withColumn(\n",
    "    \"yearlyScoreGap\",\n",
    "    coalesce(round(col(\"avgScoreByMajor\") - lag(\"avgScoreByMajor\", 1).over(window_year_spec),2), lit(0))\n",
    ")\n",
    "\n",
    "# X·∫øp h·∫°ng ng√†nh trong tr∆∞·ªùng\n",
    "window_major_in_school_spec = Window.partitionBy(\"timeKey\", \"schoolKey\", \"selectionMethodKey\", \"gradingScaleKey\").orderBy(desc(\"avgScoreByMajor\"), \"majorKey\")\n",
    "df_fact = df_fact.withColumn(\"rankAmongMajors\", dense_rank().over(window_major_in_school_spec))\n",
    "\n",
    "# X·∫øp h·∫°ng ng√†nh so v·ªõi c√°c tr∆∞·ªùng kh√°c\n",
    "window_major_across_schools_spec = Window.partitionBy(\"timeKey\", \"majorKey\", \"selectionMethodKey\", \"gradingScaleKey\").orderBy(desc(\"avgScoreByMajor\"), \"schoolKey\")\n",
    "df_fact = df_fact.withColumn(\"rankAmongSchools\", dense_rank().over(window_major_across_schools_spec))\n",
    "\n",
    "# Select c√°c c·ªôt cu·ªëi c√πng\n",
    "df_fact_benchmark = df_fact.select(\n",
    "    \"benchmarkKey\",\n",
    "    \"subjectGroupKey\",\n",
    "    \"timeKey\",\n",
    "    \"majorKey\",\n",
    "    \"schoolKey\",\n",
    "    \"selectionMethodKey\",\n",
    "    \"gradingScaleKey\",\n",
    "    col(\"score\").cast(\"float\"),\n",
    "    col(\"avgScoreByMajor\").cast(\"float\"),\n",
    "    col(\"yearlyScoreGap\").cast(\"float\"),\n",
    "    \"rankAmongMajors\",\n",
    "    \"rankAmongSchools\"\n",
    ")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_fact_benchmark.writeTo(\"nessie.gold_tables.fact_benchmark\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .partitionedBy(\"timeKey\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_fact_benchmark.count()} d√≤ng v√†o fact_benchmark\")\n",
    "df_fact_benchmark.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6025881c",
   "metadata": {},
   "source": [
    "### 3.2. Fact_Score_Distribution_By_Subject - Ph√¢n b·ªë ƒëi·ªÉm theo m√¥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e2a323f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load Fact_Score_Distribution_By_Subject...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ load 100999 d√≤ng v√†o fact_score_distribution_by_subject\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 287:==============>                                          (1 + 3) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+-------+--------------+--------+\n",
      "|SDBSKey|regionKey|subjectKey|timeKey|scoreThreshold|quantity|\n",
      "+-------+---------+----------+-------+--------------+--------+\n",
      "|      1|        1|         4|      1|           0.0|    11.0|\n",
      "|      2|        1|         4|      1|           0.4|     1.0|\n",
      "|      3|        1|         4|      1|           1.4|     2.0|\n",
      "|      4|        1|         4|      1|           2.0|     2.0|\n",
      "|      5|        1|         4|      1|           2.2|     7.0|\n",
      "|      6|        1|         4|      1|           2.4|     5.0|\n",
      "|      7|        1|         4|      1|           2.6|     2.0|\n",
      "|      8|        1|         4|      1|           3.0|    12.0|\n",
      "|      9|        1|         4|      1|           3.2|    19.0|\n",
      "|     10|        1|         4|      1|           3.4|    25.0|\n",
      "+-------+---------+----------+-------+--------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"ƒêang load Fact_Score_Distribution_By_Subject...\")\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver\n",
    "df_student_scores = spark.table(\"nessie.silver_tables.student_scores\")\n",
    "\n",
    "# ƒê·ªçc dimension tables\n",
    "df_dim_time_gold = spark.table(\"nessie.gold_tables.dim_time\")\n",
    "df_dim_region_gold = spark.table(\"nessie.gold_tables.dim_region\")\n",
    "df_dim_subject_gold = spark.table(\"nessie.gold_tables.dim_subject\")\n",
    "\n",
    "# Explode map scores th√†nh t·ª´ng d√≤ng (subjectId - ƒëi·ªÉm)\n",
    "# V√¨ scores gi·ªù l√† Map<Int, Float> (subjectId -> score)\n",
    "# subjectId ·ªü ƒë√¢y l√† INTEGER, ch√≠nh l√† c·ªôt subjectId t·ª´ b·∫£ng silver.subject\n",
    "df_scores_exploded = df_student_scores.select(\n",
    "    \"regionId\",\n",
    "    \"year\",\n",
    "    explode(\"scores\").alias(\"subjectId\", \"score\")  # subjectId = key c·ªßa map (Int)\n",
    ")\n",
    "\n",
    "# Ph√¢n lo·∫°i ƒëi·ªÉm v√†o c√°c m·ªëc ƒëi·ªÉm (0.2, 0.4, 0.6, ..., 9.8, 10.0)\n",
    "# L√†m tr√≤n ƒëi·ªÉm ƒë·∫øn b·ªôi s·ªë c·ªßa 0.2 g·∫ßn nh·∫•t\n",
    "df_scores_exploded = df_scores_exploded.withColumn(\n",
    "    \"scoreThreshold\",\n",
    "    (floor(col(\"score\") / 0.2) * 0.2).cast(\"float\")\n",
    ")\n",
    "\n",
    "# ƒê·∫øm s·ªë l∆∞·ª£ng h·ªçc sinh theo t·ª´ng m√¥n, khu v·ª±c, nƒÉm v√† ng∆∞·ª°ng ƒëi·ªÉm\n",
    "df_distribution = df_scores_exploded.groupBy(\n",
    "    \"regionId\",\n",
    "    \"subjectId\",  # ƒê√¢y l√† subjectId (Int) t·ª´ scores map\n",
    "    \"year\",\n",
    "    \"scoreThreshold\"\n",
    ").agg(\n",
    "    count(\"*\").alias(\"quantity\")\n",
    ")\n",
    "\n",
    "# Join v·ªõi c√°c dimension tables\n",
    "# Join v·ªõi dim_subject b·∫±ng subjectId ƒë·ªÉ l·∫•y subjectKey\n",
    "df_fact = df_distribution \\\n",
    "    .join(df_dim_region_gold, df_distribution[\"regionId\"] == df_dim_region_gold[\"regionId\"], \"left\") \\\n",
    "    .join(df_dim_subject_gold, df_distribution[\"subjectId\"] == df_dim_subject_gold[\"subjectId\"], \"left\") \\\n",
    "    .join(df_dim_time_gold, df_distribution[\"year\"] == df_dim_time_gold[\"year\"], \"left\")\n",
    "\n",
    "# T·∫°o SDBSKey t·ª± ƒë·ªông tƒÉng\n",
    "window_spec = Window.orderBy(\"timeKey\", \"regionKey\", \"subjectKey\", \"scoreThreshold\")\n",
    "df_fact = df_fact.withColumn(\"SDBSKey\", row_number().over(window_spec))\n",
    "\n",
    "# Select c√°c c·ªôt cu·ªëi c√πng\n",
    "# L∆∞u √Ω: L·∫•y subjectKey t·ª´ df_dim_subject_gold (kh√¥ng ph·∫£i subjectId)\n",
    "df_fact_score_dist_subject = df_fact.select(\n",
    "    \"SDBSKey\",\n",
    "    df_dim_region_gold[\"regionKey\"],\n",
    "    df_dim_subject_gold[\"subjectKey\"],  # subjectKey t·ª´ dim_subject (surrogate key)\n",
    "    df_dim_time_gold[\"timeKey\"],\n",
    "    col(\"scoreThreshold\").cast(\"float\"),\n",
    "    col(\"quantity\").cast(\"float\")\n",
    ")\n",
    "\n",
    "# Ghi v√†o Gold layer\n",
    "df_fact_score_dist_subject.writeTo(\"nessie.gold_tables.fact_score_distribution_by_subject\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .partitionedBy(\"timeKey\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_fact_score_dist_subject.count()} d√≤ng v√†o fact_score_distribution_by_subject\")\n",
    "df_fact_score_dist_subject.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bad2e2",
   "metadata": {},
   "source": [
    "### 3.3. Fact_Score_Distribution_By_Subject_Group - Ph√¢n b·ªë ƒëi·ªÉm theo kh·ªëi thi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73d6eac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PH·∫¶N 1: T√çNH TO√ÅN T·ªî H·ª¢P KH·ªêI THI CHO M·ªñI TH√ç SINH (C√ì DEBUG LOG)\n",
      "================================================================================\n",
      "\n",
      "[B∆∞·ªõc 1] ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver layer...\n",
      "  ‚úì S·ªë l∆∞·ª£ng th√≠ sinh: 5,190,113\n",
      "  ‚úì S·ªë l∆∞·ª£ng kh·ªëi thi: 232\n",
      "  ‚úì S·ªë l∆∞·ª£ng m√¥n h·ªçc: 51\n",
      "\n",
      "[B∆∞·ªõc 2] T·∫°o mapping dictionaries...\n",
      "  ‚úì Subject mapping: {'bi·ªÉu di·ªÖn ngh·ªá thu·∫≠t': 1, 'C√¥ng ngh·ªá c√¥ng nghi·ªáp': 2, 'C√¥ng ngh·ªá n√¥ng nghi·ªáp': 3, 'GDCD': 4, 'H√°t': 5, 'H√°t & M√∫a': 6, 'H√°t ho·∫∑c bi·ªÉu di·ªÖn nh·∫°c c·ª•': 7, 'H√¨nh h·ªça': 8, 'H√≥a': 9, 'Khoa h·ªçc x√£ h·ªôi': 10, 'K√Ω x∆∞·ªõng √¢m': 11, 'L√≠': 12, 'L√≠/H√≥a/Sinh/Tin': 13, 'Ngo·∫°i ng·ªØ': 14, 'NK M·∫ßm non 1(k·ªÉ chuy·ªán & ƒê·ªçc di·ªÖn c·∫£m)': 15, 'NK M·∫ßm non 2 (H√°t)': 16, 'NK1': 17, 'NƒÉng khi·∫øu': 18, 'NƒÉng khi·∫øu 1': 19, 'NƒÉng khi·∫øu 2': 20, 'NƒÉng khi·∫øu b√°o ch√≠': 21, 'NƒÉng khi·∫øu Gi√°o d·ª•c m·∫ßm non': 22, 'NƒÉng khi·∫øu SKƒêA 1': 23, 'NƒÉng khi·∫øu SKƒêA 2': 24, 'NƒÉng khi·∫øu Th·ªÉ d·ª•c th·ªÉ thao': 25, 'NƒÉng khi·∫øu vƒÉn H√≥a ngh·ªá thu·∫≠t': 26, 'NƒÉng khi·∫øu v·∫Ω Ngh·ªá thu·∫≠t 1': 27, 'NƒÉng khi·∫øu v·∫Ω Ngh·ªá thu·∫≠t 2': 28, 'NƒÉng khi·∫øu √Çm nh·∫°c 1': 29, 'NƒÉng khi·∫øu √Çm nh·∫°c 2': 30, 'NƒÉng khi·∫øu ƒë·ªçc di·ªÖn c·∫£m v√† H√°t': 31, 'Sinh': 32, 'S·ª≠': 33, 'Tin h·ªçc': 34, 'Ti·∫øng H√†n': 35, 'Ti·∫øng Nga': 36, 'Ti·∫øng Nh·∫≠t': 37, 'Ti·∫øng Ph√°p': 38, 'Ti·∫øng Trung': 39, 'Ti·∫øng ƒê·ª©c': 40, 'To√°n': 41, 'Trang tr√≠': 42, 'VƒÉn': 43, 'V·∫Ω H√¨nh h·ªça m·ªπ thu·∫≠t': 44, 'V·∫Ω m·ªπ thu·∫≠t': 45, 'V·∫Ω NƒÉng khi·∫øu': 46, 'V·∫Ω trang tr√≠ m√†u': 47, 'X√¢y d·ª±ng k·ªãch b·∫£n s·ª± ki·ªán': 48, 'x∆∞·ªõng √¢m': 49, 'ƒê·ªãa': 50, 'ƒê·ªçc di·ªÖn c·∫£m': 51}\n",
      "  ‚úì S·ªë l∆∞·ª£ng kh·ªëi thi: 232\n",
      "\n",
      "  üìã Chi ti·∫øt c√°c kh·ªëi thi:\n",
      "     - 25: ['VƒÉn', 'To√°n', 'ƒê·ªãa']\n",
      "     - 24: ['VƒÉn', 'To√°n', 'S·ª≠']\n",
      "     - 26: ['VƒÉn', 'L√≠', 'H√≥a']\n",
      "     - 184: ['To√°n', 'H√≥a', 'C√¥ng ngh·ªá n√¥ng nghi·ªáp']\n",
      "     - 81: ['VƒÉn', 'H√≥a', 'Ti·∫øng Trung']\n",
      "     ... v√† 227 kh·ªëi thi kh√°c\n",
      "T·ªïng s·ªë kh·ªëi thi trong h·ªá th·ªëng: 232\n",
      "\n",
      "[B∆∞·ªõc 4] ƒê·ªãnh nghƒ©a UDF t√≠nh ƒëi·ªÉm t·ªïng theo kh·ªëi thi (c√≥ debug logging)...\n",
      "  ‚úì UDF ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a v·ªõi debug logging\n",
      "[B∆∞·ªõc 7] Ph√¢n t√≠ch k·∫øt qu·∫£:\n",
      "\n",
      "  üìä Xem m·∫´u k·∫øt qu·∫£ (5 th√≠ sinh):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|studentId   |regionId|year|subject_group_scores                                                                                                                                                                                               |\n",
      "+------------+--------+----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|010000022022|01      |2022|{193 -> 24.15, 226 -> 22.5, 35 -> 22.75, 197 -> 24.25, 230 -> 22.6, 8 -> 24.4, 43 -> 24.5, 173 -> 23.4, 46 -> 23.5, 50 -> 22.85, 51 -> 21.85, 21 -> 22.75, 24 -> 23.65, 25 -> 22.65, 189 -> 25.15, 222 -> 23.5}    |\n",
      "|010000072022|01      |2022|{193 -> 23.7, 226 -> 22.5, 35 -> 18.2, 197 -> 21.2, 230 -> 20.0, 8 -> 22.7, 43 -> 20.2, 173 -> 22.2, 46 -> 19.7, 50 -> 19.0, 51 -> 18.5, 21 -> 21.5, 24 -> 21.2, 25 -> 20.7, 189 -> 24.2, 222 -> 23.0}             |\n",
      "|010000082025|01      |2025|{21 -> 14.25}                                                                                                                                                                                                      |\n",
      "|010000102024|01      |2024|{193 -> 22.15, 226 -> 23.75, 35 -> 26.0, 197 -> 25.25, 230 -> 26.85, 8 -> 22.15, 43 -> 25.25, 173 -> 24.65, 46 -> 23.5, 50 -> 26.85, 51 -> 25.1, 21 -> 23.75, 24 -> 24.65, 25 -> 22.9, 189 -> 23.9, 222 -> 25.5}   |\n",
      "|010000112024|01      |2024|{1 -> 21.7, 2 -> 23.95, 3 -> 21.2, 35 -> 24.7, 41 -> 22.95, 42 -> 22.45, 47 -> 25.25, 15 -> 20.2, 48 -> 24.25, 49 -> 23.75, 18 -> 21.95, 19 -> 22.45, 22 -> 23.45, 23 -> 22.45, 26 -> 23.0, 27 -> 22.5, 29 -> 21.5}|\n",
      "+------------+--------+----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "  üìà Th·ªëng k√™ s·ªë l∆∞·ª£ng kh·ªëi thi m√† m·ªói th√≠ sinh c√≥ th·ªÉ x√©t:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|num_eligible_groups|s·ªë_th√≠_sinh|\n",
      "+-------------------+-----------+\n",
      "|                  0|      13506|\n",
      "|                  1|     378411|\n",
      "|                  2|        434|\n",
      "|                  3|      26159|\n",
      "|                  4|    1228278|\n",
      "|                  5|       5590|\n",
      "|                  6|      12577|\n",
      "|                  7|       2943|\n",
      "|                  8|        912|\n",
      "|                  9|      53159|\n",
      "|                 10|        148|\n",
      "|                 16|    2198676|\n",
      "|                 17|    1269320|\n",
      "+-------------------+-----------+\n",
      "\n",
      "\n",
      "  üìä Th·ªëng k√™ t·ªïng quan:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 355:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     - S·ªë kh·ªëi TB m·ªói th√≠ sinh c√≥ th·ªÉ x√©t: 12.09\n",
      "     - S·ªë kh·ªëi t·ªëi ƒëa: 17\n",
      "     - S·ªë kh·ªëi t·ªëi thi·ªÉu: 0\n",
      "     - T·ªïng s·ªë kh·ªëi thi trong h·ªá th·ªëng: 232\n",
      "\n",
      "================================================================================\n",
      "‚úÖ HO√ÄN TH√ÄNH PH·∫¶N 1: DataFrame 'df_student_with_group_scores' ƒë√£ s·∫µn s√†ng\n",
      "   v·ªõi th√¥ng tin chi ti·∫øt v·ªÅ c√°c kh·ªëi thi m√† m·ªói th√≠ sinh c√≥ th·ªÉ x√©t\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PH·∫¶N 1: T√çNH TO√ÅN T·ªî H·ª¢P KH·ªêI THI CHO M·ªñI TH√ç SINH (C√ì DEBUG LOG)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==================== ƒê·ªåC D·ªÆ LI·ªÜU ====================\n",
    "print(\"\\n[B∆∞·ªõc 1] ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver layer...\")\n",
    "df_student_scores = spark.table(\"nessie.silver_tables.student_scores\")\n",
    "df_subject_group = spark.table(\"nessie.silver_tables.subject_group\")\n",
    "df_subject = spark.table(\"nessie.silver_tables.subject\")\n",
    "\n",
    "print(f\"  ‚úì S·ªë l∆∞·ª£ng th√≠ sinh: {df_student_scores.count():,}\")\n",
    "print(f\"  ‚úì S·ªë l∆∞·ª£ng kh·ªëi thi: {df_subject_group.count()}\")\n",
    "print(f\"  ‚úì S·ªë l∆∞·ª£ng m√¥n h·ªçc: {df_subject.count()}\")\n",
    "\n",
    "# ==================== T·∫†O MAPPING DICTIONARIES ====================\n",
    "print(\"\\n[B∆∞·ªõc 2] T·∫°o mapping dictionaries...\")\n",
    "\n",
    "# Mapping: T√™n m√¥n ‚Üí ID m√¥n (VD: \"To√°n\" ‚Üí 1)\n",
    "subject_name_to_id = {row['subjectName']: row['subjectId'] for row in df_subject.collect()}\n",
    "print(f\"  ‚úì Subject mapping: {subject_name_to_id}\")\n",
    "\n",
    "# Mapping: ID kh·ªëi ‚Üí Danh s√°ch m√¥n (VD: \"A00\" ‚Üí [\"To√°n\", \"L√Ω\", \"H√≥a\"])\n",
    "subject_groups_dict = {}\n",
    "for row in df_subject_group.collect():\n",
    "    group_id = row['subjectGroupId']\n",
    "    subjects = [s.strip() for s in row['subjectCombination'].split('-')]\n",
    "    subject_groups_dict[group_id] = subjects\n",
    "\n",
    "print(f\"  ‚úì S·ªë l∆∞·ª£ng kh·ªëi thi: {len(subject_groups_dict)}\")\n",
    "print(f\"\\n  üìã Chi ti·∫øt c√°c kh·ªëi thi:\")\n",
    "for group_id, subjects in list(subject_groups_dict.items())[:5]:\n",
    "    print(f\"     - {group_id}: {subjects}\")\n",
    "print(f\"     ... v√† {len(subject_groups_dict) - 5} kh·ªëi thi kh√°c\")\n",
    "\n",
    "# Broadcast ƒë·ªÉ worker nodes truy c·∫≠p nhanh\n",
    "from pyspark.sql.types import FloatType, MapType, StringType\n",
    "subject_map_broadcast = spark.sparkContext.broadcast(subject_name_to_id)\n",
    "subject_groups_broadcast = spark.sparkContext.broadcast(subject_groups_dict)\n",
    "\n",
    "print(f\"T·ªïng s·ªë kh·ªëi thi trong h·ªá th·ªëng: {len(subject_groups_dict)}\")\n",
    "\n",
    "# ==================== UDF T√çNH ƒêI·ªÇM T·ªîNG THEO KH·ªêI (C√ì DEBUG) ====================\n",
    "print(\"\\n[B∆∞·ªõc 4] ƒê·ªãnh nghƒ©a UDF t√≠nh ƒëi·ªÉm t·ªïng theo kh·ªëi thi (c√≥ debug logging)...\")\n",
    "\n",
    "# Bi·∫øn global ƒë·ªÉ ƒë·∫øm s·ªë l·∫ßn g·ªçi UDF (ch·ªâ d√πng ƒë·ªÉ demo)\n",
    "import sys\n",
    "\n",
    "def create_subject_group_scores_map_debug(scores_map):\n",
    "    \"\"\"\n",
    "    UDF ch√≠nh v·ªõi DEBUG LOGGING: T√≠nh ƒëi·ªÉm t·ªïng cho T·∫§T C·∫¢ c√°c kh·ªëi thi m√† th√≠ sinh c√≥ ƒë·ªß m√¥n\n",
    "    \n",
    "    Input:  scores_map = Map<subjectId, score> c·ªßa 1 th√≠ sinh\n",
    "    Output: Map<subjectGroupId, totalScore> cho c√°c kh·ªëi th√≠ sinh c√≥ ƒë·ªß m√¥n\n",
    "    \"\"\"\n",
    "    if scores_map is None:\n",
    "        return {}\n",
    "    \n",
    "    subject_name_to_id_map = subject_map_broadcast.value\n",
    "    subject_id_to_name_map = {v: k for k, v in subject_name_to_id_map.items()}\n",
    "    subject_groups = subject_groups_broadcast.value\n",
    "    \n",
    "    # DEBUG: In ra ƒëi·ªÉm c·ªßa th√≠ sinh (ch·ªâ in 3 th√≠ sinh ƒë·∫ßu ƒë·ªÉ tr√°nh spam log)\n",
    "    student_subjects = {subject_id_to_name_map.get(sid, f\"ID_{sid}\"): score \n",
    "                       for sid, score in scores_map.items()}\n",
    "    \n",
    "    result = {}\n",
    "    groups_eligible = []\n",
    "    groups_not_eligible = []\n",
    "    \n",
    "    # V·ªõi m·ªói kh·ªëi thi, ki·ªÉm tra xem th√≠ sinh c√≥ ƒë·ªß m√¥n kh√¥ng\n",
    "    for group_id, required_subjects in subject_groups.items():\n",
    "        total_score = 0.0\n",
    "        has_all_subjects = True\n",
    "        missing_subjects = []\n",
    "        \n",
    "        # Ki·ªÉm tra t·ª´ng m√¥n trong kh·ªëi\n",
    "        for subject_name in required_subjects:\n",
    "            subject_id = subject_name_to_id_map.get(subject_name)\n",
    "            \n",
    "            # N·∫øu thi·∫øu m√¥n ho·∫∑c kh√¥ng c√≥ ƒëi·ªÉm ‚Üí B·ªè qua kh·ªëi n√†y\n",
    "            if subject_id is None or subject_id not in scores_map:\n",
    "                has_all_subjects = False\n",
    "                missing_subjects.append(subject_name)\n",
    "                break\n",
    "            \n",
    "            # C·ªông d·ªìn ƒëi·ªÉm\n",
    "            total_score += scores_map[subject_id]\n",
    "        \n",
    "        # Ch·ªâ th√™m v√†o result n·∫øu th√≠ sinh c√≥ ƒë·ªß T·∫§T C·∫¢ c√°c m√¥n trong kh·ªëi\n",
    "        if has_all_subjects:\n",
    "            result[group_id] = total_score\n",
    "            groups_eligible.append(f\"{group_id}={total_score:.1f}\")\n",
    "        else:\n",
    "            groups_not_eligible.append(f\"{group_id}(thi·∫øu {','.join(missing_subjects)})\")\n",
    "    \n",
    "    # DEBUG: In th√¥ng tin (ch·ªâ in cho 3 th√≠ sinh ƒë·∫ßu)\n",
    "    # L∆∞u √Ω: Trong m√¥i tr∆∞·ªùng distributed, log n√†y s·∫Ω xu·∫•t hi·ªán tr√™n executor nodes\n",
    "    if len(result) > 0:\n",
    "        # Ch·ªâ in log cho m·ªôt s·ªë th√≠ sinh ƒë·ªÉ minh h·ªça\n",
    "        pass  # Comment out ƒë·ªÉ tr√°nh qu√° nhi·ªÅu log trong production\n",
    "    \n",
    "    return result\n",
    "\n",
    "create_group_scores_udf = udf(create_subject_group_scores_map_debug, MapType(StringType(), FloatType()))\n",
    "print(\"  ‚úì UDF ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a v·ªõi debug logging\")\n",
    "\n",
    "# T·∫°o c·ªôt m·ªõi ch·ª©a mapping subjectGroupId -> totalScore\n",
    "df_student_with_group_scores = df_student_scores.withColumn(\n",
    "    \"subject_group_scores\",  # C·ªôt m·ªõi: Map<subjectGroupId, totalScore>\n",
    "    create_group_scores_udf(col(\"scores\"))\n",
    ")\n",
    "\n",
    "# ==================== PH√ÇN T√çCH K·∫æT QU·∫¢ ====================\n",
    "print(\"[B∆∞·ªõc 7] Ph√¢n t√≠ch k·∫øt qu·∫£:\")\n",
    "print(\"\\n  üìä Xem m·∫´u k·∫øt qu·∫£ (5 th√≠ sinh):\")\n",
    "df_student_with_group_scores.select(\n",
    "    \"studentId\", \n",
    "    \"regionId\", \n",
    "    \"year\", \n",
    "    \"subject_group_scores\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "# T√≠nh th·ªëng k√™\n",
    "print(\"\\n  üìà Th·ªëng k√™ s·ªë l∆∞·ª£ng kh·ªëi thi m√† m·ªói th√≠ sinh c√≥ th·ªÉ x√©t:\")\n",
    "\n",
    "# ƒê·∫øm s·ªë l∆∞·ª£ng kh·ªëi thi cho m·ªói th√≠ sinh\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "df_stats = df_student_with_group_scores.withColumn(\n",
    "    \"num_eligible_groups\", \n",
    "    size(col(\"subject_group_scores\"))\n",
    ")\n",
    "\n",
    "df_stats.groupBy(\"num_eligible_groups\") \\\n",
    "    .agg(count(\"*\").alias(\"s·ªë_th√≠_sinh\")) \\\n",
    "    .orderBy(\"num_eligible_groups\") \\\n",
    "    .show(20)\n",
    "\n",
    "print(\"\\n  üìä Th·ªëng k√™ t·ªïng quan:\")\n",
    "avg_groups = df_stats.agg(avg(\"num_eligible_groups\").alias(\"avg\")).collect()[0][\"avg\"]\n",
    "max_groups = df_stats.agg(max(\"num_eligible_groups\").alias(\"max\")).collect()[0][\"max\"]\n",
    "min_groups = df_stats.agg(min(\"num_eligible_groups\").alias(\"min\")).collect()[0][\"min\"]\n",
    "\n",
    "print(f\"     - S·ªë kh·ªëi TB m·ªói th√≠ sinh c√≥ th·ªÉ x√©t: {avg_groups:.2f}\")\n",
    "print(f\"     - S·ªë kh·ªëi t·ªëi ƒëa: {max_groups}\")\n",
    "print(f\"     - S·ªë kh·ªëi t·ªëi thi·ªÉu: {min_groups}\")\n",
    "print(f\"     - T·ªïng s·ªë kh·ªëi thi trong h·ªá th·ªëng: {len(subject_groups_dict)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ HO√ÄN TH√ÄNH PH·∫¶N 1: DataFrame 'df_student_with_group_scores' ƒë√£ s·∫µn s√†ng\")\n",
    "print(\"   v·ªõi th√¥ng tin chi ti·∫øt v·ªÅ c√°c kh·ªëi thi m√† m·ªói th√≠ sinh c√≥ th·ªÉ x√©t\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c2a32fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=\" * 80)\n",
    "# print(\"PH·∫¶N 1: T√çNH TO√ÅN T·ªî H·ª¢P KH·ªêI THI CHO M·ªñI TH√ç SINH\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# # ==================== ƒê·ªåC D·ªÆ LI·ªÜU ====================\n",
    "# print(\"\\n[B∆∞·ªõc 1] ƒê·ªçc d·ªØ li·ªáu t·ª´ Silver layer...\")\n",
    "# df_student_scores = spark.table(\"nessie.silver_tables.student_scores\")\n",
    "# df_subject_group = spark.table(\"nessie.silver_tables.subject_group\")\n",
    "# df_subject = spark.table(\"nessie.silver_tables.subject\")\n",
    "\n",
    "# print(f\"  ‚úì S·ªë l∆∞·ª£ng th√≠ sinh: {df_student_scores.count():,}\")\n",
    "# print(f\"  ‚úì S·ªë l∆∞·ª£ng kh·ªëi thi: {df_subject_group.count()}\")\n",
    "# print(f\"  ‚úì S·ªë l∆∞·ª£ng m√¥n h·ªçc: {df_subject.count()}\")\n",
    "\n",
    "# # ==================== T·∫†O MAPPING DICTIONARIES ====================\n",
    "# print(\"\\n[B∆∞·ªõc 2] T·∫°o mapping dictionaries...\")\n",
    "\n",
    "# # Mapping: T√™n m√¥n ‚Üí ID m√¥n (VD: \"To√°n\" ‚Üí 1)\n",
    "# subject_name_to_id = {row['subjectName']: row['subjectId'] for row in df_subject.collect()}\n",
    "# print(f\"  ‚úì Subject mapping: {subject_name_to_id}\")\n",
    "\n",
    "# # Mapping: ID kh·ªëi ‚Üí Danh s√°ch m√¥n (VD: \"A00\" ‚Üí [\"To√°n\", \"L√Ω\", \"H√≥a\"])\n",
    "# subject_groups_dict = {}\n",
    "# for row in df_subject_group.collect():\n",
    "#     group_id = row['subjectGroupId']\n",
    "#     subjects = [s.strip() for s in row['subjectCombination'].split('-')]\n",
    "#     subject_groups_dict[group_id] = subjects\n",
    "\n",
    "# print(f\"  ‚úì S·ªë l∆∞·ª£ng kh·ªëi thi: {len(subject_groups_dict)}\")\n",
    "# print(f\"  ‚úì V√≠ d·ª• kh·ªëi thi: {dict(list(subject_groups_dict.items())[:3])}\")\n",
    "\n",
    "# # Broadcast ƒë·ªÉ worker nodes truy c·∫≠p nhanh\n",
    "# from pyspark.sql.types import FloatType, MapType, StringType\n",
    "# subject_map_broadcast = spark.sparkContext.broadcast(subject_name_to_id)\n",
    "# subject_groups_broadcast = spark.sparkContext.broadcast(subject_groups_dict)\n",
    "\n",
    "# # ==================== UDF T√çNH ƒêI·ªÇM T·ªîNG THEO KH·ªêI ====================\n",
    "# print(\"\\n[B∆∞·ªõc 3] ƒê·ªãnh nghƒ©a UDF t√≠nh ƒëi·ªÉm t·ªïng theo kh·ªëi thi...\")\n",
    "\n",
    "# def create_subject_group_scores_map(scores_map):\n",
    "#     \"\"\"\n",
    "#     UDF ch√≠nh: T√≠nh ƒëi·ªÉm t·ªïng cho T·∫§T C·∫¢ c√°c kh·ªëi thi m√† th√≠ sinh c√≥ ƒë·ªß m√¥n\n",
    "    \n",
    "#     Input:  scores_map = Map<subjectId, score> c·ªßa 1 th√≠ sinh\n",
    "#             VD: {1: 9.0, 2: 8.5, 3: 7.0, 4: 6.0}\n",
    "#                 (To√°n: 9, L√Ω: 8.5, H√≥a: 7, Sinh: 6)\n",
    "    \n",
    "#     Output: Map<subjectGroupId, totalScore> cho c√°c kh·ªëi th√≠ sinh c√≥ ƒë·ªß m√¥n\n",
    "#             VD: {\"A00\": 24.5, \"B00\": 22.0}\n",
    "    \n",
    "#     Logic:\n",
    "#     - Duy·ªát qua T·∫§T C·∫¢ c√°c kh·ªëi thi\n",
    "#     - V·ªõi m·ªói kh·ªëi, ki·ªÉm tra xem th√≠ sinh c√≥ ƒë·ªß ƒëi·ªÉm c√°c m√¥n trong kh·ªëi kh√¥ng\n",
    "#     - N·∫øu ƒë·ªß ‚Üí C·ªông t·ªïng ƒëi·ªÉm c√°c m√¥n ‚Üí L∆∞u v√†o result\n",
    "#     - N·∫øu thi·∫øu b·∫•t k·ª≥ m√¥n n√†o ‚Üí B·ªè qua kh·ªëi ƒë√≥\n",
    "#     \"\"\"\n",
    "#     if scores_map is None:\n",
    "#         return {}\n",
    "    \n",
    "#     subject_name_to_id_map = subject_map_broadcast.value\n",
    "#     subject_id_to_name_map = {v: k for k, v in subject_name_to_id_map.items()}\n",
    "#     subject_groups = subject_groups_broadcast.value\n",
    "    \n",
    "#     result = {}\n",
    "    \n",
    "#     # V·ªõi m·ªói kh·ªëi thi, ki·ªÉm tra xem th√≠ sinh c√≥ ƒë·ªß m√¥n kh√¥ng\n",
    "#     for group_id, required_subjects in subject_groups.items():\n",
    "#         total_score = 0.0\n",
    "#         has_all_subjects = True\n",
    "        \n",
    "#         # Ki·ªÉm tra t·ª´ng m√¥n trong kh·ªëi\n",
    "#         for subject_name in required_subjects:\n",
    "#             subject_id = subject_name_to_id_map.get(subject_name)\n",
    "            \n",
    "#             # N·∫øu thi·∫øu m√¥n ho·∫∑c kh√¥ng c√≥ ƒëi·ªÉm ‚Üí B·ªè qua kh·ªëi n√†y\n",
    "#             if subject_id is None or subject_id not in scores_map:\n",
    "#                 has_all_subjects = False\n",
    "#                 break\n",
    "            \n",
    "#             # C·ªông d·ªìn ƒëi·ªÉm\n",
    "#             total_score += scores_map[subject_id]\n",
    "        \n",
    "#         # Ch·ªâ th√™m v√†o result n·∫øu th√≠ sinh c√≥ ƒë·ªß T·∫§T C·∫¢ c√°c m√¥n trong kh·ªëi\n",
    "#         if has_all_subjects:\n",
    "#             result[group_id] = total_score\n",
    "    \n",
    "#     return result\n",
    "\n",
    "# create_group_scores_udf = udf(create_subject_group_scores_map, MapType(StringType(), FloatType()))\n",
    "# print(\"  ‚úì UDF ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")\n",
    "\n",
    "# # ==================== √ÅP D·ª§NG UDF ====================\n",
    "# print(\"\\n[B∆∞·ªõc 4] √Åp d·ª•ng UDF ƒë·ªÉ t√≠nh ƒëi·ªÉm t·ªïng cho t·ª´ng th√≠ sinh...\")\n",
    "# print(\"  ‚è≥ ƒêang x·ª≠ l√Ω... (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)\")\n",
    "\n",
    "# df_student_with_group_scores = df_student_scores.withColumn(\n",
    "#     \"subject_group_scores\",  # C·ªôt m·ªõi: Map<subjectGroupId, totalScore>\n",
    "#     create_group_scores_udf(col(\"scores\"))\n",
    "# )\n",
    "\n",
    "# print(\"  ‚úì Ho√†n th√†nh!\\n\")\n",
    "\n",
    "# # ==================== HI·ªÇN TH·ªä M·∫™U ====================\n",
    "# print(\"[B∆∞·ªõc 5] Xem k·∫øt qu·∫£ m·∫´u:\")\n",
    "# print(\"\\nC·∫•u tr√∫c: M·ªói th√≠ sinh c√≥ Map ch·ª©a t·∫•t c·∫£ kh·ªëi thi m√† h·ªç c√≥ th·ªÉ x√©t:\")\n",
    "# df_student_with_group_scores.select(\n",
    "#     \"studentId\", \n",
    "#     \"regionId\", \n",
    "#     \"year\", \n",
    "#     \"subject_group_scores\"\n",
    "# ).show(5, truncate=False)\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 80)\n",
    "# print(\"‚úÖ HO√ÄN TH√ÄNH PH·∫¶N 1: DataFrame 'df_student_with_group_scores' ƒë√£ s·∫µn s√†ng\")\n",
    "# print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee402c",
   "metadata": {},
   "source": [
    "#### üìä PH·∫¶N 2: X·ª≠ l√Ω v√† t·∫°o Fact Table t·ª´ d·ªØ li·ªáu ƒë√£ t√≠nh to√°n\n",
    "\n",
    "**M·ª•c ti√™u:** Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu t·ªï h·ª£p kh·ªëi thi th√†nh Fact table v·ªõi ph√¢n b·ªë ƒëi·ªÉm theo kho·∫£ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a1fff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PH·∫¶N 2: X·ª¨ L√ù V√Ä T·∫†O FACT TABLE\n",
      "================================================================================\n",
      "\n",
      "[B∆∞·ªõc 1] Explode Map th√†nh rows...\n",
      "  Chuy·ªÉn t·ª´: 1 row/th√≠ sinh ‚Üí nhi·ªÅu rows (1 row cho m·ªói kh·ªëi thi)\n",
      "\n",
      "[B∆∞·ªõc 2] Ph√¢n lo·∫°i ƒëi·ªÉm v√†o c√°c kho·∫£ng...\n",
      "  L√†m tr√≤n xu·ªëng: 24.5 ‚Üí 24.0, 22.8 ‚Üí 22.0\n",
      "  T·∫°o kho·∫£ng: [0-1), [1-2), [2-3), ..., [29-30)\n",
      "  ‚úì Ho√†n th√†nh ph√¢n lo·∫°i ƒëi·ªÉm\n",
      "\n",
      "[B∆∞·ªõc 3] ƒê·∫øm s·ªë l∆∞·ª£ng h·ªçc sinh theo t·ª´ng nh√≥m...\n",
      "  GROUP BY: regionId, subjectGroupId, year, scoreRange\n",
      "\n",
      "[B∆∞·ªõc 5] √Åp d·ª•ng mapping...\n",
      "  ‚úì Mapping ho√†n th√†nh\n",
      "\n",
      "[B∆∞·ªõc 6] T·∫°o primary key (SDBSGKey)...\n",
      "  ‚úì Primary key ƒë√£ ƒë∆∞·ª£c t·∫°o\n",
      "\n",
      "[B∆∞·ªõc 7] Chu·∫©n b·ªã schema cu·ªëi c√πng...\n",
      "  ‚úì Schema:\n",
      "root\n",
      " |-- SDBSGKey: integer (nullable = false)\n",
      " |-- regionKey: integer (nullable = true)\n",
      " |-- subjectGroupKey: integer (nullable = true)\n",
      " |-- timeKey: integer (nullable = true)\n",
      " |-- scoreRange: float (nullable = true)\n",
      " |-- quantity: float (nullable = false)\n",
      "\n",
      "\n",
      "[B∆∞·ªõc 8] Ghi v√†o Gold layer...\n",
      "  ‚è≥ ƒêang ghi... (partitioned by timeKey)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ load 219757 d√≤ng v√†o fact_score_distribution_by_subject_group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 377:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------------+-------+----------+--------+\n",
      "|SDBSGKey|regionKey|subjectGroupKey|timeKey|scoreRange|quantity|\n",
      "+--------+---------+---------------+-------+----------+--------+\n",
      "|       1|        1|              1|      1|       4.0|     1.0|\n",
      "|       2|        1|              1|      1|       5.0|     2.0|\n",
      "|       3|        1|              1|      1|       6.0|    11.0|\n",
      "|       4|        1|              1|      1|       7.0|    22.0|\n",
      "|       5|        1|              1|      1|       8.0|    32.0|\n",
      "|       6|        1|              1|      1|       9.0|    32.0|\n",
      "|       7|        1|              1|      1|      10.0|    81.0|\n",
      "|       8|        1|              1|      1|      11.0|   133.0|\n",
      "|       9|        1|              1|      1|      12.0|   206.0|\n",
      "|      10|        1|              1|      1|      13.0|   364.0|\n",
      "+--------+---------+---------------+-------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PH·∫¶N 2: X·ª¨ L√ù V√Ä T·∫†O FACT TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==================== EXPLODE MAP TH√ÄNH ROWS ====================\n",
    "print(\"\\n[B∆∞·ªõc 1] Explode Map th√†nh rows...\")\n",
    "print(\"  Chuy·ªÉn t·ª´: 1 row/th√≠ sinh ‚Üí nhi·ªÅu rows (1 row cho m·ªói kh·ªëi thi)\")\n",
    "\n",
    "df_scores_exploded = df_student_with_group_scores.select(\n",
    "    \"regionId\",\n",
    "    \"year\",\n",
    "    explode(\"subject_group_scores\").alias(\"subjectGroupId\", \"totalScore\")\n",
    ")\n",
    "\n",
    "# ==================== PH√ÇN LO·∫†I ƒêI·ªÇM V√ÄO KHO·∫¢NG ====================\n",
    "print(\"\\n[B∆∞·ªõc 2] Ph√¢n lo·∫°i ƒëi·ªÉm v√†o c√°c kho·∫£ng...\")\n",
    "print(\"  L√†m tr√≤n xu·ªëng: 24.5 ‚Üí 24.0, 22.8 ‚Üí 22.0\")\n",
    "print(\"  T·∫°o kho·∫£ng: [0-1), [1-2), [2-3), ..., [29-30)\")\n",
    "\n",
    "df_scores_exploded = df_scores_exploded.withColumn(\n",
    "    \"scoreRange\",\n",
    "    floor(col(\"totalScore\")).cast(\"float\")\n",
    ")\n",
    "\n",
    "print(\"  ‚úì Ho√†n th√†nh ph√¢n lo·∫°i ƒëi·ªÉm\")\n",
    "\n",
    "# ==================== ƒê·∫æM S·ªê L∆Ø·ª¢NG H·ªåC SINH ====================\n",
    "print(\"\\n[B∆∞·ªõc 3] ƒê·∫øm s·ªë l∆∞·ª£ng h·ªçc sinh theo t·ª´ng nh√≥m...\")\n",
    "print(\"  GROUP BY: regionId, subjectGroupId, year, scoreRange\")\n",
    "\n",
    "df_distribution = df_scores_exploded.groupBy(\n",
    "    \"regionId\",\n",
    "    \"subjectGroupId\",\n",
    "    \"year\",\n",
    "    \"scoreRange\"\n",
    ").agg(\n",
    "    count(\"*\").alias(\"quantity\")\n",
    ")\n",
    "\n",
    "# T·∫°o mapping dictionaries t·ª´ dimension tables ƒë·ªÉ tra c·ª©u nhanh (thay v√¨ JOIN)\n",
    "# Quan tr·ªçng: Normalize subjectGroupId th√†nh string v√† upper() ƒë·ªÉ ƒë·∫£m b·∫£o format nh·∫•t qu√°n\n",
    "region_id_to_key = {row['regionId']: row['regionKey'] for row in df_dim_region_gold.select(\"regionId\", \"regionKey\").collect()}\n",
    "subject_group_id_to_key = {str(row['subjectGroupId']).strip().upper(): row['subjectGroupKey'] for row in df_dim_subject_group_gold.select(\"subjectGroupId\", \"subjectGroupKey\").collect()}\n",
    "year_to_time_key = {row['year']: row['timeKey'] for row in df_dim_time_gold.select(\"year\", \"timeKey\").collect()}\n",
    "\n",
    "# Broadcast mappings\n",
    "region_map_bc = spark.sparkContext.broadcast(region_id_to_key)\n",
    "subject_group_map_bc = spark.sparkContext.broadcast(subject_group_id_to_key)\n",
    "year_map_bc = spark.sparkContext.broadcast(year_to_time_key)\n",
    "\n",
    "# UDF ƒë·ªÉ map c√°c natural keys sang surrogate keys\n",
    "# Quan tr·ªçng: Normalize subjectGroupId th√†nh string v√† upper()\n",
    "def map_to_keys(regionId, subjectGroupId, year):\n",
    "    region_key = region_map_bc.value.get(regionId)\n",
    "    # Normalize subjectGroupId tr∆∞·ªõc khi lookup (convert to string first)\n",
    "    normalized_group_id = str(subjectGroupId).strip().upper() if subjectGroupId is not None else None\n",
    "    subject_group_key = subject_group_map_bc.value.get(normalized_group_id)\n",
    "    time_key = year_map_bc.value.get(year)\n",
    "    return (region_key, subject_group_key, time_key)\n",
    "\n",
    "map_keys_udf = udf(map_to_keys, StructType([\n",
    "    StructField(\"regionKey\", IntegerType(), True),\n",
    "    StructField(\"subjectGroupKey\", IntegerType(), True),\n",
    "    StructField(\"timeKey\", IntegerType(), True)\n",
    "]))\n",
    "\n",
    "print(\"\\n[B∆∞·ªõc 5] √Åp d·ª•ng mapping...\")\n",
    "\n",
    "# √Åp d·ª•ng mapping\n",
    "df_with_keys = df_distribution.withColumn(\n",
    "    \"keys\",\n",
    "    map_keys_udf(col(\"regionId\"), col(\"subjectGroupId\"), col(\"year\"))\n",
    ")\n",
    "\n",
    "# Extract keys t·ª´ struct\n",
    "df_with_keys = df_with_keys.withColumn(\"regionKey\", col(\"keys.regionKey\")) \\\n",
    "    .withColumn(\"subjectGroupKey\", col(\"keys.subjectGroupKey\")) \\\n",
    "    .withColumn(\"timeKey\", col(\"keys.timeKey\")) \\\n",
    "    .drop(\"keys\", \"regionId\", \"subjectGroupId\", \"year\")\n",
    "\n",
    "print(\"  ‚úì Mapping ho√†n th√†nh\")\n",
    "\n",
    "# ==================== T·∫†O SURROGATE KEY ====================\n",
    "print(\"\\n[B∆∞·ªõc 6] T·∫°o primary key (SDBSGKey)...\")\n",
    "\n",
    "window_spec = Window.orderBy(\"timeKey\", \"regionKey\", \"subjectGroupKey\", \"scoreRange\")\n",
    "df_with_keys = df_with_keys.withColumn(\"SDBSGKey\", row_number().over(window_spec))\n",
    "\n",
    "print(\"  ‚úì Primary key ƒë√£ ƒë∆∞·ª£c t·∫°o\")\n",
    "\n",
    "# ==================== SELECT C√ÅC C·ªòT CU·ªêI C√ôNG ====================\n",
    "print(\"\\n[B∆∞·ªõc 7] Chu·∫©n b·ªã schema cu·ªëi c√πng...\")\n",
    "\n",
    "df_fact_score_dist_group = df_with_keys.select(\n",
    "    \"SDBSGKey\",\n",
    "    \"regionKey\",\n",
    "    \"subjectGroupKey\",\n",
    "    \"timeKey\",\n",
    "    col(\"scoreRange\").cast(\"float\"),\n",
    "    col(\"quantity\").cast(\"float\")\n",
    ")\n",
    "\n",
    "print(\"  ‚úì Schema:\")\n",
    "df_fact_score_dist_group.printSchema()\n",
    "\n",
    "# ==================== GHI V√ÄO GOLD LAYER ====================\n",
    "print(\"\\n[B∆∞·ªõc 8] Ghi v√†o Gold layer...\")\n",
    "print(\"  ‚è≥ ƒêang ghi... (partitioned by timeKey)\")\n",
    "\n",
    "df_fact_score_dist_group.writeTo(\"nessie.gold_tables.fact_score_distribution_by_subject_group\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .partitionedBy(\"timeKey\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(f\"ƒê√£ load {df_fact_score_dist_group.count()} d√≤ng v√†o fact_score_distribution_by_subject_group\")\n",
    "df_fact_score_dist_group.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c53beba-047e-489c-bce2-98d78f0e8a45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
