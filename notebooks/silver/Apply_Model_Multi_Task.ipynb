{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d139c45d",
   "metadata": {},
   "source": [
    "## 1. Import Libraries và Khởi tạo Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59904f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, udf, current_timestamp\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "from vncorenlp import VnCoreNLP\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set AWS environment variables for MinIO\n",
    "os.environ['AWS_REGION'] = 'us-east-1'\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'admin'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'admin123'\n",
    "\n",
    "# Khởi tạo Spark Session với Iceberg và Nessie catalog\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Apply_Model_Multi_Task\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.memory\", \"1536m\")  # 1.5GB - an toàn với worker 2GB\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.network.timeout\", \"600s\")\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\")\n",
    "    .config(\"spark.storage.blockManagerSlaveTimeoutMs\", \"600000\")\n",
    "    .config(\"spark.rpc.askTimeout\", \"600s\")\n",
    "    # ===== Iceberg Catalog qua Nessie =====\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v2\")\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://silver/\")\n",
    "    .config(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    # ===== Cấu hình MinIO (S3-compatible) =====\n",
    "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.access-key-id\", \"admin\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.secret-access-key\", \"admin123\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.region\", \"us-east-1\")\n",
    "    # ===== Spark + Hadoop S3 connector =====\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.region\", \"us-east-1\")\n",
    "    # Propagate environment variables to executors\n",
    "    .config(\"spark.executorEnv.AWS_REGION\", \"us-east-1\")\n",
    "    .config(\"spark.executorEnv.AWS_ACCESS_KEY_ID\", \"admin\")\n",
    "    .config(\"spark.executorEnv.AWS_SECRET_ACCESS_KEY\", \"admin123\")\n",
    "    # ===== Sử dụng JAR files local =====\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/hadoop-aws-3.3.4.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"Spark Session da duoc khoi tao voi Nessie catalog!\")\n",
    "print(f\"Spark Master: {spark.sparkContext.master}\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc96d5",
   "metadata": {},
   "source": [
    "## 2. Đọc Dữ Liệu từ Bảng Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f4f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc dữ liệu từ bảng article\n",
    "df_articles = spark.table(\"nessie.silver_tables.article\")\n",
    "print(f\"Tổng số articles: {df_articles.count()}\")\n",
    "\n",
    "# Cache để tránh đọc lại nhiều lần\n",
    "df_articles.cache()\n",
    "print(\"✓ Đã cache df_articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed41567",
   "metadata": {},
   "source": [
    "## 3. Load Model và Cấu Hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6483a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đường dẫn model local (đã mount vào container)\n",
    "local_model_path = '/opt/spark-apps/phobert_multitask_final'\n",
    "\n",
    "# Đọc JSON files từ local filesystem\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open(f'{local_model_path}/model_config.json', 'r') as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "with open(f'{local_model_path}/label_mappings.json', 'r') as f:\n",
    "    label_mappings = json.load(f)\n",
    "\n",
    "print(f\"NER labels: {model_config['num_ner_labels']}\")\n",
    "print(f\"Topic labels: {model_config['num_topic_labels']}\")\n",
    "print(f\"Intent labels: {model_config['num_intent_labels']}\")\n",
    "\n",
    "# Kiểm tra VnCoreNLP JAR file tại path đã mount\n",
    "print(\"\\n=== Kiểm tra VnCoreNLP JAR file ===\")\n",
    "vncorenlp_path = '/opt/spark-apps/VnCoreNLP-1.2/VnCoreNLP-1.2.jar'\n",
    "\n",
    "if os.path.exists(vncorenlp_path):\n",
    "    print(f\"Tìm thấy: {vncorenlp_path}\")\n",
    "    print(f\"\\nĐang khởi tạo VnCoreNLP...\")\n",
    "    try:\n",
    "        annotator = VnCoreNLP(vncorenlp_path, annotators=\"wseg\", max_heap_size='-Xmx2g')\n",
    "        print(\"VnCoreNLP loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khởi tạo VnCoreNLP: {e}\")\n",
    "        annotator = None\n",
    "else:\n",
    "    print(f\"Không tìm thấy VnCoreNLP JAR tại: {vncorenlp_path}\")\n",
    "    print(\"Kiểm tra lại volume mount trong docker-compose.yaml\")\n",
    "    annotator = None\n",
    "\n",
    "# Lưu model_path cho các cell sau\n",
    "model_path = local_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3bb58b",
   "metadata": {},
   "source": [
    "## 4. Định Nghĩa Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f8a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaModel\n",
    "\n",
    "\n",
    "class MultiTaskPhoBERT_WithFusion(nn.Module):\n",
    "    \"\"\"Multi-Task PhoBERT với Feature Fusion\"\"\"\n",
    "    def __init__(self, phobert_path, num_ner_labels, num_topic_labels, num_intent_labels, dropout=0.2):\n",
    "        super(MultiTaskPhoBERT_WithFusion, self).__init__()\n",
    "        \n",
    "        self.phobert = RobertaModel.from_pretrained(phobert_path)\n",
    "        self.phobert.config.hidden_dropout_prob = 0.25\n",
    "        self.phobert.config.attention_probs_dropout_prob = 0.25\n",
    "        self.hidden_size = self.phobert.config.hidden_size\n",
    "        self.num_ner_labels = num_ner_labels\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_heavy = nn.Dropout(dropout * 1.5)\n",
    "        \n",
    "        # NER head\n",
    "        self.ner_hidden = nn.Linear(self.hidden_size, self.hidden_size // 2)\n",
    "        self.ner_norm = nn.LayerNorm(self.hidden_size // 2)\n",
    "        self.ner_classifier = nn.Linear(self.hidden_size // 2, num_ner_labels)\n",
    "        \n",
    "        # Intent head\n",
    "        self.intent_hidden = nn.Linear(self.hidden_size, self.hidden_size // 2)\n",
    "        self.intent_norm = nn.LayerNorm(self.hidden_size // 2)\n",
    "        self.intent_classifier = nn.Linear(self.hidden_size // 2, num_intent_labels)\n",
    "        \n",
    "        # Topic Fusion Head\n",
    "        fusion_input_size = self.hidden_size + num_ner_labels\n",
    "        self.topic_input_proj = nn.Linear(fusion_input_size, self.hidden_size)\n",
    "        \n",
    "        self.topic_layer1 = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.LayerNorm(self.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 0.5)\n",
    "        )\n",
    "        \n",
    "        self.topic_layer2 = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.LayerNorm(self.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 0.5)\n",
    "        )\n",
    "        \n",
    "        self.topic_classifier = nn.Linear(self.hidden_size, num_topic_labels)\n",
    "        \n",
    "        # NER attention mechanism\n",
    "        self.ner_attention = nn.Sequential(\n",
    "            nn.Linear(num_ner_labels, num_ner_labels // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_ner_labels // 2, num_ner_labels),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Cross-Attention\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=self.hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.cross_attn_norm = nn.LayerNorm(self.hidden_size)\n",
    "        \n",
    "        # Auxiliary head\n",
    "        self.aux_topic_classifier = nn.Sequential(\n",
    "            nn.Linear(num_ner_labels, num_ner_labels // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(num_ner_labels // 2, num_topic_labels)\n",
    "        )\n",
    "    \n",
    "    def extract_ner_features(self, ner_logits, attention_mask):\n",
    "        \"\"\"Trích xuất NER features với MAX + AVG pooling\"\"\"\n",
    "        ner_probs = F.softmax(ner_logits, dim=-1)\n",
    "        \n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand_as(ner_probs)\n",
    "        ner_probs_masked = ner_probs * attention_mask_expanded\n",
    "        \n",
    "        max_features, _ = ner_probs_masked.max(dim=1)\n",
    "        \n",
    "        seq_lengths = attention_mask.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "        avg_features = ner_probs_masked.sum(dim=1) / seq_lengths\n",
    "        \n",
    "        ner_features = 0.5 * max_features + 0.5 * avg_features\n",
    "        \n",
    "        attention_weights = self.ner_attention(ner_features)\n",
    "        ner_features_weighted = ner_features * attention_weights\n",
    "        \n",
    "        return ner_features_weighted\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, ner_labels=None, topic_labels=None, intent_labels=None):\n",
    "        outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        cls_output = sequence_output[:, 0, :]\n",
    "        \n",
    "        cls_expanded = cls_output.unsqueeze(1)\n",
    "        attn_output, _ = self.cross_attention(\n",
    "            query=cls_expanded,\n",
    "            key=sequence_output,\n",
    "            value=sequence_output,\n",
    "            key_padding_mask=(attention_mask == 0) if attention_mask is not None else None\n",
    "        )\n",
    "        cls_output = self.cross_attn_norm(cls_output + attn_output.squeeze(1))\n",
    "        cls_output_dropped = self.dropout_heavy(cls_output)\n",
    "        \n",
    "        # NER predictions\n",
    "        ner_hidden = self.ner_hidden(sequence_output)\n",
    "        ner_hidden = self.ner_norm(ner_hidden)\n",
    "        ner_hidden = F.gelu(ner_hidden)\n",
    "        ner_hidden = self.dropout(ner_hidden)\n",
    "        ner_logits = self.ner_classifier(ner_hidden)\n",
    "        \n",
    "        ner_features = self.extract_ner_features(ner_logits, attention_mask)\n",
    "        \n",
    "        # Topic prediction\n",
    "        topic_input = torch.cat([cls_output_dropped, ner_features], dim=-1)\n",
    "        topic_hidden = self.topic_input_proj(topic_input)\n",
    "        topic_hidden = topic_hidden + self.topic_layer1(topic_hidden)\n",
    "        topic_hidden = topic_hidden + self.topic_layer2(topic_hidden)\n",
    "        topic_logits = self.topic_classifier(topic_hidden)\n",
    "        \n",
    "        # Intent prediction\n",
    "        intent_hidden = self.intent_hidden(cls_output_dropped)\n",
    "        intent_hidden = self.intent_norm(intent_hidden)\n",
    "        intent_hidden = F.gelu(intent_hidden)\n",
    "        intent_hidden = self.dropout(intent_hidden)\n",
    "        intent_logits = self.intent_classifier(intent_hidden)\n",
    "        \n",
    "        return {\n",
    "            'loss': None,\n",
    "            'ner_logits': ner_logits,\n",
    "            'topic_logits': topic_logits,\n",
    "            'intent_logits': intent_logits\n",
    "        }\n",
    "\n",
    "print(\"Model class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ed000e",
   "metadata": {},
   "source": [
    "## 5. Load Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3097df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đường dẫn model local (đã mount vào container)\n",
    "local_model_path = '/opt/spark-apps/phobert_multitask_final'\n",
    "\n",
    "print(f\"Loading model from: {local_model_path}\")\n",
    "\n",
    "# Load tokenizer và model\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "\n",
    "model = MultiTaskPhoBERT_WithFusion(\n",
    "    phobert_path=model_config['phobert_base'],\n",
    "    num_ner_labels=model_config['num_ner_labels'],\n",
    "    num_topic_labels=model_config['num_topic_labels'],\n",
    "    num_intent_labels=model_config['num_intent_labels'],\n",
    "    dropout=model_config['dropout']\n",
    ")\n",
    "\n",
    "# Load trọng số đã train\n",
    "state_dict = torch.load(f'{local_model_path}/pytorch_model.bin', map_location='cpu')\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "# Chuyển model sang GPU nếu có\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded successfully on {device}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a120284",
   "metadata": {},
   "source": [
    "## 6. Định Nghĩa Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text_with_underscore(text, annotator):\n",
    "    \"\"\"Chuẩn hóa văn bản tiếng Việt bằng VnCoreNLP\"\"\"\n",
    "    try:\n",
    "        sentences = annotator.tokenize(text)\n",
    "        \n",
    "        normalized_sentences = []\n",
    "        for sentence in sentences:\n",
    "            words = []\n",
    "            for word in sentence:\n",
    "                if isinstance(word, list):\n",
    "                    words.append('_'.join(word))\n",
    "                else:\n",
    "                    words.append(word)\n",
    "            normalized_sentences.append(' '.join(words))\n",
    "        \n",
    "        normalized_text = ' '.join(normalized_sentences)\n",
    "        return normalized_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error normalizing text: {e}\")\n",
    "        return text\n",
    "\n",
    "\n",
    "def predict_labels(text, model, tokenizer, label_mappings, device, annotator, max_length=256):\n",
    "    \"\"\"Dự đoán NER, Topic, Intent cho văn bản đầu vào\"\"\"\n",
    "    if not text or text.strip() == '':\n",
    "        return {\n",
    "            'normalized_text': '',\n",
    "            'ner_labels': 'O',\n",
    "            'topic_label': 'None',\n",
    "            'intent_label': 'Unknown'\n",
    "        }\n",
    "    \n",
    "    # Chuẩn hóa text bằng VnCoreNLP\n",
    "    normalized_text = normalize_text_with_underscore(text, annotator)\n",
    "    normalized_tokens = normalized_text.split()\n",
    "    num_tokens = len(normalized_tokens)\n",
    "    \n",
    "    # Tokenize cho PhoBERT\n",
    "    inputs = tokenizer(\n",
    "        normalized_text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    if 'token_type_ids' in inputs:\n",
    "        inputs.pop('token_type_ids')\n",
    "    \n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Dự đoán\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    ner_logits = outputs['ner_logits']\n",
    "    topic_logits = outputs['topic_logits']\n",
    "    intent_logits = outputs['intent_logits']\n",
    "    \n",
    "    # NER predictions\n",
    "    ner_preds = torch.argmax(ner_logits, dim=-1)[0]\n",
    "    phobert_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Align labels với VnCoreNLP tokens\n",
    "    ner_labels = []\n",
    "    token_idx = 0\n",
    "    \n",
    "    for word in normalized_tokens:\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "        \n",
    "        while token_idx < len(phobert_tokens) and phobert_tokens[token_idx] == '<s>':\n",
    "            token_idx += 1\n",
    "        \n",
    "        if token_idx < len(phobert_tokens) and phobert_tokens[token_idx] not in ['</s>', '<pad>']:\n",
    "            label = label_mappings['ner_id2label'][str(ner_preds[token_idx].item())]\n",
    "            ner_labels.append(label)\n",
    "            token_idx += len(word_tokens)\n",
    "        else:\n",
    "            ner_labels.append('O')\n",
    "            break\n",
    "    \n",
    "    # Topic predictions (multi-label, threshold=0.5)\n",
    "    topic_probs = torch.sigmoid(topic_logits)[0]\n",
    "    topic_preds = (topic_probs > 0.5).nonzero(as_tuple=True)[0].cpu().tolist()\n",
    "    \n",
    "    if len(topic_preds) > 0:\n",
    "        topic_labels = [label_mappings['topic_id2label'][str(idx)] for idx in topic_preds]\n",
    "        topic_label = '|'.join(topic_labels)\n",
    "    else:\n",
    "        topic_label = 'None'\n",
    "    \n",
    "    # Intent prediction (single-label)\n",
    "    intent_pred = torch.argmax(intent_logits, dim=-1).item()\n",
    "    intent_label = label_mappings['intent_id2label'][str(intent_pred)]\n",
    "    \n",
    "    return {\n",
    "        'normalized_text': normalized_text,\n",
    "        'ner_labels': ' '.join(ner_labels),\n",
    "        'topic_label': topic_label,\n",
    "        'intent_label': intent_label\n",
    "    }\n",
    "\n",
    "print(\"Prediction functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c24a20d",
   "metadata": {},
   "source": [
    "## 7. Apply Model trên Toàn Bộ Dữ Liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a9733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa schema cho DataFrame kết quả\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import days\n",
    "import gc\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"postID\", StringType(), False),\n",
    "    StructField(\"timePublish\", TimestampType(), False),\n",
    "    StructField(\"description_Normalized\", StringType(), True),\n",
    "    StructField(\"Label_NER\", StringType(), True),\n",
    "    StructField(\"Label_Topic\", StringType(), True),\n",
    "    StructField(\"Label_Intent\", StringType(), True),\n",
    "    StructField(\"likeCount\", IntegerType(), True),\n",
    "    StructField(\"commentCount\", IntegerType(), True),\n",
    "    StructField(\"shareCount\", IntegerType(), True),\n",
    "    StructField(\"type\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Kiểm tra xem table result_multi_model đã tồn tại chưa\n",
    "try:\n",
    "    df_existing = spark.table(\"nessie.silver_tables.result_multi_model\")\n",
    "    existing_post_ids = [row.postID for row in df_existing.select(\"postID\").distinct().collect()]\n",
    "    print(f\"Tìm thấy {len(existing_post_ids)} articles đã được xử lý trước đó\")\n",
    "    \n",
    "    # Giải phóng df_existing\n",
    "    df_existing.unpersist()\n",
    "    del df_existing\n",
    "except Exception as e:\n",
    "    print(f\"Table result_multi_model chưa tồn tại hoặc rỗng: {e}\")\n",
    "    existing_post_ids = []\n",
    "\n",
    "# Lọc ra các articles chưa được xử lý\n",
    "if existing_post_ids:\n",
    "    df_articles_new = df_articles.filter(~col(\"articleID\").isin(existing_post_ids))\n",
    "    print(f\"Tổng số articles mới cần xử lý: {df_articles_new.count()}\")\n",
    "else:\n",
    "    df_articles_new = df_articles\n",
    "    print(f\"Xử lý toàn bộ {df_articles.count()} articles\")\n",
    "\n",
    "print(\"\\n=== Collecting data to process ===\")\n",
    "# Collect data về driver để xử lý (phù hợp với dataset nhỏ)\n",
    "articles_to_process = df_articles_new.select(\"articleID\", \"description\", \"timePublish\", \n",
    "                                              \"likeCount\", \"commentCount\", \"shareCount\", \"type\").collect()\n",
    "print(f\"Đã collect {len(articles_to_process)} articles về driver\")\n",
    "\n",
    "# Giải phóng df_articles_new sau khi collect\n",
    "df_articles_new.unpersist()\n",
    "del df_articles_new\n",
    "gc.collect()\n",
    "print(\"✓ Đã giải phóng df_articles_new\")\n",
    "\n",
    "# Xử lý từng article bằng model đã load sẵn trên driver\n",
    "results = []\n",
    "batch_size = 50  # Xử lý và lưu mỗi 50 records\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"\\n=== Processing articles with model ===\")\n",
    "for idx, row in enumerate(articles_to_process):\n",
    "    if idx % 10 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        avg_time = elapsed / (idx + 1) if idx > 0 else 0\n",
    "        remaining = avg_time * (len(articles_to_process) - idx)\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Processed {idx}/{len(articles_to_process)} | \"\n",
    "              f\"Elapsed: {elapsed:.1f}s | ETA: {remaining:.1f}s\")\n",
    "    \n",
    "    article_id = row['articleID']\n",
    "    description = row['description']\n",
    "    \n",
    "    if not description or description.strip() == '':\n",
    "        results.append({\n",
    "            'postID': article_id,\n",
    "            'timePublish': row['timePublish'],\n",
    "            'description_Normalized': '',\n",
    "            'Label_NER': 'O',\n",
    "            'Label_Topic': 'None',\n",
    "            'Label_Intent': 'Unknown',\n",
    "            'likeCount': int(row['likeCount']) if row['likeCount'] else 0,\n",
    "            'commentCount': int(row['commentCount']) if row['commentCount'] else 0,\n",
    "            'shareCount': int(row['shareCount']) if row['shareCount'] else 0,\n",
    "            'type': row['type']\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Sử dụng hàm predict_labels đã định nghĩa ở cell 6\n",
    "        predictions = predict_labels(description, model, tokenizer, label_mappings, device, annotator)\n",
    "        \n",
    "        results.append({\n",
    "            'postID': article_id,\n",
    "            'timePublish': row['timePublish'],\n",
    "            'description_Normalized': predictions['normalized_text'],\n",
    "            'Label_NER': predictions['ner_labels'],\n",
    "            'Label_Topic': predictions['topic_label'],\n",
    "            'Label_Intent': predictions['intent_label'],\n",
    "            'likeCount': int(row['likeCount']) if row['likeCount'] else 0,\n",
    "            'commentCount': int(row['commentCount']) if row['commentCount'] else 0,\n",
    "            'shareCount': int(row['shareCount']) if row['shareCount'] else 0,\n",
    "            'type': row['type']\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing article {article_id}: {e}\")\n",
    "        results.append({\n",
    "            'postID': article_id,\n",
    "            'timePublish': row['timePublish'],\n",
    "            'description_Normalized': description,\n",
    "            'Label_NER': 'O',\n",
    "            'Label_Topic': 'None',\n",
    "            'Label_Intent': 'Unknown',\n",
    "            'likeCount': int(row['likeCount']) if row['likeCount'] else 0,\n",
    "            'commentCount': int(row['commentCount']) if row['commentCount'] else 0,\n",
    "            'shareCount': int(row['shareCount']) if row['shareCount'] else 0,\n",
    "            'type': row['type']\n",
    "        })\n",
    "    \n",
    "    # Lưu batch và reset results để giải phóng memory\n",
    "    if (idx + 1) % batch_size == 0 and len(results) > 0:\n",
    "        try:\n",
    "            batch_df = spark.createDataFrame(results, schema=schema)\n",
    "            batch_df = batch_df.withColumn(\"created_at\", current_timestamp()) \\\n",
    "                               .withColumn(\"updated_at\", current_timestamp())\n",
    "            \n",
    "            # Append batch vào table\n",
    "            batch_df.writeTo(\"nessie.silver_tables.result_multi_model\") \\\n",
    "                .using(\"iceberg\") \\\n",
    "                .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "                .tableProperty(\"write.metadata.compression-codec\", \"gzip\") \\\n",
    "                .tableProperty(\"write.parquet.compression-codec\", \"gzip\") \\\n",
    "                .append()\n",
    "            \n",
    "            print(f\"✓ Saved batch: {len(results)} records | Total processed: {idx + 1}\")\n",
    "            \n",
    "            # Giải phóng batch_df\n",
    "            batch_df.unpersist()\n",
    "            del batch_df\n",
    "            \n",
    "            results = []  # Reset\n",
    "            gc.collect()  # Thu gom rác\n",
    "            \n",
    "        except Exception as batch_err:\n",
    "            # Nếu table chưa tồn tại, tạo mới\n",
    "            if \"table does not exist\" in str(batch_err).lower() or \"not found\" in str(batch_err).lower():\n",
    "                print(\"Creating table for first batch...\")\n",
    "                batch_df.writeTo(\"nessie.silver_tables.result_multi_model\") \\\n",
    "                    .using(\"iceberg\") \\\n",
    "                    .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "                    .tableProperty(\"write.metadata.compression-codec\", \"gzip\") \\\n",
    "                    .tableProperty(\"write.parquet.compression-codec\", \"gzip\") \\\n",
    "                    .partitionedBy(days(col(\"timePublish\"))) \\\n",
    "                    .create()\n",
    "                print(f\"✓ Table created with {len(results)} records\")\n",
    "                \n",
    "                # Giải phóng\n",
    "                batch_df.unpersist()\n",
    "                del batch_df\n",
    "                results = []\n",
    "                gc.collect()\n",
    "            else:\n",
    "                print(f\"✗ Error saving batch: {batch_err}\")\n",
    "\n",
    "# Lưu records còn lại (nếu có)\n",
    "if len(results) > 0:\n",
    "    try:\n",
    "        final_df = spark.createDataFrame(results, schema=schema)\n",
    "        final_df = final_df.withColumn(\"created_at\", current_timestamp()) \\\n",
    "                           .withColumn(\"updated_at\", current_timestamp())\n",
    "        \n",
    "        final_df.writeTo(\"nessie.silver_tables.result_multi_model\") \\\n",
    "            .using(\"iceberg\") \\\n",
    "            .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "            .tableProperty(\"write.metadata.compression-codec\", \"gzip\") \\\n",
    "            .tableProperty(\"write.parquet.compression-codec\", \"gzip\") \\\n",
    "            .append()\n",
    "        \n",
    "        print(f\"✓ Saved final batch: {len(results)} records\")\n",
    "        \n",
    "        # Giải phóng\n",
    "        final_df.unpersist()\n",
    "        del final_df\n",
    "        \n",
    "    except Exception as final_err:\n",
    "        if \"table does not exist\" in str(final_err).lower():\n",
    "            final_df.writeTo(\"nessie.silver_tables.result_multi_model\") \\\n",
    "                .using(\"iceberg\") \\\n",
    "                .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "                .tableProperty(\"write.metadata.compression-codec\", \"gzip\") \\\n",
    "                .tableProperty(\"write.parquet.compression-codec\", \"gzip\") \\\n",
    "                .partitionedBy(days(col(\"timePublish\"))) \\\n",
    "                .create()\n",
    "            print(f\"✓ Table created with {len(results)} records\")\n",
    "            \n",
    "            # Giải phóng\n",
    "            final_df.unpersist()\n",
    "            del final_df\n",
    "\n",
    "# Giải phóng bộ nhớ cuối cùng\n",
    "del results, articles_to_process\n",
    "gc.collect()\n",
    "print(\"✓ Đã giải phóng bộ nhớ processing\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n=== Completed processing in {total_time:.1f}s ===\")\n",
    "\n",
    "# Kiểm tra kết quả đã lưu\n",
    "print(\"\\n=== Checking saved results ===\")\n",
    "try:\n",
    "    df_check = spark.table(\"nessie.silver_tables.result_multi_model\")\n",
    "    print(f\"Total records in table: {df_check.count()}\")\n",
    "    print(\"\\nRecent 5 records:\")\n",
    "    df_check.orderBy(col(\"created_at\").desc()).show(5, truncate=80)\n",
    "    \n",
    "    # Giải phóng df_check\n",
    "    df_check.unpersist()\n",
    "    del df_check\n",
    "    gc.collect()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not read table: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74e20ed",
   "metadata": {},
   "source": [
    "## 8. Verify Dữ Liệu Đã Lưu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e1459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dữ liệu đã lưu (Cell 7 đã tự động lưu theo batch)\n",
    "import gc\n",
    "\n",
    "try:\n",
    "    df_verify = spark.table(\"nessie.silver_tables.result_multi_model\")\n",
    "    total_count = df_verify.count()\n",
    "    \n",
    "    print(f\"✓ Total records in table: {total_count}\")\n",
    "    print(\"\\nRecent saved records:\")\n",
    "    df_verify.orderBy(col(\"created_at\").desc()).show(5, truncate=False)\n",
    "    \n",
    "    # Thống kê số lượng records theo thời gian tạo\n",
    "    print(\"\\n=== Records saved by time ===\")\n",
    "    df_verify.groupBy(\"created_at\").count() \\\n",
    "        .orderBy(col(\"created_at\").desc()) \\\n",
    "        .show(20, truncate=False)\n",
    "    \n",
    "    # Giải phóng bộ nhớ\n",
    "    df_verify.unpersist()\n",
    "    del df_verify\n",
    "    gc.collect()\n",
    "    print(\"\\n✓ Đã giải phóng bộ nhớ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error reading table: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f684149c",
   "metadata": {},
   "source": [
    "## 9. Thống kê kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fec9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc lại dữ liệu từ table\n",
    "import gc\n",
    "\n",
    "df_verify = spark.table(\"nessie.silver_tables.result_multi_model\")\n",
    "\n",
    "print(f\"Total records in result_multi_model: {df_verify.count()}\")\n",
    "print(\"\\nSample records:\")\n",
    "df_verify.show(10, truncate=False)\n",
    "\n",
    "print(\"\\n=== Label Statistics ===\")\n",
    "print(\"\\nTopic distribution:\")\n",
    "df_topic = df_verify.groupBy(\"Label_Topic\").count().orderBy(col(\"count\").desc())\n",
    "df_topic.show(10, truncate=False)\n",
    "del df_topic\n",
    "\n",
    "print(\"\\nIntent distribution:\")\n",
    "df_intent = df_verify.groupBy(\"Label_Intent\").count().orderBy(col(\"count\").desc())\n",
    "df_intent.show(truncate=False)\n",
    "del df_intent\n",
    "\n",
    "# Giải phóng bộ nhớ\n",
    "df_verify.unpersist()\n",
    "del df_verify\n",
    "gc.collect()\n",
    "print(\"\\n✓ Đã giải phóng bộ nhớ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba3479",
   "metadata": {},
   "source": [
    "## 10. Dừng Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa7a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giải phóng bộ nhớ trước khi dừng Spark\n",
    "import gc\n",
    "\n",
    "# Unpersist df_articles nếu còn cache\n",
    "try:\n",
    "    df_articles.unpersist()\n",
    "    del df_articles\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Giải phóng model và tokenizer\n",
    "try:\n",
    "    del model, tokenizer, label_mappings\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Giải phóng VnCoreNLP\n",
    "try:\n",
    "    if annotator is not None:\n",
    "        del annotator\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Thu gom rác Python\n",
    "gc.collect()\n",
    "print(\"✓ Đã giải phóng tất cả bộ nhớ\")\n",
    "\n",
    "# Dừng Spark Session\n",
    "spark.stop()\n",
    "print(\"✓ Spark Session đã được dừng!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
