{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e337a4f1",
   "metadata": {},
   "source": [
    "## 1. Import Libraries và Khởi tạo Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import (\n",
    "    col, explode, split, trim, current_timestamp, row_number, \n",
    "    when, regexp_replace, monotonically_increasing_id, dense_rank, udf, collect_list, broadcast\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "\n",
    "# Set AWS environment variables for MinIO\n",
    "os.environ['AWS_REGION'] = 'us-east-1'\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'admin'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'admin123'\n",
    "\n",
    "# Khởi tạo Spark Session - SIMPLE CONFIG cho 1 worker x 1.5GB x 2 cores\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Load_Data_To_Gold_Tables\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    \n",
    "    # === MEMORY CONFIG - Đơn giản cho 1 worker ===\n",
    "    .config(\"spark.executor.memory\", \"1536m\")     # 1.5GB\n",
    "    .config(\"spark.executor.cores\", \"2\")          # 2 cores\n",
    "    .config(\"spark.driver.memory\", \"512m\")\n",
    "    .config(\"spark.memory.fraction\", \"0.6\")\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\")\n",
    "    \n",
    "    # === SHUFFLE CONFIG ===\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "    .config(\"spark.default.parallelism\", \"50\")\n",
    "    \n",
    "    # === ADAPTIVE QUERY EXECUTION ===\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    \n",
    "    # === BROADCAST JOIN ===\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\")\n",
    "    \n",
    "    # === COMPRESSION ===\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\n",
    "    .config(\"spark.shuffle.compress\", \"true\")\n",
    "    .config(\"spark.rdd.compress\", \"true\")\n",
    "    \n",
    "    # === SERIALIZATION ===\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    \n",
    "    # ===== Iceberg Catalog qua Nessie =====\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v2\")\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://gold/\")\n",
    "    .config(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    \n",
    "    # ===== Cấu hình MinIO =====\n",
    "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.access-key-id\", \"admin\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.secret-access-key\", \"admin123\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.region\", \"us-east-1\")\n",
    "    \n",
    "    # ===== Spark + Hadoop S3 connector =====\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.region\", \"us-east-1\")\n",
    "    .config(\"spark.executorEnv.AWS_REGION\", \"us-east-1\")\n",
    "    .config(\"spark.executorEnv.AWS_ACCESS_KEY_ID\", \"admin\")\n",
    "    .config(\"spark.executorEnv.AWS_SECRET_ACCESS_KEY\", \"admin123\")\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/hadoop-aws-3.3.4.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"✓ Spark Session - Simple config cho 1 worker!\")\n",
    "print(f\"Spark Master: {spark.sparkContext.master}\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"Memory: 1.5GB executor + 512MB driver\")\n",
    "print(f\"Cores: 2\")\n",
    "print(f\"Shuffle Partitions: 50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afcde3c",
   "metadata": {},
   "source": [
    "## 2. Đọc Dữ Liệu từ Result Model và Lọc Records Chưa Xử Lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7798a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc dữ liệu từ bảng result_multi_model\n",
    "import gc\n",
    "\n",
    "df_result = spark.table(\"nessie.silver_tables.result_multi_model\")\n",
    "\n",
    "print(f\"Tổng số records trong result_multi_model: {df_result.count()}\")\n",
    "\n",
    "# Kiểm tra xem Post table đã có dữ liệu chưa\n",
    "try:\n",
    "    df_existing_posts = spark.table(\"nessie.gold_result_model_multi_task.Post\")\n",
    "    existing_count = df_existing_posts.count()\n",
    "    print(f\"Tìm thấy {existing_count} posts đã được xử lý trước đó\")\n",
    "    \n",
    "    # OPTIMIZE: Lọc bằng join anti thay vì collect() + isin()\n",
    "    if existing_count > 0:\n",
    "        df_result_new = df_result.join(\n",
    "            df_existing_posts.select(\"postID\"),\n",
    "            on=\"postID\",\n",
    "            how=\"left_anti\"  # LEFT ANTI JOIN = NOT IN\n",
    "        )\n",
    "        new_count = df_result_new.count()\n",
    "        print(f\"Số lượng posts mới cần xử lý: {new_count}\")\n",
    "        \n",
    "        if new_count == 0:\n",
    "            print(\"\\n⚠ Không có posts mới để xử lý. Tất cả dữ liệu đã được load vào Gold tables.\")\n",
    "            print(\"Nếu muốn load lại, hãy xóa dữ liệu trong Gold tables trước.\")\n",
    "        else:\n",
    "            df_result = df_result_new\n",
    "    else:\n",
    "        print(\"Không có posts nào trong Gold tables, sẽ xử lý toàn bộ dữ liệu\")\n",
    "    \n",
    "    # Giải phóng df_existing_posts\n",
    "    df_existing_posts.unpersist()\n",
    "    del df_existing_posts\n",
    "    gc.collect()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Gold tables chưa có dữ liệu hoặc chưa tồn tại: {e}\")\n",
    "    print(\"Sẽ xử lý toàn bộ dữ liệu từ result_multi_model\")\n",
    "\n",
    "# OPTIMIZE: Cache df_result vì sẽ dùng nhiều lần\n",
    "df_result = df_result.repartition(100).cache()\n",
    "\n",
    "print(\"\\nSchema:\")\n",
    "df_result.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "df_result.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4a51a1",
   "metadata": {},
   "source": [
    "## 3. Transform và Load vào Table Entity\n",
    "\n",
    "Entity table chỉ lưu các loại entity types và mô tả (VD: MAJOR - \"Chuyên ngành học\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abdd22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa mô tả cho các entity types\n",
    "import gc\n",
    "\n",
    "entity_type_descriptions = {\n",
    "    \"ORG\": \"ORGANIZATION : Tổ chức/Trường (ví dụ: Trường Đại học Bách khoa Hà Nội, Viện Công nghệ, các viện/khoa/bộ môn)\",\n",
    "    \"PRO\": \"PROGRAM : Chương trình đào tạo (ví dụ: Chương trình Tiên tiến, Chương trình KSCLC, ELITECH)\",\n",
    "    \"MAJ\": \"MAJOR : Ngành học, chuyên ngành, nghề nghiệp (ví dụ: Khoa học máy tính, Công nghệ thông tin, Kỹ thuật Điện tử - Viễn thông, Bác sĩ)\",\n",
    "    \"SCO\": \"SCORE : Điểm số (ví dụ: 27.5 điểm, Điểm chuẩn 28)\",\n",
    "    \"DATE\": \"DATE : Thời gian (ví dụ: Năm 2024, Năm 2025, 15/8/2024)\",\n",
    "    \"EX\": \"EXAM : Kỳ thi (ví dụ: Kỳ thi đánh giá năng lực, TOEIC, IELTS, TOPIK)\",\n",
    "    \"LOC\": \"LOCATION : Địa điểm (ví dụ: Hà Nội, Thành phố Hồ Chí Minh)\",\n",
    "    \"FEE\": \"FEE : Học phí/Chi phí (ví dụ: Học phí, Mức đóng, Học bổng)\",\n",
    "    \"SUBJ\": \"SUBJECT : Môn học (ví dụ: Toán, Lý, Hóa)\",\n",
    "    \"TERM\": \"TERM : Tổ hợp môn (ví dụ: A1, B1)\",\n",
    "    \"SAL\": \"SALARY : Lương (ví dụ: Mức lương, Thu nhập)\"\n",
    "}\n",
    "\n",
    "# Parse NER labels để extract entity types\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def extract_entity_types(ner_labels):\n",
    "    \"\"\"Extract unique entity types from NER labels\"\"\"\n",
    "    if not ner_labels:\n",
    "        return []\n",
    "    \n",
    "    labels = ner_labels.split()\n",
    "    entity_types = set()\n",
    "    \n",
    "    for label in labels:\n",
    "        if label.startswith('B-') or label.startswith('I-'):\n",
    "            entity_type = label[2:]  # Lấy phần sau \"B-\" hoặc \"I-\"\n",
    "            entity_types.add(entity_type)\n",
    "    \n",
    "    return list(entity_types)\n",
    "\n",
    "# Extract unique entity types từ toàn bộ dữ liệu\n",
    "df_entity_types_raw = df_result.select(\n",
    "    explode(extract_entity_types(col(\"Label_NER\"))).alias(\"entityType\")\n",
    ").filter(\n",
    "    col(\"entityType\").isNotNull()\n",
    ").distinct()\n",
    "\n",
    "# Tạo DataFrame với entityDetail từ mapping\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Tạo list các entity types với descriptions\n",
    "entity_list = []\n",
    "for entity_type in df_entity_types_raw.collect():\n",
    "    et = entity_type.entityType\n",
    "    entity_list.append({\n",
    "        \"entityType\": et,\n",
    "        \"entityDetail\": entity_type_descriptions.get(et, f\"Entity type: {et}\")\n",
    "    })\n",
    "\n",
    "if entity_list:\n",
    "    df_unique_entities = spark.createDataFrame(entity_list)\n",
    "    \n",
    "    # Thêm entityID\n",
    "    window_spec = Window.orderBy(\"entityType\")\n",
    "    df_entities = df_unique_entities.withColumn(\n",
    "        \"entityID\",\n",
    "        row_number().over(window_spec)\n",
    "    ).withColumn(\n",
    "        \"created_at\", current_timestamp()\n",
    "    ).withColumn(\n",
    "        \"updated_at\", current_timestamp()\n",
    "    ).select(\n",
    "        col(\"entityID\"),\n",
    "        col(\"entityType\"),\n",
    "        col(\"entityDetail\"),\n",
    "        col(\"created_at\"),\n",
    "        col(\"updated_at\")\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSố lượng unique entity types từ dữ liệu mới: {df_entities.count()}\")\n",
    "    df_entities.show(20, truncate=False)\n",
    "    \n",
    "    # Kiểm tra và merge với entities đã có\n",
    "    print(\"\\n=== Loading data into Entity table ===\")\n",
    "    try:\n",
    "        df_existing_entities = spark.table(\"nessie.gold_result_model_multi_task.Entity\")\n",
    "        existing_count = df_existing_entities.count()\n",
    "        print(f\"Tìm thấy {existing_count} entity types đã tồn tại\")\n",
    "        \n",
    "        max_entity_id = df_existing_entities.agg({\"entityID\": \"max\"}).collect()[0][0]\n",
    "        if max_entity_id is None:\n",
    "            max_entity_id = 0\n",
    "        print(f\"Max entityID hiện tại: {max_entity_id}\")\n",
    "        \n",
    "        # Chỉ thêm các entity types mới\n",
    "        df_new_entities = df_entities.join(\n",
    "            df_existing_entities.select(\"entityType\"),\n",
    "            on=\"entityType\",\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "        \n",
    "        new_count = df_new_entities.count()\n",
    "        print(f\"Số lượng entity types mới cần thêm: {new_count}\")\n",
    "        \n",
    "        if new_count > 0:\n",
    "            window_spec = Window.orderBy(\"entityType\")\n",
    "            df_new_entities = df_new_entities.withColumn(\n",
    "                \"entityID\",\n",
    "                row_number().over(window_spec) + max_entity_id\n",
    "            ).select(\n",
    "                col(\"entityID\"),\n",
    "                col(\"entityType\"),\n",
    "                col(\"entityDetail\"),\n",
    "                col(\"created_at\"),\n",
    "                col(\"updated_at\")\n",
    "            )\n",
    "            \n",
    "            df_new_entities.writeTo(\"nessie.gold_result_model_multi_task.Entity\") \\\n",
    "                .using(\"iceberg\") \\\n",
    "                .append()\n",
    "            print(f\"Đã thêm {new_count} entity types mới!\")\n",
    "            \n",
    "            df_entities = df_existing_entities.union(df_new_entities)\n",
    "            \n",
    "            # Giải phóng df_new_entities\n",
    "            df_new_entities.unpersist()\n",
    "            del df_new_entities\n",
    "        else:\n",
    "            print(\"Không có entity types mới cần thêm\")\n",
    "            df_entities = df_existing_entities\n",
    "        \n",
    "        # Giải phóng df_existing_entities\n",
    "        df_existing_entities.unpersist()\n",
    "        del df_existing_entities\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Table Entity chưa có dữ liệu hoặc chưa tồn tại: {e}\")\n",
    "        print(\"Tạo mới table với dữ liệu hiện tại\")\n",
    "        df_entities.writeTo(\"nessie.gold_result_model_multi_task.Entity\") \\\n",
    "            .using(\"iceberg\") \\\n",
    "            .create()\n",
    "        print(f\"Đã tạo table Entity với {df_entities.count()} records!\")\n",
    "    \n",
    "    # Giải phóng các DataFrame tạm\n",
    "    df_entity_types_raw.unpersist()\n",
    "    df_unique_entities.unpersist()\n",
    "    del df_entity_types_raw, df_unique_entities\n",
    "    gc.collect()\n",
    "    print(\"✓ Đã giải phóng bộ nhớ\")\n",
    "else:\n",
    "    print(\"Không tìm thấy entity types trong dữ liệu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a66385",
   "metadata": {},
   "source": [
    "## 4. Transform và Load vào Table Topic\n",
    "\n",
    "Topic table lưu các chủ đề và mô tả (VD: MAJOR - \"Ngành học, chuyên ngành\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a1ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa mô tả cho các topic types\n",
    "import gc\n",
    "\n",
    "topic_descriptions = {\n",
    "    \"MAJOR\": \"Ngành học, chuyên ngành\",\n",
    "    \"SUBJECT_COMBINATION\": \"Tổ hợp môn thi (A, B, C, D)\",\n",
    "    \"TUITION\": \"Học phí, học bổng\",\n",
    "    \"CERTIFICATE\": \"IELTS, TOEFL, SAT, kỳ thi năng lực\",\n",
    "    \"UNIVERSITY\": \"Thông tin trường\",\n",
    "    \"STUDENT_LIFE\": \"Ký túc xá, CLB, làm thêm\",\n",
    "    \"CAREER\": \"Nghề nghiệp, lương\",\n",
    "    \"STUDY\": \"Phương pháp học, môn học ĐH\",\n",
    "    \"LANGUAGE\": \"Tiếng Anh, ngoại ngữ\",\n",
    "    \"OTHER\": \"Khác\"\n",
    "}\n",
    "\n",
    "# Parse Label_Topic (multi-label, phân cách bằng '|')\n",
    "df_topics_raw = df_result.select(\n",
    "    explode(split(col(\"Label_Topic\"), \"\\\\|\")).alias(\"topicName\")\n",
    ").filter(\n",
    "    (col(\"topicName\").isNotNull()) & \n",
    "    (col(\"topicName\") != \"None\") &\n",
    "    (trim(col(\"topicName\")) != \"\")\n",
    ").select(\n",
    "    trim(col(\"topicName\")).alias(\"topicName\")\n",
    ").distinct()\n",
    "\n",
    "# Tạo list các topics với descriptions\n",
    "topic_list = []\n",
    "for topic in df_topics_raw.collect():\n",
    "    topic_name = topic.topicName\n",
    "    topic_list.append({\n",
    "        \"topicName\": topic_name,\n",
    "        \"topicDetail\": topic_descriptions.get(topic_name, \"Chủ đề khác\")\n",
    "    })\n",
    "\n",
    "if topic_list:\n",
    "    df_unique_topics = spark.createDataFrame(topic_list)\n",
    "    \n",
    "    # Thêm topicID\n",
    "    window_spec = Window.orderBy(\"topicName\")\n",
    "    df_topics = df_unique_topics.withColumn(\n",
    "        \"topicID\",\n",
    "        row_number().over(window_spec)\n",
    "    ).withColumn(\n",
    "        \"created_at\", current_timestamp()\n",
    "    ).withColumn(\n",
    "        \"updated_at\", current_timestamp()\n",
    "    ).select(\n",
    "        col(\"topicID\"),\n",
    "        col(\"topicName\"),\n",
    "        col(\"topicDetail\"),\n",
    "        col(\"created_at\"),\n",
    "        col(\"updated_at\")\n",
    "    )\n",
    "    \n",
    "    print(f\"Số lượng unique topics từ dữ liệu mới: {df_topics.count()}\")\n",
    "    df_topics.show(20, truncate=False)\n",
    "    \n",
    "    print(\"\\n=== Loading data into Topic table ===\")\n",
    "    try:\n",
    "        df_existing_topics = spark.table(\"nessie.gold_result_model_multi_task.Topic\")\n",
    "        existing_count = df_existing_topics.count()\n",
    "        print(f\"Tìm thấy {existing_count} topics đã tồn tại\")\n",
    "        \n",
    "        max_topic_id = df_existing_topics.agg({\"topicID\": \"max\"}).collect()[0][0]\n",
    "        if max_topic_id is None:\n",
    "            max_topic_id = 0\n",
    "        print(f\"Max topicID hiện tại: {max_topic_id}\")\n",
    "        \n",
    "        # Chỉ thêm các topics mới\n",
    "        df_new_topics = df_topics.join(\n",
    "            df_existing_topics.select(\"topicName\"),\n",
    "            on=\"topicName\",\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "        \n",
    "        new_count = df_new_topics.count()\n",
    "        print(f\"Số lượng topics mới cần thêm: {new_count}\")\n",
    "        \n",
    "        if new_count > 0:\n",
    "            window_spec = Window.orderBy(\"topicName\")\n",
    "            df_new_topics = df_new_topics.withColumn(\n",
    "                \"topicID\",\n",
    "                row_number().over(window_spec) + max_topic_id\n",
    "            ).select(\n",
    "                col(\"topicID\"),\n",
    "                col(\"topicName\"),\n",
    "                col(\"topicDetail\"),\n",
    "                col(\"created_at\"),\n",
    "                col(\"updated_at\")\n",
    "            )\n",
    "            \n",
    "            df_new_topics.writeTo(\"nessie.gold_result_model_multi_task.Topic\") \\\n",
    "                .using(\"iceberg\") \\\n",
    "                .append()\n",
    "            print(f\"Đã thêm {new_count} topics mới!\")\n",
    "            \n",
    "            df_topics = df_existing_topics.union(df_new_topics)\n",
    "            \n",
    "            # Giải phóng df_new_topics\n",
    "            df_new_topics.unpersist()\n",
    "            del df_new_topics\n",
    "        else:\n",
    "            print(\"Không có topics mới cần thêm\")\n",
    "            df_topics = df_existing_topics\n",
    "        \n",
    "        # Giải phóng df_existing_topics\n",
    "        df_existing_topics.unpersist()\n",
    "        del df_existing_topics\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Table Topic chưa có dữ liệu hoặc chưa tồn tại: {e}\")\n",
    "        print(\"Tạo mới table với dữ liệu hiện tại\")\n",
    "        df_topics.writeTo(\"nessie.gold_result_model_multi_task.Topic\") \\\n",
    "            .using(\"iceberg\") \\\n",
    "            .create()\n",
    "        print(f\"Đã tạo table Topic với {df_topics.count()} records!\")\n",
    "    \n",
    "    # Giải phóng các DataFrame tạm\n",
    "    df_topics_raw.unpersist()\n",
    "    df_unique_topics.unpersist()\n",
    "    del df_topics_raw, df_unique_topics\n",
    "    gc.collect()\n",
    "    print(\"✓ Đã giải phóng bộ nhớ\")\n",
    "else:\n",
    "    print(\"Không tìm thấy topics trong dữ liệu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f9f697",
   "metadata": {},
   "source": [
    "## 5. Transform và Load vào Table Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9948ecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# --- CAU HINH ---\n",
    "BATCH_SIZE = 1000  # Xu ly 1000 dong moi lan\n",
    "\n",
    "print(f\"=== CHAY VOI CAU HINH: 3 WORKERS | BATCH: {BATCH_SIZE} ===\")\n",
    "\n",
    "# 1. CHUAN BI DATA\n",
    "# Lay nguyen goc du lieu (khong cat chuoi vi du lieu < 256 ky tu)\n",
    "df_posts_final = df_result.select(\n",
    "    col(\"postID\"),\n",
    "    col(\"description_Normalized\").alias(\"description\"),\n",
    "    col(\"timePublish\"),\n",
    "    col(\"likeCount\"),\n",
    "    col(\"commentCount\"),\n",
    "    col(\"shareCount\"),\n",
    "    col(\"Label_Intent\").alias(\"intent\"),\n",
    "    col(\"type\")\n",
    ").withColumn(\"created_at\", current_timestamp()) \\\n",
    " .withColumn(\"updated_at\", current_timestamp())\n",
    "\n",
    "# 2. LAY LIST ID DE CHIA VIEC\n",
    "all_ids = [r.postID for r in df_posts_final.select(\"postID\").collect()]\n",
    "total_records = len(all_ids)\n",
    "print(f\"Tong so bai can xu ly: {total_records}\")\n",
    "\n",
    "if total_records > 0:\n",
    "    # 3. KHOI TAO BANG (neu chua co)\n",
    "    try:\n",
    "        # Lay 1 dong mau de tao bang\n",
    "        df_posts_final.limit(1).writeTo(\"nessie.gold_result_model_multi_task.Post\") \\\n",
    "            .using(\"iceberg\").createOrReplace()\n",
    "        print(\"Da khoi tao bang dich.\")\n",
    "    except Exception as e:\n",
    "        print(\"Bang da ton tai, chuyen sang che do Append.\")\n",
    "\n",
    "    # 4. CHAY VONG LAP (BATCH PROCESSING)\n",
    "    num_batches = math.ceil(total_records / BATCH_SIZE)\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start = i * BATCH_SIZE\n",
    "        end = start + BATCH_SIZE\n",
    "        batch_ids = all_ids[start:end]\n",
    "        \n",
    "        print(f\"Batch {i+1}/{num_batches}: Dang xu ly {len(batch_ids)} dong...\")\n",
    "        \n",
    "        # Loc du lieu cho batch nay\n",
    "        df_batch = df_posts_final.filter(col(\"postID\").isin(batch_ids))\n",
    "        \n",
    "        # Repartition = 6 (toi uu cho 3 workers x 2 cores)\n",
    "        try:\n",
    "            df_batch.repartition(6).writeTo(\"nessie.gold_result_model_multi_task.Post\") \\\n",
    "                .using(\"iceberg\") \\\n",
    "                .append()\n",
    "            print(f\"   Batch {i+1} OK.\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Batch {i+1} LOI: {e}\")\n",
    "            \n",
    "        # Don dep RAM\n",
    "        df_batch.unpersist()\n",
    "        del df_batch\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"\\nHOAN TAT! Du lieu da duoc ghi thanh cong.\")\n",
    "    \n",
    "    # Giải phóng df_posts_final\n",
    "    df_posts_final.unpersist()\n",
    "    del df_posts_final, all_ids\n",
    "    gc.collect()\n",
    "    print(\"✓ Đã giải phóng bộ nhớ\")\n",
    "else:\n",
    "    print(\"Khong co du lieu moi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecfe089",
   "metadata": {},
   "source": [
    "## 6. Transform và Load vào Table Post_Entity\n",
    "\n",
    "Bảng quan hệ M:N giữa Post và Entity - Lưu các entity values thực tế được extract từ posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94c6277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import (\n",
    "    udf, col, explode, row_number, current_timestamp, \n",
    "    trim, lower, regexp_replace, when, count, broadcast, lit\n",
    ")\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType\n",
    "import gc\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. DEFINE UDF (User Defined Function)\n",
    "# ==============================================================================\n",
    "@udf(returnType=ArrayType(StructType([\n",
    "    StructField(\"entityType\", StringType()),\n",
    "    StructField(\"entityName\", StringType())\n",
    "])))\n",
    "def extract_entities_raw(ner_labels, text):\n",
    "    \"\"\"\n",
    "    Extract entities từ BIO labels và text.\n",
    "    \"\"\"\n",
    "    if not ner_labels or not text:\n",
    "        return []\n",
    "    \n",
    "    # Clean nhẹ và split\n",
    "    labels = ner_labels.strip().split()\n",
    "    words = text.strip().split()\n",
    "    \n",
    "    # Kiểm tra độ dài để tránh lỗi index out of range\n",
    "    if len(labels) != len(words):\n",
    "        return []\n",
    "    \n",
    "    entities = []\n",
    "    current_type = None\n",
    "    current_words = []\n",
    "    \n",
    "    for label, word in zip(labels, words):\n",
    "        label = label.strip()\n",
    "        \n",
    "        if label.startswith('B-'):\n",
    "            if current_type and current_words:\n",
    "                entities.append({'entityType': current_type, 'entityName': ' '.join(current_words)})\n",
    "            current_type = label[2:]\n",
    "            current_words = [word]\n",
    "            \n",
    "        elif label.startswith('I-') and current_type:\n",
    "            # Strict check: I-tag phải khớp với B-tag\n",
    "            if label[2:] == current_type:\n",
    "                current_words.append(word)\n",
    "            else:\n",
    "                entities.append({'entityType': current_type, 'entityName': ' '.join(current_words)})\n",
    "                current_type = None\n",
    "                current_words = []\n",
    "        else:\n",
    "            if current_type and current_words:\n",
    "                entities.append({'entityType': current_type, 'entityName': ' '.join(current_words)})\n",
    "            current_type = None\n",
    "            current_words = []\n",
    "    \n",
    "    if current_type and current_words:\n",
    "        entities.append({'entityType': current_type, 'entityName': ' '.join(current_words)})\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. MAIN PROCESSING FLOW\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=== Extracting entities từ posts ===\")\n",
    "\n",
    "# --- Bước A: Extract & Clean Data ---\n",
    "# Giả định df_result đã có sẵn từ các bước trước\n",
    "df_extracted = df_result.select(\n",
    "    col(\"postID\"),\n",
    "    col(\"Label_NER\"),\n",
    "    col(\"description_Normalized\")\n",
    ").select(\n",
    "    col(\"postID\"),\n",
    "    explode(extract_entities_raw(col(\"Label_NER\"), col(\"description_Normalized\"))).alias(\"entity\")\n",
    ").select(\n",
    "    col(\"postID\"),\n",
    "    col(\"entity.entityType\").alias(\"entityType\"),\n",
    "    col(\"entity.entityName\").alias(\"raw_name\")\n",
    ").withColumn(\n",
    "    \"entityName\",\n",
    "    regexp_replace(col(\"raw_name\"), \"_\", \" \")\n",
    ").withColumn(\n",
    "    \"entityName\",\n",
    "    regexp_replace(col(\"entityName\"), r\"^\\.+|\\.+$\", \"\")\n",
    ").withColumn(\n",
    "    \"entityName\",\n",
    "    trim(col(\"entityName\"))\n",
    ").filter(\n",
    "    (col(\"entityType\").isNotNull()) & (col(\"entityName\") != \"\")\n",
    ").drop(\"raw_name\")\n",
    "\n",
    "# Repartition và cache để tối ưu hiệu suất cho các bước sau\n",
    "df_extracted = df_extracted.repartition(200, \"entityType\").cache()\n",
    "\n",
    "extracted_count = df_extracted.count()\n",
    "print(f\"✓ Entities extracted (cleaned): {extracted_count}\")\n",
    "\n",
    "if extracted_count == 0:\n",
    "    print(\"⚠ Không có entities để xử lý!\")\n",
    "else:\n",
    "    # --- Bước B: Add Entity Order ---\n",
    "    window_spec = Window.partitionBy(\"postID\").orderBy(\"entityType\", \"entityName\")\n",
    "    df_ordered = df_extracted.withColumn(\n",
    "        \"entityOrder\",\n",
    "        row_number().over(window_spec)\n",
    "    )\n",
    "    \n",
    "    # --- Bước C: Chuẩn bị dữ liệu để JOIN ---\n",
    "    # Chuẩn bị bảng Entities (Master Data)\n",
    "    # Distinct để đảm bảo 1 Type không xuất hiện nhiều lần gây nhân bản dữ liệu\n",
    "    df_entities_ready = broadcast(\n",
    "        df_entities.select(\n",
    "            col(\"entityID\"),\n",
    "            col(\"entityType\"),\n",
    "            lower(trim(col(\"entityType\"))).alias(\"type_norm\")\n",
    "        ).distinct() \n",
    "    )\n",
    "    \n",
    "    # Chuẩn bị bảng Post (Transaction Data)\n",
    "    df_post_ready = df_ordered.withColumn(\n",
    "        \"type_norm\", lower(trim(col(\"entityType\")))\n",
    "    )\n",
    "    \n",
    "    # --- Bước D: JOIN ---\n",
    "    # Thêm cột entityNameNormalized với giá trị NULL\n",
    "    df_final_join = df_post_ready.join(\n",
    "        df_entities_ready,\n",
    "        on=\"type_norm\",\n",
    "        how=\"inner\"\n",
    "    ).select(\n",
    "        col(\"postID\"),\n",
    "        col(\"entityID\"),\n",
    "        col(\"entityOrder\"),\n",
    "        col(\"entityName\"),\n",
    "        lit(None).cast(StringType()).alias(\"entityNameNormalized\")  # Thêm cột mới với giá trị NULL\n",
    "    ).withColumn(\n",
    "        \"created_at\", current_timestamp()\n",
    "    ).withColumn(\n",
    "        \"updated_at\", current_timestamp()\n",
    "    )\n",
    "    \n",
    "    final_count = df_final_join.count()\n",
    "    print(f\"✓ Số lượng quan hệ Post-Entity: {final_count}\")\n",
    "    \n",
    "    if final_count > 0:\n",
    "        # Tối ưu kích thước file Iceberg (Tránh small files problem)\n",
    "        # Giả sử target 100k - 200k dòng mỗi file\n",
    "        num_partitions = max(1, final_count // 100000) \n",
    "        df_final_join = df_final_join.coalesce(num_partitions)\n",
    "        \n",
    "        # --- WRITE TO ICEBERG ---\n",
    "        target_table = \"nessie.gold_result_model_multi_task.Post_Entity\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\n Đang ghi {final_count} records vào {target_table}...\")\n",
    "            df_final_join.writeTo(target_table) \\\n",
    "                .using(\"iceberg\") \\\n",
    "                .append()\n",
    "            print(f\"✓ SUCCESS: Đã thêm {final_count} dòng vào bảng Post_Entity.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Xử lý trường hợp bảng chưa tồn tại\n",
    "            if \"not found\" in str(e).lower() or \"does not exist\" in str(e).lower():\n",
    "                print(f\"Table {target_table} chưa tồn tại. Đang tạo mới...\")\n",
    "                df_final_join.writeTo(target_table) \\\n",
    "                    .using(\"iceberg\") \\\n",
    "                    .create()\n",
    "                print(f\"✓ SUCCESS: Đã tạo bảng và insert {final_count} dòng.\")\n",
    "            else:\n",
    "                print(\" ERROR: Ghi dữ liệu thất bại.\")\n",
    "                # In ra schema để debug nếu vẫn lỗi\n",
    "                print(\"Schema DataFrame hiện tại:\")\n",
    "                df_final_join.printSchema()\n",
    "                raise e\n",
    "    else:\n",
    "        print(\"⚠ WARNING: Không có dữ liệu sau khi Join (Check lại bảng df_entities có map đúng Type không).\")\n",
    "    \n",
    "    # Cleanup memory\n",
    "    df_extracted.unpersist()\n",
    "    df_ordered.unpersist()\n",
    "    df_final_join.unpersist()\n",
    "    del df_extracted, df_ordered, df_post_ready, df_final_join\n",
    "    gc.collect()y\n",
    "    print(\"\\n✓ Đã giải phóng bộ nhớ cache.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c09b3a0",
   "metadata": {},
   "source": [
    "## 7. Transform và Load vào Table Post_Topic\n",
    "\n",
    "Bảng quan hệ M:N giữa Post và Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa584629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse topics cho mỗi post\n",
    "import gc\n",
    "\n",
    "df_post_topics_raw = df_result.select(\n",
    "    col(\"postID\"),\n",
    "    explode(split(col(\"Label_Topic\"), \"\\\\|\")).alias(\"topicName\")\n",
    ").filter(\n",
    "    (col(\"topicName\").isNotNull()) & \n",
    "    (col(\"topicName\") != \"None\") &\n",
    "    (trim(col(\"topicName\")) != \"\")\n",
    ").select(\n",
    "    col(\"postID\"),\n",
    "    trim(col(\"topicName\")).alias(\"topicName\")\n",
    ").distinct()\n",
    "\n",
    "# OPTIMIZE: Broadcast Topic table vì nhỏ\n",
    "df_post_topic = df_post_topics_raw.join(\n",
    "    broadcast(df_topics.select(\"topicID\", \"topicName\")),\n",
    "    on=\"topicName\",\n",
    "    how=\"inner\"\n",
    ").select(\n",
    "    col(\"postID\"),\n",
    "    col(\"topicID\")\n",
    ").withColumn(\n",
    "    \"created_at\", current_timestamp()\n",
    ").withColumn(\n",
    "    \"updated_at\", current_timestamp()\n",
    ")\n",
    "\n",
    "topic_count = df_post_topic.count()\n",
    "print(f\"Tổng số quan hệ Post-Topic mới: {topic_count}\")\n",
    "df_post_topic.show(10, truncate=False)\n",
    "\n",
    "print(\"\\n=== Loading data into Post_Topic table ===\")\n",
    "if topic_count > 0:\n",
    "    # OPTIMIZE: Coalesce để giảm số partitions\n",
    "    num_partitions = max(1, topic_count // 1000)\n",
    "    df_post_topic = df_post_topic.coalesce(num_partitions)\n",
    "    \n",
    "    try:\n",
    "        df_post_topic.writeTo(\"nessie.gold_result_model_multi_task.Post_Topic\") \\\n",
    "            .using(\"iceberg\") \\\n",
    "            .append()\n",
    "        print(f\"✓ Đã thêm {topic_count} quan hệ Post-Topic mới!\")\n",
    "    except Exception as e:\n",
    "        if \"table does not exist\" in str(e).lower() or \"not found\" in str(e).lower():\n",
    "            print(\"Table Post_Topic chưa tồn tại, tạo mới...\")\n",
    "            df_post_topic.writeTo(\"nessie.gold_result_model_multi_task.Post_Topic\") \\\n",
    "                .using(\"iceberg\") \\\n",
    "                .create()\n",
    "            print(f\"✓ Đã tạo table Post_Topic với {topic_count} records!\")\n",
    "        else:\n",
    "            raise e\n",
    "else:\n",
    "    print(\"Không có quan hệ Post-Topic mới cần thêm\")\n",
    "\n",
    "# Giải phóng bộ nhớ\n",
    "df_post_topics_raw.unpersist()\n",
    "df_post_topic.unpersist()\n",
    "del df_post_topics_raw, df_post_topic\n",
    "gc.collect()\n",
    "print(\"✓ Đã giải phóng bộ nhớ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7991a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MEMORY CLEANUP ===\n",
    "import gc\n",
    "\n",
    "print(\"\\n=== Cleaning up memory ===\")\n",
    "df_result.unpersist()\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Giải phóng các biến còn lại\n",
    "try:\n",
    "    del df_entities, df_topics\n",
    "except:\n",
    "    pass\n",
    "\n",
    "gc.collect()\n",
    "print(\"✓ Đã giải phóng toàn bộ bộ nhớ cache.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab2023",
   "metadata": {},
   "source": [
    "## 8. Verify Dữ Liệu Đã Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db7b329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SUMMARY - Data Loaded to Gold Tables\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tables = [\n",
    "    \"Entity\",\n",
    "    \"Topic\",\n",
    "    \"Post\",\n",
    "    \"Post_Entity\",\n",
    "    \"Post_Topic\"\n",
    "]\n",
    "\n",
    "for table_name in tables:\n",
    "    df_verify = spark.table(f\"nessie.gold_result_model_multi_task.{table_name}\")\n",
    "    count = df_verify.count()\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Table: {table_name}\")\n",
    "    print(f\"Total records: {count}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    df_verify.show(5, truncate=False)\n",
    "    \n",
    "    # Giải phóng từng DataFrame sau khi show\n",
    "    df_verify.unpersist()\n",
    "    del df_verify\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Hoàn thành load dữ liệu vào tất cả Gold tables!\")\n",
    "print(\"=\"*80)\n",
    "print(\"✓ Đã giải phóng bộ nhớ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4292c0d4",
   "metadata": {},
   "source": [
    "## 9. Thống Kê và Phân Tích"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4466de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DATA PROFILING & STATISTICS (FIXED COLUMN NAMES)\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Thống kê Entity Types và Descriptions\n",
    "print(\"\\n=== 1. Entity Types và Descriptions ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT entityType, entityDetail\n",
    "    FROM nessie.gold_result_model_multi_task.Entity\n",
    "    ORDER BY entityType\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# 2. Thống kê số lượng entity mentions theo type (Top 20)\n",
    "print(\"\\n=== 2. Entity Mentions Distribution by Type (Top 20) ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT e.entityType, e.entityDetail, COUNT(pe.postID) as mention_count\n",
    "    FROM nessie.gold_result_model_multi_task.Entity e\n",
    "    LEFT JOIN nessie.gold_result_model_multi_task.Post_Entity pe ON e.entityID = pe.entityID\n",
    "    GROUP BY e.entityType, e.entityDetail\n",
    "    ORDER BY mention_count DESC\n",
    "    LIMIT 20\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# 3. Thống kê Topics\n",
    "print(\"\\n=== 3. Topics và Descriptions ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT topicName, topicDetail\n",
    "    FROM nessie.gold_result_model_multi_task.Topic\n",
    "    ORDER BY topicName\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# 4. Thống kê số lượng posts theo topic\n",
    "print(\"\\n=== 4. Topic Distribution ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT t.topicName, t.topicDetail, COUNT(pt.postID) as post_count\n",
    "    FROM nessie.gold_result_model_multi_task.Topic t\n",
    "    LEFT JOIN nessie.gold_result_model_multi_task.Post_Topic pt ON t.topicID = pt.topicID\n",
    "    GROUP BY t.topicName, t.topicDetail\n",
    "    ORDER BY post_count DESC\n",
    "\"\"\").show(20, truncate=False)\n",
    "\n",
    "# 5. Thống kê intent distribution\n",
    "print(\"\\n=== 5. Intent Distribution ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT intent, COUNT(*) as count\n",
    "    FROM nessie.gold_result_model_multi_task.Post\n",
    "    GROUP BY intent\n",
    "    ORDER BY count DESC\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# 6. Top entity values được nhắc đến nhiều nhất\n",
    "# FIX: Đã đổi entityValue -> entityName\n",
    "print(\"\\n=== 6. Top 20 Most Mentioned Entity Values ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.entityType,\n",
    "        pe.entityName,  -- <--- SỬA LẠI TÊN CỘT Ở ĐÂY\n",
    "        COUNT(DISTINCT pe.postID) as mention_count\n",
    "    FROM nessie.gold_result_model_multi_task.Post_Entity pe\n",
    "    JOIN nessie.gold_result_model_multi_task.Entity e ON pe.entityID = e.entityID\n",
    "    GROUP BY e.entityType, pe.entityName -- <--- VÀ Ở ĐÂY\n",
    "    ORDER BY mention_count DESC\n",
    "    LIMIT 20\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# 7. Sample Entity Values by Type (5 samples per type)\n",
    "# FIX: Đã đổi entityValue -> entityName\n",
    "print(\"\\n=== 7. Sample Entity Values by Type (5 samples per type) ===\")\n",
    "spark.sql(\"\"\"\n",
    "    WITH RankedValues AS (\n",
    "        SELECT \n",
    "            e.entityType,\n",
    "            e.entityDetail,\n",
    "            pe.entityName, -- <--- SỬA LẠI TÊN CỘT Ở ĐÂY\n",
    "            ROW_NUMBER() OVER (PARTITION BY e.entityType ORDER BY pe.entityName) as rn\n",
    "        FROM nessie.gold_result_model_multi_task.Entity e\n",
    "        LEFT JOIN nessie.gold_result_model_multi_task.Post_Entity pe ON e.entityID = pe.entityID\n",
    "        WHERE pe.entityName IS NOT NULL\n",
    "    )\n",
    "    SELECT \n",
    "        entityType,\n",
    "        entityDetail,\n",
    "        entityName as sample_value\n",
    "    FROM RankedValues\n",
    "    WHERE rn <= 5\n",
    "    ORDER BY entityType, rn\n",
    "\"\"\").show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a142d",
   "metadata": {},
   "source": [
    "## 10. Dừng Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae5244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giải phóng bộ nhớ trước khi dừng Spark\n",
    "import gc\n",
    "\n",
    "# Xóa tất cả cache\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Thu gom rác\n",
    "gc.collect()\n",
    "print(\"✓ Đã giải phóng toàn bộ bộ nhớ\")\n",
    "\n",
    "# Dừng Spark Session\n",
    "spark.stop()\n",
    "print(\"✓ Spark Session đã được dừng!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
