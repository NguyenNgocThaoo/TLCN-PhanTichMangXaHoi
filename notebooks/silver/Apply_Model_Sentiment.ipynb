{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "070f8335",
   "metadata": {},
   "source": [
    "# PhÃ¢n TÃ­ch Cáº£m XÃºc Comment vá»›i Vietnamese-Sentiment-visobert\n",
    "\n",
    "Notebook nÃ y sáº½:\n",
    "1. Láº¥y dá»¯ liá»‡u comment tá»« báº£ng `nessie.silver_tables.comment` trong khoáº£ng thá»i gian nháº¥t Ä‘á»‹nh\n",
    "2. Ãp dá»¥ng model Vietnamese-Sentiment-visobert Ä‘á»ƒ phÃ¢n tÃ­ch cáº£m xÃºc\n",
    "3. Tá»•ng há»£p cÃ¡c chá»‰ sá»‘ POS (tÃ­ch cá»±c), NEG (tiÃªu cá»±c), NEU (trung láº­p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8f0214",
   "metadata": {},
   "source": [
    "## 1. Import Libraries vÃ  Khá»Ÿi táº¡o Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, to_date, avg, sum as spark_sum\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Set AWS environment variables for MinIO\n",
    "os.environ['AWS_REGION'] = 'us-east-1'\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'admin'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'admin123'\n",
    "\n",
    "# Khá»Ÿi táº¡o Spark Session vá»›i Iceberg vÃ  Nessie catalog\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Apply_Sentiment_Model\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    # ===== Iceberg Catalog qua Nessie =====\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v2\")\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://silver/\")\n",
    "    .config(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    # ===== Cáº¥u hÃ¬nh MinIO (S3-compatible) =====\n",
    "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.access-key-id\", \"admin\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.secret-access-key\", \"admin123\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.region\", \"us-east-1\")\n",
    "    # ===== Spark + Hadoop S3 connector =====\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.region\", \"us-east-1\")\n",
    "    # Propagate environment variables to executors\n",
    "    .config(\"spark.executorEnv.AWS_REGION\", \"us-east-1\")\n",
    "    .config(\"spark.executorEnv.AWS_ACCESS_KEY_ID\", \"admin\")\n",
    "    .config(\"spark.executorEnv.AWS_SECRET_ACCESS_KEY\", \"admin123\")\n",
    "    # ===== JAR files =====\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/hadoop-aws-3.3.4.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"Spark Session Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o!\")\n",
    "print(f\"Spark Master: {spark.sparkContext.master}\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead0dcd4",
   "metadata": {},
   "source": [
    "## 2. Load Vietnamese Sentiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d2fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ÄÆ°á»ng dáº«n Ä‘áº¿n model\n",
    "model_path = \"/opt/spark-apps/Vietnamese-Sentiment-visobert\"\n",
    "\n",
    "print(\"â³ Äang táº£i model Vietnamese-Sentiment-visobert...\")\n",
    "\n",
    "# Load tokenizer vÃ  model (use_fast=False Ä‘á»ƒ trÃ¡nh lá»—i tokenizer.json)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Kiá»ƒm tra vÃ  sá»­ dá»¥ng GPU náº¿u cÃ³\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng!\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Labels: {model.config.id2label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce9fb8",
   "metadata": {},
   "source": [
    "## 3. Äá»‹nh nghÄ©a HÃ m Dá»± ÄoÃ¡n Cáº£m XÃºc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a430925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    \"\"\"\n",
    "    Dá»± Ä‘oÃ¡n cáº£m xÃºc cá»§a vÄƒn báº£n\n",
    "    Returns: sentiment label (POS, NEG, NEU)\n",
    "    \"\"\"\n",
    "    if not text or text.strip() == \"\":\n",
    "        return \"NEU\"\n",
    "    \n",
    "    try:\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256, padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Dá»± Ä‘oÃ¡n\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "        \n",
    "        # Chuyá»ƒn Ä‘á»•i id sang label\n",
    "        sentiment = model.config.id2label[predicted_class]\n",
    "        return sentiment\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {str(e)}\")\n",
    "        return \"NEU\"\n",
    "\n",
    "# Test function\n",
    "test_text = \"TrÆ°á»ng nÃ y ráº¥t tá»‘t, giáº£ng viÃªn nhiá»‡t tÃ¬nh!\"\n",
    "print(f\"Test text: {test_text}\")\n",
    "print(f\"Sentiment: {predict_sentiment(test_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415b5862",
   "metadata": {},
   "source": [
    "## 4. Láº¥y Dá»¯ Liá»‡u Comment tá»« Báº£ng Silver (Batch Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c374296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cáº¥u hÃ¬nh batch processing\n",
    "BATCH_SIZE = 20000  # Sá»‘ lÆ°á»£ng comment xá»­ lÃ½ má»—i batch\n",
    "\n",
    "print(\"ðŸ” Äang load batch comment chÆ°a xá»­ lÃ½ tá»« Silver...\")\n",
    "\n",
    "# ===== Tá»I Æ¯U: DÃ¹ng SQL vá»›i LEFT ANTI JOIN (SUBQUERY) =====\n",
    "# CÃ¡ch nÃ y NHANH NHáº¤T vÃ¬:\n",
    "# 1. Spark optimize toÃ n bá»™ query thÃ nh 1 execution plan\n",
    "# 2. Iceberg pruning: chá»‰ scan partition/file cáº§n thiáº¿t\n",
    "# 3. KHÃ”NG load toÃ n bá»™ Silver hay Gold vÃ o memory\n",
    "# 4. Push-down predicates xuá»‘ng storage layer\n",
    "\n",
    "try:\n",
    "    # Query vá»›i LEFT ANTI JOIN ngay trong SQL\n",
    "    # Spark sáº½ optimize vÃ  chá»‰ scan cáº§n thiáº¿t\n",
    "    df_comments = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            s.articleID as postID,\n",
    "            s.commentID,\n",
    "            s.comment,\n",
    "            s.commentTime,\n",
    "            s.commentLike,\n",
    "            s.levelComment,\n",
    "            s.numberOfReply,\n",
    "            s.created_at,\n",
    "            s.updated_at\n",
    "        FROM nessie.silver_tables.comment s\n",
    "        LEFT ANTI JOIN nessie.gold_result_model_multi_task.Comment_Sentiment g\n",
    "            ON s.commentID = g.commentID\n",
    "        WHERE s.comment IS NOT NULL\n",
    "          AND TRIM(s.comment) != ''\n",
    "        LIMIT {BATCH_SIZE}\n",
    "    \"\"\")\n",
    "    \n",
    "    total_comments = df_comments.count()\n",
    "    \n",
    "    # # Äáº¿m sá»‘ comment Ä‘Ã£ xá»­ lÃ½ (náº¿u cáº§n thá»‘ng kÃª)\n",
    "    # try:\n",
    "    #     existing_count = spark.sql(\"\"\"\n",
    "    #         SELECT COUNT(DISTINCT commentID) as count\n",
    "    #         FROM nessie.gold_result_model_multi_task.Comment_Sentiment\n",
    "    #     \"\"\").collect()[0]['count']\n",
    "    #     print(f\"ÄÃ£ cÃ³ trong Gold: {existing_count:,} comment\")\n",
    "    # except:\n",
    "    #     print(\"Báº£ng Gold Ä‘ang trá»‘ng\")\n",
    "    \n",
    "    print(f\"ðŸ“¦ Batch hiá»‡n táº¡i: {total_comments:,} comment\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Báº£ng Gold chÆ°a tá»“n táº¡i: {str(e)}\")\n",
    "    print(\"ðŸ“ Load batch Ä‘áº§u tiÃªn tá»« Silver...\")\n",
    "    \n",
    "    # Náº¿u Gold chÆ°a cÃ³, láº¥y batch Ä‘áº§u tiÃªn\n",
    "    df_comments = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            articleID as postID,\n",
    "            commentID,\n",
    "            comment,\n",
    "            commentTime,\n",
    "            commentLike,\n",
    "            levelComment,\n",
    "            numberOfReply,\n",
    "            created_at,\n",
    "            updated_at\n",
    "        FROM nessie.silver_tables.comment\n",
    "        WHERE comment IS NOT NULL\n",
    "          AND TRIM(comment) != ''\n",
    "        LIMIT {BATCH_SIZE}\n",
    "    \"\"\")\n",
    "    \n",
    "    total_comments = df_comments.count()\n",
    "    print(f\"ðŸ“¦ Batch Ä‘áº§u tiÃªn: {total_comments:,} comment\")\n",
    "\n",
    "if total_comments > 0:\n",
    "    print(f\"\\nSáºµn sÃ ng xá»­ lÃ½ {total_comments:,} comment\")\n",
    "else:\n",
    "    print(\"\\nðŸŽ‰ Táº¥t cáº£ comment Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½!\")\n",
    "    print(\"KhÃ´ng cÃ³ comment má»›i cáº§n phÃ¢n tÃ­ch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9e67d7",
   "metadata": {},
   "source": [
    "## 5. Ãp Dá»¥ng Model Sentiment cho Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0556f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, TimestampType\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# ===== HÃ€M Xá»¬ LÃ GIÃ TRá»Š TIMESTAMP =====\n",
    "def safe_ts(v):\n",
    "    \"\"\"Chuyá»ƒn pandas NaT / NaN / None thÃ nh None hoáº·c datetime.\"\"\"\n",
    "    if v is None or pd.isna(v):\n",
    "        return None\n",
    "    if isinstance(v, pd.Timestamp):\n",
    "        return v.to_pydatetime()\n",
    "    return v\n",
    "\n",
    "print(\"Äang Ã¡p dá»¥ng model sentiment...\")\n",
    "\n",
    "if total_comments > 0:\n",
    "    print(\"\\nðŸ“¥ BÆ°á»›c 1: Thu tháº­p dá»¯ liá»‡u vá» driver...\")\n",
    "    comments_pd = df_comments.toPandas()\n",
    "    print(f\"Thu tháº­p {len(comments_pd)} comment\")\n",
    "\n",
    "    batch_save_size = 1000\n",
    "    results = []\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"postID\", StringType(), True),\n",
    "        StructField(\"commentID\", LongType(), True),\n",
    "        StructField(\"comment\", StringType(), True),\n",
    "        StructField(\"sentiment\", StringType(), True),\n",
    "        StructField(\"commentTime\", TimestampType(), True),\n",
    "        StructField(\"commentLike\", IntegerType(), True),\n",
    "        StructField(\"levelComment\", IntegerType(), True),\n",
    "        StructField(\"numberOfReply\", IntegerType(), True),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "        StructField(\"updated_at\", TimestampType(), True)\n",
    "    ])\n",
    "\n",
    "    print(\"\\nBÆ°á»›c 2: Chuáº©n bá»‹ báº£ng Gold...\")\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS nessie.gold_result_model_multi_task\")\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS nessie.gold_result_model_multi_task.Comment_Sentiment (\n",
    "        postID STRING,\n",
    "        commentID BIGINT,\n",
    "        comment STRING,\n",
    "        sentiment STRING,\n",
    "        commentTime TIMESTAMP,\n",
    "        commentLike INT,\n",
    "        levelComment INT,\n",
    "        numberOfReply INT,\n",
    "        created_at TIMESTAMP,\n",
    "        updated_at TIMESTAMP\n",
    "    ) USING iceberg\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Báº£ng Ä‘Ã£ tá»“n táº¡i hoáº·c Ä‘Æ°á»£c táº¡o má»›i.\")\n",
    "\n",
    "    print(f\"\\n BÆ°á»›c 3: Xá»­ lÃ½ {len(comments_pd)} comment...\")\n",
    "\n",
    "    for idx, row in comments_pd.iterrows():\n",
    "\n",
    "        # ---- Predict sentiment ----\n",
    "        sentiment = predict_sentiment(row['comment'])\n",
    "\n",
    "        # ---- Append káº¿t quáº£ vÃ o list ----\n",
    "        results.append({\n",
    "            'postID': row['postID'],\n",
    "            'commentID': int(row['commentID']) if not pd.isna(row['commentID']) else None,\n",
    "            'comment': row['comment'],\n",
    "            'sentiment': sentiment,\n",
    "            'commentTime': safe_ts(row['commentTime']),\n",
    "            'commentLike': int(row['commentLike']) if not pd.isna(row['commentLike']) else 0,\n",
    "            'levelComment': int(row['levelComment']) if not pd.isna(row['levelComment']) else 0,\n",
    "            'numberOfReply': int(row['numberOfReply']) if not pd.isna(row['numberOfReply']) else 0,\n",
    "            'created_at': safe_ts(row['created_at']),\n",
    "            'updated_at': datetime.now()\n",
    "        })\n",
    "\n",
    "        if (idx + 1) % 500 == 0:\n",
    "            print(f\"   âœ“ ÄÃ£ xá»­ lÃ½ {idx+1}/{len(comments_pd)} comment...\")\n",
    "\n",
    "        # ---- LÆ°u batch ----\n",
    "        if len(results) >= batch_save_size:\n",
    "            print(f\"\\nLÆ°u batch {len(results)} vÃ o Gold...\")\n",
    "            df_batch = spark.createDataFrame(results, schema=schema)\n",
    "            df_batch.writeTo(\"nessie.gold_result_model_multi_task.Comment_Sentiment\") \\\n",
    "                .using(\"iceberg\") \\\n",
    "                .append()\n",
    "            print(\"   â†’ ÄÃ£ lÆ°u xong batch\")\n",
    "            results = []\n",
    "\n",
    "    # ---- LÆ°u batch cuá»‘i ----\n",
    "    if len(results) > 0:\n",
    "        print(f\"\\nLÆ°u batch cuá»‘i {len(results)} comment...\")\n",
    "        df_batch = spark.createDataFrame(results, schema=schema)\n",
    "        df_batch.writeTo(\"nessie.gold_result_model_multi_task.Comment_Sentiment\") \\\n",
    "            .using(\"iceberg\") \\\n",
    "            .append()\n",
    "        print(\"   â†’ ÄÃ£ lÆ°u xong batch cuá»‘i\")\n",
    "\n",
    "    print(\"\\nHoÃ n táº¥t phÃ¢n tÃ­ch sentiment vÃ  lÆ°u Gold layer!\")\n",
    "\n",
    "    print(\"\\nVÃ­ dá»¥ 10 dÃ²ng má»›i nháº¥t tá»« Gold:\")\n",
    "    df_gold_sample = spark.sql(\"\"\"\n",
    "        SELECT postID, commentID, comment, sentiment, commentTime, commentLike\n",
    "        FROM nessie.gold_result_model_multi_task.Comment_Sentiment\n",
    "        ORDER BY updated_at DESC\n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "    df_gold_sample.show(10, truncate=50)\n",
    "\n",
    "    df_with_sentiment = spark.sql(\"\"\"\n",
    "        SELECT * FROM nessie.gold_result_model_multi_task.Comment_Sentiment\n",
    "        ORDER BY updated_at DESC\n",
    "        LIMIT {}\n",
    "    \"\"\".format(total_comments))\n",
    "\n",
    "else:\n",
    "    print(\"KhÃ´ng cÃ³ comment Ä‘á»ƒ xá»­ lÃ½!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8b81f9",
   "metadata": {},
   "source": [
    "## 6. Dá»«ng Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddad9bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giáº£i phÃ³ng cache\n",
    "if total_comments > 0:\n",
    "    df_with_sentiment.unpersist()\n",
    "\n",
    "# Dá»«ng Spark Session\n",
    "spark.stop()\n",
    "print(\"ðŸ›‘ Spark Session Ä‘Ã£ Ä‘Æ°á»£c dá»«ng!\")\n",
    "print(\"âœ… HoÃ n táº¥t!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
