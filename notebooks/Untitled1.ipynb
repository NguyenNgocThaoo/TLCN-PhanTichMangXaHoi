{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c57aa9d0-7a99-4353-94a3-98cbd85f548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Table 'nessie.demo_db.sales' created.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.demo_db\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nessie.demo_db.sales (\n",
    "    order_id INT,\n",
    "    customer STRING,\n",
    "    amount DOUBLE\n",
    ") USING iceberg\n",
    "\"\"\")\n",
    "\n",
    "print(\"âœ… Table 'nessie.demo_db.sales' created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a96dc6bb-7fb5-463c-971f-c74f6b7060f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/21 07:18:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SparkSession connected to Nessie + MinIO via Iceberg.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Iceberg-Nessie-MinIO-Demo\")\n",
    "    # ===== Iceberg Catalog qua Nessie =====\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v1\")\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://warehouse/\")\n",
    "    # ===== Cáº¥u hÃ¬nh MinIO (S3-compatible) =====\n",
    "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.access-key\", \"admin\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.secret-key\", \"admin123\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\")\n",
    "    # ===== Spark + Hadoop S3 connector =====\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"âœ… SparkSession connected to Nessie + MinIO via Iceberg.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b36a94-8b67-483b-b64b-4dd5e0593a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/21 06:56:27 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to demo_db/sales_806d5e7f-1618-408f-8a27-5f2c4e83847e/data/00007-7-45d5d560-e5d2-452a-99ff-24ab6edf3ce3-0-00001.parquet. This is unsupported\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data written successfully!\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", 100.5),\n",
    "    (2, \"Bob\", 230.75),\n",
    "    (3, \"Charlie\", 315.2)\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"order_id\", \"customer\", \"amount\"])\n",
    "\n",
    "df.writeTo(\"nessie.demo_db.sales\").append()\n",
    "\n",
    "print(\"âœ… Data written successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf4374bb-c6d5-4968-998b-9502ed7aa4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+\n",
      "|order_id|customer|amount|\n",
      "+--------+--------+------+\n",
      "|       1|   Alice| 100.5|\n",
      "|       2|     Bob|230.75|\n",
      "|       3| Charlie| 315.2|\n",
      "+--------+--------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/21 06:57:44 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-040921c4-1a09-4451-b6bc-7957b4eecf18/userFiles-e029a435-891e-40d6-9645-2fe90f71c1f9. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/spark-040921c4-1a09-4451-b6bc-7957b4eecf18/userFiles-e029a435-891e-40d6-9645-2fe90f71c1f9\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM nessie.demo_db.sales\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08843edc-309c-4192-8a71-e945018a7d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'defaultBranch': 'main', 'maxSupportedApiVersion': 2}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "resp = requests.get(\"http://nessie:19120/api/v1/config\")\n",
    "print(resp.status_code)\n",
    "print(resp.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da3ed4b7-4a06-4c9b-9b3b-b695930c832a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to Nessie\n",
      "Current default branch: main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pynessie import NessieClient\n",
    "from pynessie.conf import build_config\n",
    "\n",
    "# 1. Khai bÃ¡o endpoint Nessie (container trong cÃ¹ng network)\n",
    "os.environ[\"NESSIE_ENDPOINT\"] = \"http://nessie:19120/api/v1\"\n",
    "\n",
    "# 2. Táº¡o Ä‘á»‘i tÆ°á»£ng config tá»« environment\n",
    "config = build_config()\n",
    "\n",
    "# 3. Truyá»n config nÃ y vÃ o NessieClient\n",
    "client = NessieClient(config)\n",
    "\n",
    "# 4. Kiá»ƒm tra káº¿t ná»‘i\n",
    "print(\"âœ… Connected to Nessie\")\n",
    "print(\"Current default branch:\", client.get_default_branch())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4133b9c7-2018-4301-a508-4352097c3d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/21 07:31:00 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/10/21 07:31:03 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to demo_db/sales_806d5e7f-1618-408f-8a27-5f2c4e83847e/data/00005-5-b783c797-959e-4b9c-bd1c-dcb4aa30f3d7-0-00001.parquet. This is unsupported\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+\n",
      "|order_id|customer|amount|\n",
      "+--------+--------+------+\n",
      "|       1|   Alice| 100.5|\n",
      "|       4|   David| 420.0|\n",
      "|       2|     Bob|230.75|\n",
      "|       5|     Eva| 180.5|\n",
      "|       3| Charlie| 315.2|\n",
      "+--------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2 = [\n",
    "    (4, \"David\", 420.0),\n",
    "    (5, \"Eva\", 180.5)\n",
    "]\n",
    "spark.createDataFrame(data2, [\"order_id\", \"customer\", \"amount\"])\\\n",
    "    .writeTo(\"nessie.demo_db.sales\").append()\n",
    "\n",
    "spark.sql(\"SELECT * FROM nessie.demo_db.sales\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87d38aa5-8c16-44df-874e-944490ec86b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogEntry(commit_meta=CommitMeta(hash_='636d8430c9628ac233a3ec88a814921e4122ff2c6767c38dafeea66a5353a617', commitTime=datetime.datetime(2025, 10, 21, 7, 31, 3, 708641, tzinfo=datetime.timezone.utc), authorTime=datetime.datetime(2025, 10, 21, 7, 31, 3, 708641, tzinfo=datetime.timezone.utc), committer='', author='root', signedOffBy=None, message='Iceberg append against demo_db.sales', properties={'app-id': 'local-1761031120340', 'iceberg.operation': 'append', 'application-type': 'iceberg'}), parent_commit_hash='337cba8ab76111ea4f24b0a0c96255dad18fec0d8c48a44d6cc981c37b0673db', operations=None)\n",
      "['__annotations__', '__attrs_attrs__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__match_args__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'commit_meta', 'operations', 'parent_commit_hash']\n"
     ]
    }
   ],
   "source": [
    "entry = next(client.get_log(\"main\"))\n",
    "print(entry)\n",
    "print(dir(entry))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24ef2a8a-d7b0-4be6-bb0c-9a6ff5650584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit hash: None | Message: Iceberg append against demo_db.sales\n",
      "Commit hash: None | Message: Iceberg append against demo_db.sales\n",
      "Commit hash: None | Message: Iceberg table created/registered with name demo_db.sales\n",
      "Commit hash: None | Message: create namespace demo_db\n"
     ]
    }
   ],
   "source": [
    "for log_entry in client.get_log(\"main\"):\n",
    "    print(f\"Commit hash: {log_entry.__hash__} | Message: {log_entry.commit_meta.message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71512baf-b626-420f-bae0-5c7b430abdf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NessieException",
     "evalue": "Client Error Bad Request: Target hash must be provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNessieException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_branch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexperiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Branch \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m created from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pynessie/client/nessie_client.py:119\u001b[0m, in \u001b[0;36mNessieClient.create_branch\u001b[0;34m(self, branch, ref, hash_on_ref)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a branch.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m:param branch: name of new branch to create\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m:return: Nessie branch object\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m ref_json \u001b[38;5;241m=\u001b[39m ReferenceSchema()\u001b[38;5;241m.\u001b[39mdump(Branch(branch, hash_on_ref))\n\u001b[0;32m--> 119\u001b[0m ref_obj \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_reference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ssl_verify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(Branch, ReferenceSchema()\u001b[38;5;241m.\u001b[39mload(ref_obj))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pynessie/client/_endpoints.py:167\u001b[0m, in \u001b[0;36mcreate_reference\u001b[0;34m(base_url, auth, ref_json, source_ref, ssl_verify)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_ref:\n\u001b[1;32m    166\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msourceRefName\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m source_ref\n\u001b[0;32m--> 167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[38;5;28mdict\u001b[39m, \u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mssl_verify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssl_verify\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pynessie/client/_endpoints.py:73\u001b[0m, in \u001b[0;36m_post\u001b[0;34m(url, auth, json, ssl_verify, params, timeout_sec)\u001b[0m\n\u001b[1;32m     69\u001b[0m     json \u001b[38;5;241m=\u001b[39m jsonlib\u001b[38;5;241m.\u001b[39mloads(json)\n\u001b[1;32m     70\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m     71\u001b[0m     url, headers\u001b[38;5;241m=\u001b[39m_get_headers(json \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m), verify\u001b[38;5;241m=\u001b[39mssl_verify, json\u001b[38;5;241m=\u001b[39mjson, params\u001b[38;5;241m=\u001b[39mparams, auth\u001b[38;5;241m=\u001b[39mauth, timeout\u001b[38;5;241m=\u001b[39mtimeout_sec\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_check_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pynessie/client/_endpoints.py:119\u001b[0m, in \u001b[0;36m_check_error\u001b[0;34m(r)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:  \u001b[38;5;66;03m# NOQA # pylint: disable=W0702\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# rare/unexpected case when the server responds with a non-JSON payload for an error\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     parsed_response \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _create_exception(parsed_response, r\u001b[38;5;241m.\u001b[39mstatus_code, reason, r\u001b[38;5;241m.\u001b[39murl)\n",
      "\u001b[0;31mNessieException\u001b[0m: Client Error Bad Request: Target hash must be provided."
     ]
    }
   ],
   "source": [
    "client.create_branch(\"experiment\", \"main\")\n",
    "print(\"âœ… Branch 'experiment' created from 'main'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b1d56-372d-4695-a0e5-8ca021ce3105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Táº¡o SparkSession trá» tá»›i branch 'experiment'\n",
    "spark_branch = (\n",
    "    SparkSession.builder.appName(\"NessieExperimentBranch\")\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v1\")\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", \"experiment\")   # ðŸ‘ˆ Chuyá»ƒn branch\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://warehouse/\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.access-key\", \"admin\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.secret-key\", \"admin123\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Ghi thÃªm dá»¯ liá»‡u chá»‰ trong branch 'experiment'\n",
    "data3 = [(6, \"Frank\", 999.9)]\n",
    "spark_branch.createDataFrame(data3, [\"order_id\", \"customer\", \"amount\"])\\\n",
    "    .writeTo(\"nessie.demo_db.sales\").append()\n",
    "\n",
    "print(\"âœ… Wrote data to branch 'experiment'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0430d88-0905-430b-bf56-42a61e639f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM nessie.demo_db.sales ORDER BY order_id\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a9a100-09ae-4dc7-9070-c665af7e8719",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_branch.sql(\"SELECT * FROM nessie.demo_db.sales ORDER BY order_id\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9eaa15c2-d9fd-481e-9a09-60915a27140c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+------------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|committed_at           |snapshot_id        |parent_id         |operation|manifest_list                                                                                                                                   |summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "+-----------------------+-------------------+------------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2025-10-21 06:56:28.281|614667612930511733 |NULL              |append   |s3a://warehouse/demo_db/sales_806d5e7f-1618-408f-8a27-5f2c4e83847e/metadata/snap-614667612930511733-1-5492bd63-f2e7-4a52-8759-086d2e3fd66b.avro |{spark.app.id -> local-1761029644992, added-data-files -> 3, added-records -> 3, added-files-size -> 2979, changed-partition-count -> 1, total-records -> 3, total-files-size -> 2979, total-data-files -> 3, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.5.5, app-id -> local-1761029644992, engine-name -> spark, iceberg-version -> Apache Iceberg 1.8.1 (commit 9ce0fcf0af7becf25ad9fc996c3bad2afdcfd33d)}|\n",
      "|2025-10-21 07:31:03.566|1791065480393736880|614667612930511733|append   |s3a://warehouse/demo_db/sales_806d5e7f-1618-408f-8a27-5f2c4e83847e/metadata/snap-1791065480393736880-1-f6305c06-0a55-41d2-b4ab-374017e29db5.avro|{spark.app.id -> local-1761031120340, added-data-files -> 2, added-records -> 2, added-files-size -> 1972, changed-partition-count -> 1, total-records -> 5, total-files-size -> 4951, total-data-files -> 5, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.5.5, app-id -> local-1761031120340, engine-name -> spark, iceberg-version -> Apache Iceberg 1.8.1 (commit 9ce0fcf0af7becf25ad9fc996c3bad2afdcfd33d)}|\n",
      "+-----------------------+-------------------+------------------+---------+------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM nessie.demo_db.sales.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5da85dc-da54-4761-b8b2-c31393cacf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+\n",
      "|order_id|customer|amount|\n",
      "+--------+--------+------+\n",
      "|       1|   Alice| 100.5|\n",
      "|       2|     Bob|230.75|\n",
      "|       3| Charlie| 315.2|\n",
      "|       4|   David| 420.0|\n",
      "|       5|     Eva| 180.5|\n",
      "+--------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snapshot_id = 1791065480393736880\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT * FROM nessie.demo_db.sales VERSION AS OF {snapshot_id}\n",
    "\"\"\"\n",
    "old_df = spark.sql(query)\n",
    "old_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0051e185-9338-4150-8131-5c7312034cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogEntry(commit_meta=CommitMeta(hash_='636d8430c9628ac233a3ec88a814921e4122ff2c6767c38dafeea66a5353a617', commitTime=datetime.datetime(2025, 10, 21, 7, 31, 3, 708641, tzinfo=datetime.timezone.utc), authorTime=datetime.datetime(2025, 10, 21, 7, 31, 3, 708641, tzinfo=datetime.timezone.utc), committer='', author='root', signedOffBy=None, message='Iceberg append against demo_db.sales', properties={'app-id': 'local-1761031120340', 'iceberg.operation': 'append', 'application-type': 'iceberg'}), parent_commit_hash='337cba8ab76111ea4f24b0a0c96255dad18fec0d8c48a44d6cc981c37b0673db', operations=None)\n",
      "['__annotations__', '__attrs_attrs__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__match_args__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'commit_meta', 'operations', 'parent_commit_hash']\n"
     ]
    }
   ],
   "source": [
    "entry = next(client.get_log(\"main\"))\n",
    "print(entry)\n",
    "print(dir(entry))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d3d09-3c1e-4e63-aefc-2d463715d1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
