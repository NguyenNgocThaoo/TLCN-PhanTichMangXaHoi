{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d943b6",
   "metadata": {},
   "source": [
    "## 1. Kh·ªüi t·∫°o Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3545b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/08 15:58:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Spark Session ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, current_timestamp\n",
    "import os\n",
    "\n",
    "# Set AWS environment variables for MinIO\n",
    "os.environ['AWS_REGION'] = 'us-east-1'\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'admin'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'admin123'\n",
    "\n",
    "# Kh·ªüi t·∫°o Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Reset_EntityNameNormalized\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.memory\", \"1536m\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    # ===== Iceberg Catalog qua Nessie =====\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v2\")\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://gold/\")\n",
    "    .config(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    # ===== C·∫•u h√¨nh MinIO =====\n",
    "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.access-key-id\", \"admin\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.secret-access-key\", \"admin123\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\")\n",
    "    .config(\"spark.sql.catalog.nessie.s3.region\", \"us-east-1\")\n",
    "    # ===== Spark + Hadoop S3 connector =====\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.region\", \"us-east-1\")\n",
    "    .config(\"spark.executorEnv.AWS_REGION\", \"us-east-1\")\n",
    "    .config(\"spark.executorEnv.AWS_ACCESS_KEY_ID\", \"admin\")\n",
    "    .config(\"spark.executorEnv.AWS_SECRET_ACCESS_KEY\", \"admin123\")\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/hadoop-aws-3.3.4.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"‚úì Spark Session ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c26fd0",
   "metadata": {},
   "source": [
    "## 2. Ki·ªÉm tra d·ªØ li·ªáu hi·ªán t·∫°i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4bb2c0",
   "metadata": {},
   "source": [
    "## 1.5. L√†m s·∫°ch b·ªô nh·ªõ cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88be9fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ x√≥a cache!\n",
      "‚úì ƒê√£ unpersist c√°c b·∫£ng cached!\n",
      "‚úì B·ªô nh·ªõ ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch, s·∫µn s√†ng ch·∫°y reset!\n"
     ]
    }
   ],
   "source": [
    "# L√†m s·∫°ch cache v√† b·ªô nh·ªõ\n",
    "spark.catalog.clearCache()\n",
    "print(\"‚úì ƒê√£ x√≥a cache!\")\n",
    "\n",
    "# Unpersist t·∫•t c·∫£ c√°c DataFrame ƒëang cached\n",
    "for table in spark.catalog.listTables():\n",
    "    try:\n",
    "        spark.catalog.uncacheTable(f\"{table.database}.{table.name}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"‚úì ƒê√£ unpersist c√°c b·∫£ng cached!\")\n",
    "print(\"‚úì B·ªô nh·ªõ ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch, s·∫µn s√†ng ch·∫°y reset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01b43520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä D·ªØ li·ªáu hi·ªán t·∫°i:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+-----------+--------------------------------------+----------------------------+\n",
      "|postID          |entityID|entityOrder|entityName                            |entityNameNormalized        |\n",
      "+----------------+--------+-----------+--------------------------------------+----------------------------+\n",
      "|1805987310258377|12      |10         |kh·ªëi D                                |NULL                        |\n",
      "|1847846212739153|6       |5          |T·ªët nghi·ªáp                            |NULL                        |\n",
      "|1852139512309823|5       |1          |To√°n h·ªçc                              |NULL                        |\n",
      "|1859840021539772|6       |2          |h·ªçc sinh gi·ªèi                         |NULL                        |\n",
      "|1860049688185472|5       |1          |T·ªï ch·ª©c S·ª± ki·ªán                       |NULL                        |\n",
      "|1874577546732686|6       |2          |ƒë·∫°i h·ªçc                               |NULL                        |\n",
      "|1880661956124245|3       |3          |h·ªçc ph√≠                               |NULL                        |\n",
      "|1883141982542909|7       |1          |Tr∆∞·ªùng ƒê·∫°i h·ªçc Ki√™n Giang             |tr∆∞·ªùng - ƒë·∫°i h·ªçc ki√™n giang |\n",
      "|1883376065852834|7       |1          |ƒê·∫°i h·ªçc M·ªπ thu·∫≠t Th√†nh ph·ªë H·ªì Ch√≠ Minh|tr∆∞·ªùng - ƒë·∫°i h·ªçc lu·∫≠t tp.hcm|\n",
      "|1886976108826163|4       |9          |HN                                    |NULL                        |\n",
      "+----------------+--------+-----------+--------------------------------------+----------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Th·ªëng k√™:\n",
      "   ‚Ä¢ T·ªïng s·ªë records: 16,744\n",
      "   ‚Ä¢ Records c√≥ entityNameNormalized: 2,465\n",
      "   ‚Ä¢ Records NULL: 14,279\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªçc b·∫£ng Post_Entity\n",
    "df = spark.sql(\"SELECT * FROM nessie.gold_result_model_multi_task.Post_Entity LIMIT 10\")\n",
    "\n",
    "print(\"üìä D·ªØ li·ªáu hi·ªán t·∫°i:\")\n",
    "df.select(\"postID\", \"entityID\", \"entityOrder\", \"entityName\", \"entityNameNormalized\").show(10, truncate=False)\n",
    "\n",
    "# Th·ªëng k√™\n",
    "total_records = spark.sql(\"SELECT COUNT(*) as total FROM nessie.gold_result_model_multi_task.Post_Entity\").collect()[0]['total']\n",
    "normalized_records = spark.sql(\"SELECT COUNT(*) as total FROM nessie.gold_result_model_multi_task.Post_Entity WHERE entityNameNormalized IS NOT NULL\").collect()[0]['total']\n",
    "\n",
    "print(f\"\\nüìà Th·ªëng k√™:\")\n",
    "print(f\"   ‚Ä¢ T·ªïng s·ªë records: {total_records:,}\")\n",
    "print(f\"   ‚Ä¢ Records c√≥ entityNameNormalized: {normalized_records:,}\")\n",
    "print(f\"   ‚Ä¢ Records NULL: {total_records - normalized_records:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a52e0a",
   "metadata": {},
   "source": [
    "## 3. Reset c·ªôt entityNameNormalized\n",
    "\n",
    "**Ch·ªçn 1 trong 3 option:**\n",
    "- **Option 1:** Reset TO√ÄN B·ªò (set NULL cho t·∫•t c·∫£)\n",
    "- **Option 2:** Reset ch·ªâ entityType = 'MAJ'\n",
    "- **Option 3:** Reset ch·ªâ entityType = 'ORG'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eaa9c4",
   "metadata": {},
   "source": [
    "### Option 1: Reset TO√ÄN B·ªò b·∫£ng Post_Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26905c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  ƒêang reset TO√ÄN B·ªò c·ªôt entityNameNormalized...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ reset to√†n b·ªô c·ªôt entityNameNormalized = NULL!\n",
      "‚úì S·ªë records c√≥ entityNameNormalized = NULL: 16,744\n"
     ]
    }
   ],
   "source": [
    "# ‚ö†Ô∏è C·∫¢NH B√ÅO: Thao t√°c n√†y s·∫Ω x√≥a TO√ÄN B·ªò d·ªØ li·ªáu entityNameNormalized!\n",
    "\n",
    "print(\"‚öôÔ∏è  ƒêang reset TO√ÄN B·ªò c·ªôt entityNameNormalized...\")\n",
    "\n",
    "# L√†m s·∫°ch cache tr∆∞·ªõc khi ch·∫°y\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# ƒê·ªçc to√†n b·ªô d·ªØ li·ªáu\n",
    "df_all = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        postID,\n",
    "        entityID,\n",
    "        entityOrder,\n",
    "        entityName,\n",
    "        created_at\n",
    "    FROM nessie.gold_result_model_multi_task.Post_Entity\n",
    "\"\"\")\n",
    "\n",
    "# Th√™m c·ªôt entityNameNormalized = NULL v√† updated_at\n",
    "df_reset = df_all.withColumn(\"entityNameNormalized\", lit(None).cast(\"string\")) \\\n",
    "                 .withColumn(\"updated_at\", current_timestamp())\n",
    "\n",
    "# T·∫°o temporary view\n",
    "df_reset.createOrReplaceTempView(\"temp_reset_all\")\n",
    "\n",
    "# MERGE ƒë·ªÉ update\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO nessie.gold_result_model_multi_task.Post_Entity AS target\n",
    "    USING temp_reset_all AS source\n",
    "    ON target.postID = source.postID \n",
    "        AND target.entityID = source.entityID\n",
    "        AND target.entityOrder = source.entityOrder\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        target.entityNameNormalized = source.entityNameNormalized,\n",
    "        target.updated_at = source.updated_at\n",
    "\"\"\")\n",
    "\n",
    "# X√≥a temporary view v√† DataFrame\n",
    "spark.catalog.dropTempView(\"temp_reset_all\")\n",
    "df_reset.unpersist()\n",
    "df_all.unpersist()\n",
    "\n",
    "print(\"‚úì ƒê√£ reset to√†n b·ªô c·ªôt entityNameNormalized = NULL!\")\n",
    "\n",
    "# X√°c minh\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as total_null \n",
    "    FROM nessie.gold_result_model_multi_task.Post_Entity \n",
    "    WHERE entityNameNormalized IS NULL\n",
    "\"\"\").collect()[0]['total_null']\n",
    "\n",
    "print(f\"‚úì S·ªë records c√≥ entityNameNormalized = NULL: {result:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a0d5f6",
   "metadata": {},
   "source": [
    "### Option 2: Reset ch·ªâ entityType = 'MAJ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af847455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  ƒêang reset entityNameNormalized cho entityType = 'MAJ'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ƒê√£ reset entityNameNormalized cho entityType = 'MAJ'!\n",
      "‚úì S·ªë records MAJ c√≥ entityNameNormalized = NULL: 5,382\n"
     ]
    }
   ],
   "source": [
    "# Reset ch·ªâ c√°c records c√≥ entityType = 'MAJ'\n",
    "\n",
    "print(\"‚öôÔ∏è  ƒêang reset entityNameNormalized cho entityType = 'MAJ'...\")\n",
    "\n",
    "# L√†m s·∫°ch cache tr∆∞·ªõc khi ch·∫°y\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu v·ªõi JOIN Entity ƒë·ªÉ l·ªçc MAJ\n",
    "df_maj = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        pe.postID,\n",
    "        pe.entityID,\n",
    "        pe.entityOrder,\n",
    "        pe.entityName,\n",
    "        pe.created_at\n",
    "    FROM nessie.gold_result_model_multi_task.Post_Entity pe\n",
    "    INNER JOIN nessie.gold_result_model_multi_task.Entity e\n",
    "        ON pe.entityID = e.entityID\n",
    "    WHERE e.entityType = 'MAJ'\n",
    "\"\"\")\n",
    "\n",
    "# Set entityNameNormalized = NULL\n",
    "df_reset_maj = df_maj.withColumn(\"entityNameNormalized\", lit(None).cast(\"string\")) \\\n",
    "                     .withColumn(\"updated_at\", current_timestamp())\n",
    "\n",
    "# T·∫°o temporary view\n",
    "df_reset_maj.createOrReplaceTempView(\"temp_reset_maj\")\n",
    "\n",
    "# MERGE ƒë·ªÉ update ch·ªâ MAJ\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO nessie.gold_result_model_multi_task.Post_Entity AS target\n",
    "    USING temp_reset_maj AS source\n",
    "    ON target.postID = source.postID \n",
    "        AND target.entityID = source.entityID\n",
    "        AND target.entityOrder = source.entityOrder\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        target.entityNameNormalized = source.entityNameNormalized,\n",
    "        target.updated_at = source.updated_at\n",
    "\"\"\")\n",
    "\n",
    "# X√≥a temporary view v√† DataFrame\n",
    "spark.catalog.dropTempView(\"temp_reset_maj\")\n",
    "df_reset_maj.unpersist()\n",
    "df_maj.unpersist()\n",
    "\n",
    "print(\"‚úì ƒê√£ reset entityNameNormalized cho entityType = 'MAJ'!\")\n",
    "\n",
    "# X√°c minh\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as total_null \n",
    "    FROM nessie.gold_result_model_multi_task.Post_Entity pe\n",
    "    INNER JOIN nessie.gold_result_model_multi_task.Entity e \n",
    "        ON pe.entityID = e.entityID\n",
    "    WHERE e.entityType = 'MAJ' \n",
    "    AND pe.entityNameNormalized IS NULL\n",
    "\"\"\").collect()[0]['total_null']\n",
    "\n",
    "print(f\"‚úì S·ªë records MAJ c√≥ entityNameNormalized = NULL: {result:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d03cd8",
   "metadata": {},
   "source": [
    "### Option 3: Reset ch·ªâ entityType = 'ORG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "509d8b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  ƒêang reset entityNameNormalized cho entityType = 'ORG'...\n",
      "‚úì ƒê√£ reset entityNameNormalized cho entityType = 'ORG'!\n",
      "‚úì S·ªë records ORG c√≥ entityNameNormalized = NULL: 2,555\n"
     ]
    }
   ],
   "source": [
    "# Reset ch·ªâ c√°c records c√≥ entityType = 'ORG'\n",
    "\n",
    "print(\"‚öôÔ∏è  ƒêang reset entityNameNormalized cho entityType = 'ORG'...\")\n",
    "\n",
    "# L√†m s·∫°ch cache tr∆∞·ªõc khi ch·∫°y\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu v·ªõi JOIN Entity ƒë·ªÉ l·ªçc ORG\n",
    "df_org = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        pe.postID,\n",
    "        pe.entityID,\n",
    "        pe.entityOrder,\n",
    "        pe.entityName,\n",
    "        pe.created_at\n",
    "    FROM nessie.gold_result_model_multi_task.Post_Entity pe\n",
    "    INNER JOIN nessie.gold_result_model_multi_task.Entity e\n",
    "        ON pe.entityID = e.entityID\n",
    "    WHERE e.entityType = 'ORG'\n",
    "\"\"\")\n",
    "\n",
    "# Set entityNameNormalized = NULL\n",
    "df_reset_org = df_org.withColumn(\"entityNameNormalized\", lit(None).cast(\"string\")) \\\n",
    "                     .withColumn(\"updated_at\", current_timestamp())\n",
    "\n",
    "# T·∫°o temporary view\n",
    "df_reset_org.createOrReplaceTempView(\"temp_reset_org\")\n",
    "\n",
    "# MERGE ƒë·ªÉ update ch·ªâ ORG\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO nessie.gold_result_model_multi_task.Post_Entity AS target\n",
    "    USING temp_reset_org AS source\n",
    "    ON target.postID = source.postID \n",
    "        AND target.entityID = source.entityID\n",
    "        AND target.entityOrder = source.entityOrder\n",
    "    WHEN MATCHED THEN UPDATE SET\n",
    "        target.entityNameNormalized = source.entityNameNormalized,\n",
    "        target.updated_at = source.updated_at\n",
    "\"\"\")\n",
    "\n",
    "# X√≥a temporary view v√† DataFrame\n",
    "spark.catalog.dropTempView(\"temp_reset_org\")\n",
    "df_reset_org.unpersist()\n",
    "df_org.unpersist()\n",
    "\n",
    "print(\"‚úì ƒê√£ reset entityNameNormalized cho entityType = 'ORG'!\")\n",
    "\n",
    "# X√°c minh\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as total_null \n",
    "    FROM nessie.gold_result_model_multi_task.Post_Entity pe\n",
    "    INNER JOIN nessie.gold_result_model_multi_task.Entity e \n",
    "        ON pe.entityID = e.entityID\n",
    "    WHERE e.entityType = 'ORG' \n",
    "    AND pe.entityNameNormalized IS NULL\n",
    "\"\"\").collect()[0]['total_null']\n",
    "\n",
    "print(f\"‚úì S·ªë records ORG c√≥ entityNameNormalized = NULL: {result:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268b3e9f",
   "metadata": {},
   "source": [
    "## 4. Ki·ªÉm tra k·∫øt qu·∫£ sau khi reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "601b8072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä D·ªØ li·ªáu sau khi reset:\n",
      "+----------------+--------+-----------+--------------------------------------+--------------------+----------+\n",
      "|postID          |entityID|entityOrder|entityName                            |entityNameNormalized|entityType|\n",
      "+----------------+--------+-----------+--------------------------------------+--------------------+----------+\n",
      "|1805987310258377|12      |10         |kh·ªëi D                                |NULL                |TERM      |\n",
      "|1847846212739153|6       |5          |T·ªët nghi·ªáp                            |NULL                |MISC      |\n",
      "|1852139512309823|5       |1          |To√°n h·ªçc                              |NULL                |MAJ       |\n",
      "|1859840021539772|6       |2          |h·ªçc sinh gi·ªèi                         |NULL                |MISC      |\n",
      "|1860049688185472|5       |1          |T·ªï ch·ª©c S·ª± ki·ªán                       |NULL                |MAJ       |\n",
      "|1874577546732686|6       |2          |ƒë·∫°i h·ªçc                               |NULL                |MISC      |\n",
      "|1880661956124245|3       |3          |h·ªçc ph√≠                               |NULL                |FEE       |\n",
      "|1883141982542909|7       |1          |Tr∆∞·ªùng ƒê·∫°i h·ªçc Ki√™n Giang             |NULL                |ORG       |\n",
      "|1883376065852834|7       |1          |ƒê·∫°i h·ªçc M·ªπ thu·∫≠t Th√†nh ph·ªë H·ªì Ch√≠ Minh|NULL                |ORG       |\n",
      "|1886976108826163|4       |9          |HN                                    |NULL                |LOC       |\n",
      "|1909705566553217|6       |2          |K·ªπ s∆∞                                 |NULL                |MISC      |\n",
      "|1911827156341058|4       |1          |H√† N·ªôi                                |NULL                |LOC       |\n",
      "|1928794971310943|7       |2          |tr∆∞·ªùng ƒê·∫°i h·ªçc Kinh t·∫ø - Lu·∫≠t         |NULL                |ORG       |\n",
      "|1940273453496428|7       |1          |Tr∆∞·ªùng ULSA                           |NULL                |ORG       |\n",
      "|1962686491255124|10      |4          |33,04 ƒëi·ªÉm                            |NULL                |SCO       |\n",
      "|2026962678160838|7       |3          |H·ªçc vi·ªán H√†ng kh√¥ng Vi·ªát Nam          |NULL                |ORG       |\n",
      "|2029317831258656|5       |1          |Kƒ© thu·∫≠t √¥ t√¥                         |NULL                |MAJ       |\n",
      "|2032414017615704|10      |2          |5,5                                   |NULL                |SCO       |\n",
      "|2035376963986076|5       |6          |ng√†nh ngh·ªÅ k·ªπ thu·∫≠t                   |NULL                |MAJ       |\n",
      "|2046851396171966|5       |1          |ng√†nh lu·∫≠t th∆∞∆°ng m·∫°i qu·ªëc t·∫ø         |NULL                |MAJ       |\n",
      "+----------------+--------+-----------+--------------------------------------+--------------------+----------+\n",
      "\n",
      "\n",
      "üìà Th·ªëng k√™ theo entityType:\n",
      "+----------+-------------+----------+--------------+\n",
      "|entityType|total_records|null_count|not_null_count|\n",
      "+----------+-------------+----------+--------------+\n",
      "|DATE      |1143         |1143      |0             |\n",
      "|EX        |350          |350       |0             |\n",
      "|FEE       |804          |804       |0             |\n",
      "|LOC       |844          |844       |0             |\n",
      "|MAJ       |5382         |5382      |0             |\n",
      "|MISC      |3424         |3424      |0             |\n",
      "|ORG       |2555         |2555      |0             |\n",
      "|PRO       |162          |162       |0             |\n",
      "|SAL       |265          |265       |0             |\n",
      "|SCO       |715          |715       |0             |\n",
      "|SUBJ      |759          |759       |0             |\n",
      "|TERM      |341          |341       |0             |\n",
      "+----------+-------------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Xem l·∫°i d·ªØ li·ªáu sau khi reset\n",
    "df_after = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        pe.postID,\n",
    "        pe.entityID,\n",
    "        pe.entityOrder,\n",
    "        pe.entityName,\n",
    "        pe.entityNameNormalized,\n",
    "        e.entityType\n",
    "    FROM nessie.gold_result_model_multi_task.Post_Entity pe\n",
    "    INNER JOIN nessie.gold_result_model_multi_task.Entity e \n",
    "        ON pe.entityID = e.entityID\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüìä D·ªØ li·ªáu sau khi reset:\")\n",
    "df_after.show(20, truncate=False)\n",
    "\n",
    "# Th·ªëng k√™ theo entityType\n",
    "stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.entityType,\n",
    "        COUNT(*) as total_records,\n",
    "        SUM(CASE WHEN pe.entityNameNormalized IS NULL THEN 1 ELSE 0 END) as null_count,\n",
    "        SUM(CASE WHEN pe.entityNameNormalized IS NOT NULL THEN 1 ELSE 0 END) as not_null_count\n",
    "    FROM nessie.gold_result_model_multi_task.Post_Entity pe\n",
    "    INNER JOIN nessie.gold_result_model_multi_task.Entity e \n",
    "        ON pe.entityID = e.entityID\n",
    "    GROUP BY e.entityType\n",
    "    ORDER BY e.entityType\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüìà Th·ªëng k√™ theo entityType:\")\n",
    "stats.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d119ae",
   "metadata": {},
   "source": [
    "## 5. D·ª´ng Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b84df56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Spark Session ƒë√£ ƒë∆∞·ª£c d·ª´ng!\n"
     ]
    }
   ],
   "source": [
    "# D·ª´ng Spark Session\n",
    "spark.stop()\n",
    "print(\"‚úì Spark Session ƒë√£ ƒë∆∞·ª£c d·ª´ng!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
